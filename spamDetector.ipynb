{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to estimate the cost of development and monitor the project, a checklist of tasks to be carried out must be drawn up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://trello.com/b/JNiVTMvb/spam-detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I used Trello, to follow my project and to develop my roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You must create functions for the different parts of your code so that you can easily reuse them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/randon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                                sms\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
      "6   ham  Even my brother is not like to speak with me. ...\n",
      "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n"
     ]
    }
   ],
   "source": [
    "#Import data, make sep ad \\t and Names the news colones\n",
    "df = pd.read_csv(\"/home/randon/git/brief-01-12-spamDetector/data/SMSSpamCollection.txt\", sep ='\\t',names=[\"label\", \"sms\"])\n",
    "\n",
    "#Lets see 10 first rows\n",
    "print(df[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">sms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sms                                                               \n",
       "      count unique                                                top freq\n",
       "label                                                                     \n",
       "ham    4825   4516                             Sorry, I'll call later   30\n",
       "spam    747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group now by label to see, how moch ham and how much spam\n",
    "\n",
    "df.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have 4825 Spam, and 747 spam\n",
    "\n",
    "Now lets transforme colonne in 0 or 1, 0 for Ham, and 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                                sms  labelnumber  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...            1   \n",
      "1   ham                      Ok lar... Joking wif u oni...            1   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...            0   \n",
      "3   ham  U dun say so early hor... U c already then say...            1   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...            1   \n",
      "5  spam  FreeMsg Hey there darling it's been 3 week's n...            0   \n",
      "6   ham  Even my brother is not like to speak with me. ...            1   \n",
      "7   ham  As per your request 'Melle Melle (Oru Minnamin...            1   \n",
      "\n",
      "   labelf1ham  labelf1spam  \n",
      "0           1            0  \n",
      "1           1            0  \n",
      "2           0            1  \n",
      "3           1            0  \n",
      "4           1            0  \n",
      "5           0            1  \n",
      "6           1            0  \n",
      "7           1            0  \n"
     ]
    }
   ],
   "source": [
    "df['labelf1ham']= df['label'].map({'ham': 1, 'spam': 0})\n",
    "df['labelf1spam']= df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "#lets see what happening\n",
    "print(df[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets create vectorisation and stop world function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#define stop world\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean(x):\n",
    "    vectoriser = TfidfVectorizer(stop_words = stopwords)\n",
    "    x = vectoriser.fit_transform(x)\n",
    "    return x\n",
    "\n",
    "def fitting(X, y, mod):\n",
    "    mod.fit(X, y)\n",
    "\n",
    "def predict(X, mod):\n",
    "    xx = mod.predict(X)\n",
    "    return xx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df['sms'])\n",
    "y = np.array(df['labelf1ham'])\n",
    "\n",
    "#Clean data\n",
    "x = clean(x)\n",
    "\n",
    "#Define model classification\n",
    "LogReg = LogisticRegression()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,), activation='logistic', alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.08, verbose=True, max_iter=250)\n",
    "svclass= SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'execution = 0.145042 seconde\n",
      "\n",
      "[1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 0]\n",
      "[1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "\n",
    "#fit and predict\n",
    "tmps1=time.time()\n",
    "\n",
    "fitting(x_train, y_train, LogReg)\n",
    "\n",
    "tmps2=time.time() - tmps1\n",
    "print(\"Temps d'execution = %f seconde\\n\" %tmps2)\n",
    "\n",
    "\n",
    "ypred = predict(x_test, LogReg)\n",
    "\n",
    "#test on test set\n",
    "print(ypred[:50])\n",
    "print(y_test[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.92       160\n",
      "           1       0.97      1.00      0.99       955\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_report = classification_report(y_test, ypred)\n",
    "print(logreg_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.42672297\n",
      "Iteration 2, loss = 0.39062156\n",
      "Iteration 3, loss = 0.38927769\n",
      "Iteration 4, loss = 0.38823304\n",
      "Iteration 5, loss = 0.38727843\n",
      "Iteration 6, loss = 0.38680031\n",
      "Iteration 7, loss = 0.38587299\n",
      "Iteration 8, loss = 0.38504709\n",
      "Iteration 9, loss = 0.38506499\n",
      "Iteration 10, loss = 0.38268475\n",
      "Iteration 11, loss = 0.38214738\n",
      "Iteration 12, loss = 0.38086973\n",
      "Iteration 13, loss = 0.37929672\n",
      "Iteration 14, loss = 0.37804895\n",
      "Iteration 15, loss = 0.37609545\n",
      "Iteration 16, loss = 0.37436649\n",
      "Iteration 17, loss = 0.37121310\n",
      "Iteration 18, loss = 0.36856274\n",
      "Iteration 19, loss = 0.36624328\n",
      "Iteration 20, loss = 0.36203743\n",
      "Iteration 21, loss = 0.35806391\n",
      "Iteration 22, loss = 0.35366808\n",
      "Iteration 23, loss = 0.34877907\n",
      "Iteration 24, loss = 0.34301373\n",
      "Iteration 25, loss = 0.33653664\n",
      "Iteration 26, loss = 0.32932764\n",
      "Iteration 27, loss = 0.32134159\n",
      "Iteration 28, loss = 0.31251685\n",
      "Iteration 29, loss = 0.30340942\n",
      "Iteration 30, loss = 0.29275862\n",
      "Iteration 31, loss = 0.28200521\n",
      "Iteration 32, loss = 0.27085289\n",
      "Iteration 33, loss = 0.25947873\n",
      "Iteration 34, loss = 0.24723675\n",
      "Iteration 35, loss = 0.23542181\n",
      "Iteration 36, loss = 0.22361514\n",
      "Iteration 37, loss = 0.21265476\n",
      "Iteration 38, loss = 0.20176955\n",
      "Iteration 39, loss = 0.19142382\n",
      "Iteration 40, loss = 0.18134320\n",
      "Iteration 41, loss = 0.17255560\n",
      "Iteration 42, loss = 0.16391333\n",
      "Iteration 43, loss = 0.15589131\n",
      "Iteration 44, loss = 0.14903591\n",
      "Iteration 45, loss = 0.14207700\n",
      "Iteration 46, loss = 0.13596960\n",
      "Iteration 47, loss = 0.13056632\n",
      "Iteration 48, loss = 0.12510834\n",
      "Iteration 49, loss = 0.12011358\n",
      "Iteration 50, loss = 0.11554574\n",
      "Iteration 51, loss = 0.11144925\n",
      "Iteration 52, loss = 0.10761851\n",
      "Iteration 53, loss = 0.10371527\n",
      "Iteration 54, loss = 0.10054034\n",
      "Iteration 55, loss = 0.09728187\n",
      "Iteration 56, loss = 0.09432609\n",
      "Iteration 57, loss = 0.09114041\n",
      "Iteration 58, loss = 0.08873723\n",
      "Iteration 59, loss = 0.08600681\n",
      "Iteration 60, loss = 0.08352533\n",
      "Iteration 61, loss = 0.08112295\n",
      "Iteration 62, loss = 0.07897452\n",
      "Iteration 63, loss = 0.07692593\n",
      "Iteration 64, loss = 0.07498263\n",
      "Iteration 65, loss = 0.07295144\n",
      "Iteration 66, loss = 0.07132047\n",
      "Iteration 67, loss = 0.06936198\n",
      "Iteration 68, loss = 0.06747057\n",
      "Iteration 69, loss = 0.06594401\n",
      "Iteration 70, loss = 0.06429437\n",
      "Iteration 71, loss = 0.06279979\n",
      "Iteration 72, loss = 0.06136325\n",
      "Iteration 73, loss = 0.06018072\n",
      "Iteration 74, loss = 0.05874838\n",
      "Iteration 75, loss = 0.05731960\n",
      "Iteration 76, loss = 0.05589097\n",
      "Iteration 77, loss = 0.05466441\n",
      "Iteration 78, loss = 0.05349667\n",
      "Iteration 79, loss = 0.05265255\n",
      "Iteration 80, loss = 0.05116921\n",
      "Iteration 81, loss = 0.05008636\n",
      "Iteration 82, loss = 0.04884075\n",
      "Iteration 83, loss = 0.04791799\n",
      "Iteration 84, loss = 0.04691534\n",
      "Iteration 85, loss = 0.04596581\n",
      "Iteration 86, loss = 0.04511176\n",
      "Iteration 87, loss = 0.04426572\n",
      "Iteration 88, loss = 0.04326399\n",
      "Iteration 89, loss = 0.04230190\n",
      "Iteration 90, loss = 0.04157029\n",
      "Iteration 91, loss = 0.04072087\n",
      "Iteration 92, loss = 0.03992244\n",
      "Iteration 93, loss = 0.03916265\n",
      "Iteration 94, loss = 0.03836079\n",
      "Iteration 95, loss = 0.03767211\n",
      "Iteration 96, loss = 0.03694077\n",
      "Iteration 97, loss = 0.03616841\n",
      "Iteration 98, loss = 0.03548527\n",
      "Iteration 99, loss = 0.03508692\n",
      "Iteration 100, loss = 0.03424879\n",
      "Iteration 101, loss = 0.03360565\n",
      "Iteration 102, loss = 0.03302494\n",
      "Iteration 103, loss = 0.03242050\n",
      "Iteration 104, loss = 0.03188943\n",
      "Iteration 105, loss = 0.03129389\n",
      "Iteration 106, loss = 0.03057999\n",
      "Iteration 107, loss = 0.03010038\n",
      "Iteration 108, loss = 0.02962994\n",
      "Iteration 109, loss = 0.02922114\n",
      "Iteration 110, loss = 0.02877076\n",
      "Iteration 111, loss = 0.02818537\n",
      "Iteration 112, loss = 0.02757253\n",
      "Iteration 113, loss = 0.02718501\n",
      "Iteration 114, loss = 0.02671933\n",
      "Iteration 115, loss = 0.02622970\n",
      "Iteration 116, loss = 0.02571128\n",
      "Iteration 117, loss = 0.02537054\n",
      "Iteration 118, loss = 0.02488524\n",
      "Iteration 119, loss = 0.02449442\n",
      "Iteration 120, loss = 0.02413539\n",
      "Iteration 121, loss = 0.02374140\n",
      "Iteration 122, loss = 0.02334360\n",
      "Iteration 123, loss = 0.02297910\n",
      "Iteration 124, loss = 0.02261414\n",
      "Iteration 125, loss = 0.02228820\n",
      "Iteration 126, loss = 0.02193958\n",
      "Iteration 127, loss = 0.02156986\n",
      "Iteration 128, loss = 0.02126518\n",
      "Iteration 129, loss = 0.02089646\n",
      "Iteration 130, loss = 0.02058193\n",
      "Iteration 131, loss = 0.02026282\n",
      "Iteration 132, loss = 0.01993168\n",
      "Iteration 133, loss = 0.01968367\n",
      "Iteration 134, loss = 0.01940379\n",
      "Iteration 135, loss = 0.01905923\n",
      "Iteration 136, loss = 0.01880912\n",
      "Iteration 137, loss = 0.01855877\n",
      "Iteration 138, loss = 0.01827219\n",
      "Iteration 139, loss = 0.01800002\n",
      "Iteration 140, loss = 0.01773870\n",
      "Iteration 141, loss = 0.01749410\n",
      "Iteration 142, loss = 0.01733647\n",
      "Iteration 143, loss = 0.01699501\n",
      "Iteration 144, loss = 0.01685561\n",
      "Iteration 145, loss = 0.01657609\n",
      "Iteration 146, loss = 0.01634913\n",
      "Iteration 147, loss = 0.01613483\n",
      "Iteration 148, loss = 0.01588583\n",
      "Iteration 149, loss = 0.01567741\n",
      "Iteration 150, loss = 0.01547125\n",
      "Iteration 151, loss = 0.01529805\n",
      "Iteration 152, loss = 0.01508358\n",
      "Iteration 153, loss = 0.01488559\n",
      "Iteration 154, loss = 0.01477290\n",
      "Iteration 155, loss = 0.01458145\n",
      "Iteration 156, loss = 0.01434169\n",
      "Iteration 157, loss = 0.01413406\n",
      "Iteration 158, loss = 0.01401910\n",
      "Iteration 159, loss = 0.01378046\n",
      "Iteration 160, loss = 0.01361479\n",
      "Iteration 161, loss = 0.01345675\n",
      "Iteration 162, loss = 0.01329951\n",
      "Iteration 163, loss = 0.01313391\n",
      "Iteration 164, loss = 0.01298393\n",
      "Iteration 165, loss = 0.01284522\n",
      "Iteration 166, loss = 0.01266833\n",
      "Iteration 167, loss = 0.01257898\n",
      "Iteration 168, loss = 0.01238967\n",
      "Iteration 169, loss = 0.01223768\n",
      "Iteration 170, loss = 0.01209781\n",
      "Iteration 171, loss = 0.01196447\n",
      "Iteration 172, loss = 0.01181805\n",
      "Iteration 173, loss = 0.01168826\n",
      "Iteration 174, loss = 0.01154343\n",
      "Iteration 175, loss = 0.01143181\n",
      "Iteration 176, loss = 0.01130684\n",
      "Iteration 177, loss = 0.01120472\n",
      "Iteration 178, loss = 0.01106276\n",
      "Iteration 179, loss = 0.01096167\n",
      "Iteration 180, loss = 0.01082345\n",
      "Iteration 181, loss = 0.01070817\n",
      "Iteration 182, loss = 0.01059563\n",
      "Iteration 183, loss = 0.01047567\n",
      "Iteration 184, loss = 0.01038678\n",
      "Iteration 185, loss = 0.01027120\n",
      "Iteration 186, loss = 0.01017275\n",
      "Iteration 187, loss = 0.01006446\n",
      "Iteration 188, loss = 0.00996606\n",
      "Iteration 189, loss = 0.00986523\n",
      "Iteration 190, loss = 0.00976006\n",
      "Iteration 191, loss = 0.00969016\n",
      "Iteration 192, loss = 0.00959796\n",
      "Iteration 193, loss = 0.00951779\n",
      "Iteration 194, loss = 0.00940795\n",
      "Iteration 195, loss = 0.00930864\n",
      "Iteration 196, loss = 0.00920019\n",
      "Iteration 197, loss = 0.00912330\n",
      "Iteration 198, loss = 0.00902861\n",
      "Iteration 199, loss = 0.00898193\n",
      "Iteration 200, loss = 0.00887339\n",
      "Iteration 201, loss = 0.00880447\n",
      "Iteration 202, loss = 0.00870878\n",
      "Iteration 203, loss = 0.00861534\n",
      "Iteration 204, loss = 0.00855816\n",
      "Iteration 205, loss = 0.00846458\n",
      "Iteration 206, loss = 0.00838439\n",
      "Iteration 207, loss = 0.00831606\n",
      "Iteration 208, loss = 0.00824967\n",
      "Iteration 209, loss = 0.00819169\n",
      "Iteration 210, loss = 0.00809894\n",
      "Iteration 211, loss = 0.00802808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Temps d'execution = 19.994856 seconde\n",
      "\n",
      "[1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "#fit and predict\n",
    "tmps1=time.time()\n",
    "\n",
    "fitting(x_train, y_train, mlp)\n",
    "\n",
    "tmps2=time.time() - tmps1\n",
    "print(\"Temps d'execution = %f seconde\\n\" %tmps2)\n",
    "\n",
    "ypred = predict(x_test, mlp)\n",
    "\n",
    "#test on test set\n",
    "print(ypred[:100])\n",
    "print(y_test[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.89      0.94       160\n",
      "           1       0.98      1.00      0.99       955\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_report = classification_report(y_test, ypred)\n",
    "print(nlp_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'execution = 1.820585 seconde\n",
      "\n",
      "[1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 0]\n",
      "[1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "\n",
    "#fit and predict\n",
    "tmps1=time.time()\n",
    "\n",
    "fitting(x_train, y_train, svclass)\n",
    "\n",
    "tmps2=time.time() - tmps1\n",
    "print(\"Temps d'execution = %f seconde\\n\" %tmps2)\n",
    "\n",
    "\n",
    "ypred = predict(x_test, svclass)\n",
    "\n",
    "#test on test set\n",
    "print(ypred[:50])\n",
    "print(y_test[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.92       160\n",
      "           1       0.97      1.00      0.99       955\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_report = classification_report(y_test, ypred)\n",
    "print(svc_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### You must perform a cross-validation on 10 different learning and testing sets. The seed should be set at 42 and the test set should represent 20% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'execution = 1.076684 seconde\n",
      "\n",
      "Cross-validation F1 ham scores: [0.97672065 0.97163121 0.97380586 0.98330804 0.97531486 0.97052846\n",
      " 0.97782258 0.9721519  0.96390442 0.97192445]\n",
      "\n",
      "\n",
      "Temps d'execution = 0.946598 seconde\n",
      "\n",
      "Cross-validation F1 spam scores: [0.81889764 0.78125    0.81978799 0.86956522 0.8        0.77862595\n",
      " 0.82113821 0.78431373 0.73003802 0.79704797]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y = np.array(df['labelf1ham'])\n",
    "y2 = np.array(df['labelf1spam'])\n",
    "cv = ShuffleSplit(n_splits=10, test_size=.20, random_state=42)\n",
    "cv.get_n_splits(x)\n",
    "\n",
    "tmps1=time.time()\n",
    "\n",
    "scores = cross_val_score(LogReg, x, y, cv=cv, scoring='f1')\n",
    "\n",
    "tmps2=time.time() - tmps1\n",
    "print(\"Temps d'execution = %f seconde\\n\" %tmps2)\n",
    "\n",
    "print(\"Cross-validation F1 ham scores: {}\".format(scores))\n",
    "\n",
    "tmps1=time.time()\n",
    "\n",
    "scores2 = cross_val_score(LogReg, x, y2, cv=cv, scoring='f1')\n",
    "\n",
    "tmps2=time.time() - tmps1\n",
    "print(\"\\n\\nTemps d'execution = %f seconde\\n\" %tmps2)\n",
    "\n",
    "print(\"Cross-validation F1 spam scores: {}\".format(scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPL Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.43194236\n",
      "Iteration 2, loss = 0.39543011\n",
      "Iteration 3, loss = 0.39334119\n",
      "Iteration 4, loss = 0.39270048\n",
      "Iteration 5, loss = 0.39181674\n",
      "Iteration 6, loss = 0.39233062\n",
      "Iteration 7, loss = 0.39026682\n",
      "Iteration 8, loss = 0.38973030\n",
      "Iteration 9, loss = 0.38845797\n",
      "Iteration 10, loss = 0.38800361\n",
      "Iteration 11, loss = 0.38615389\n",
      "Iteration 12, loss = 0.38535544\n",
      "Iteration 13, loss = 0.38363531\n",
      "Iteration 14, loss = 0.38243993\n",
      "Iteration 15, loss = 0.38072588\n",
      "Iteration 16, loss = 0.37806098\n",
      "Iteration 17, loss = 0.37523150\n",
      "Iteration 18, loss = 0.37243347\n",
      "Iteration 19, loss = 0.36979514\n",
      "Iteration 20, loss = 0.36535341\n",
      "Iteration 21, loss = 0.36115647\n",
      "Iteration 22, loss = 0.35651488\n",
      "Iteration 23, loss = 0.35124234\n",
      "Iteration 24, loss = 0.34497465\n",
      "Iteration 25, loss = 0.33879014\n",
      "Iteration 26, loss = 0.33061073\n",
      "Iteration 27, loss = 0.32210616\n",
      "Iteration 28, loss = 0.31359770\n",
      "Iteration 29, loss = 0.30345462\n",
      "Iteration 30, loss = 0.29212299\n",
      "Iteration 31, loss = 0.28102154\n",
      "Iteration 32, loss = 0.26931985\n",
      "Iteration 33, loss = 0.25765286\n",
      "Iteration 34, loss = 0.24554123\n",
      "Iteration 35, loss = 0.23406942\n",
      "Iteration 36, loss = 0.22193053\n",
      "Iteration 37, loss = 0.21074342\n",
      "Iteration 38, loss = 0.20040235\n",
      "Iteration 39, loss = 0.18960107\n",
      "Iteration 40, loss = 0.18052529\n",
      "Iteration 41, loss = 0.17153898\n",
      "Iteration 42, loss = 0.16290887\n",
      "Iteration 43, loss = 0.15520874\n",
      "Iteration 44, loss = 0.14824418\n",
      "Iteration 45, loss = 0.14165049\n",
      "Iteration 46, loss = 0.13665978\n",
      "Iteration 47, loss = 0.13067132\n",
      "Iteration 48, loss = 0.12499356\n",
      "Iteration 49, loss = 0.12011579\n",
      "Iteration 50, loss = 0.11555053\n",
      "Iteration 51, loss = 0.11208359\n",
      "Iteration 52, loss = 0.10740365\n",
      "Iteration 53, loss = 0.10396650\n",
      "Iteration 54, loss = 0.10038098\n",
      "Iteration 55, loss = 0.09729347\n",
      "Iteration 56, loss = 0.09406029\n",
      "Iteration 57, loss = 0.09145729\n",
      "Iteration 58, loss = 0.08857541\n",
      "Iteration 59, loss = 0.08590856\n",
      "Iteration 60, loss = 0.08343677\n",
      "Iteration 61, loss = 0.08130610\n",
      "Iteration 62, loss = 0.07890405\n",
      "Iteration 63, loss = 0.07663227\n",
      "Iteration 64, loss = 0.07437771\n",
      "Iteration 65, loss = 0.07291706\n",
      "Iteration 66, loss = 0.07060686\n",
      "Iteration 67, loss = 0.06881340\n",
      "Iteration 68, loss = 0.06701441\n",
      "Iteration 69, loss = 0.06572084\n",
      "Iteration 70, loss = 0.06384069\n",
      "Iteration 71, loss = 0.06238892\n",
      "Iteration 72, loss = 0.06079745\n",
      "Iteration 73, loss = 0.05963110\n",
      "Iteration 74, loss = 0.05810433\n",
      "Iteration 75, loss = 0.05670852\n",
      "Iteration 76, loss = 0.05548706\n",
      "Iteration 77, loss = 0.05414963\n",
      "Iteration 78, loss = 0.05296385\n",
      "Iteration 79, loss = 0.05175198\n",
      "Iteration 80, loss = 0.05072759\n",
      "Iteration 81, loss = 0.04955291\n",
      "Iteration 82, loss = 0.04866976\n",
      "Iteration 83, loss = 0.04739800\n",
      "Iteration 84, loss = 0.04649136\n",
      "Iteration 85, loss = 0.04545948\n",
      "Iteration 86, loss = 0.04444051\n",
      "Iteration 87, loss = 0.04349110\n",
      "Iteration 88, loss = 0.04268414\n",
      "Iteration 89, loss = 0.04192526\n",
      "Iteration 90, loss = 0.04080321\n",
      "Iteration 91, loss = 0.04010957\n",
      "Iteration 92, loss = 0.03920167\n",
      "Iteration 93, loss = 0.03843779\n",
      "Iteration 94, loss = 0.03764808\n",
      "Iteration 95, loss = 0.03694097\n",
      "Iteration 96, loss = 0.03624637\n",
      "Iteration 97, loss = 0.03552171\n",
      "Iteration 98, loss = 0.03496815\n",
      "Iteration 99, loss = 0.03416535\n",
      "Iteration 100, loss = 0.03356794\n",
      "Iteration 101, loss = 0.03288396\n",
      "Iteration 102, loss = 0.03234167\n",
      "Iteration 103, loss = 0.03162157\n",
      "Iteration 104, loss = 0.03132994\n",
      "Iteration 105, loss = 0.03046973\n",
      "Iteration 106, loss = 0.02997565\n",
      "Iteration 107, loss = 0.02942957\n",
      "Iteration 108, loss = 0.02893981\n",
      "Iteration 109, loss = 0.02833326\n",
      "Iteration 110, loss = 0.02780347\n",
      "Iteration 111, loss = 0.02740497\n",
      "Iteration 112, loss = 0.02678664\n",
      "Iteration 113, loss = 0.02640417\n",
      "Iteration 114, loss = 0.02595891\n",
      "Iteration 115, loss = 0.02545182\n",
      "Iteration 116, loss = 0.02507624\n",
      "Iteration 117, loss = 0.02457908\n",
      "Iteration 118, loss = 0.02418815\n",
      "Iteration 119, loss = 0.02386505\n",
      "Iteration 120, loss = 0.02340600\n",
      "Iteration 121, loss = 0.02300522\n",
      "Iteration 122, loss = 0.02258288\n",
      "Iteration 123, loss = 0.02221918\n",
      "Iteration 124, loss = 0.02188704\n",
      "Iteration 125, loss = 0.02169668\n",
      "Iteration 126, loss = 0.02121601\n",
      "Iteration 127, loss = 0.02085318\n",
      "Iteration 128, loss = 0.02046916\n",
      "Iteration 129, loss = 0.02031133\n",
      "Iteration 130, loss = 0.01987967\n",
      "Iteration 131, loss = 0.01963787\n",
      "Iteration 132, loss = 0.01926832\n",
      "Iteration 133, loss = 0.01896423\n",
      "Iteration 134, loss = 0.01868107\n",
      "Iteration 135, loss = 0.01846915\n",
      "Iteration 136, loss = 0.01814935\n",
      "Iteration 137, loss = 0.01783666\n",
      "Iteration 138, loss = 0.01760187\n",
      "Iteration 139, loss = 0.01733875\n",
      "Iteration 140, loss = 0.01712127\n",
      "Iteration 141, loss = 0.01686065\n",
      "Iteration 142, loss = 0.01661701\n",
      "Iteration 143, loss = 0.01634380\n",
      "Iteration 144, loss = 0.01616952\n",
      "Iteration 145, loss = 0.01591307\n",
      "Iteration 146, loss = 0.01569432\n",
      "Iteration 147, loss = 0.01549305\n",
      "Iteration 148, loss = 0.01530068\n",
      "Iteration 149, loss = 0.01509179\n",
      "Iteration 150, loss = 0.01484409\n",
      "Iteration 151, loss = 0.01474765\n",
      "Iteration 152, loss = 0.01452547\n",
      "Iteration 153, loss = 0.01426544\n",
      "Iteration 154, loss = 0.01406587\n",
      "Iteration 155, loss = 0.01393393\n",
      "Iteration 156, loss = 0.01370400\n",
      "Iteration 157, loss = 0.01355183\n",
      "Iteration 158, loss = 0.01337728\n",
      "Iteration 159, loss = 0.01322266\n",
      "Iteration 160, loss = 0.01303317\n",
      "Iteration 161, loss = 0.01289902\n",
      "Iteration 162, loss = 0.01277646\n",
      "Iteration 163, loss = 0.01259994\n",
      "Iteration 164, loss = 0.01250649\n",
      "Iteration 165, loss = 0.01226837\n",
      "Iteration 166, loss = 0.01211422\n",
      "Iteration 167, loss = 0.01197045\n",
      "Iteration 168, loss = 0.01185088\n",
      "Iteration 169, loss = 0.01181158\n",
      "Iteration 170, loss = 0.01158450\n",
      "Iteration 171, loss = 0.01143836\n",
      "Iteration 172, loss = 0.01129532\n",
      "Iteration 173, loss = 0.01118178\n",
      "Iteration 174, loss = 0.01106271\n",
      "Iteration 175, loss = 0.01098057\n",
      "Iteration 176, loss = 0.01080539\n",
      "Iteration 177, loss = 0.01070755\n",
      "Iteration 178, loss = 0.01060910\n",
      "Iteration 179, loss = 0.01047745\n",
      "Iteration 180, loss = 0.01036626\n",
      "Iteration 181, loss = 0.01025461\n",
      "Iteration 182, loss = 0.01015085\n",
      "Iteration 183, loss = 0.01002514\n",
      "Iteration 184, loss = 0.00995422\n",
      "Iteration 185, loss = 0.00982469\n",
      "Iteration 186, loss = 0.00972325\n",
      "Iteration 187, loss = 0.00963795\n",
      "Iteration 188, loss = 0.00954653\n",
      "Iteration 189, loss = 0.00944068\n",
      "Iteration 190, loss = 0.00933812\n",
      "Iteration 191, loss = 0.00926146\n",
      "Iteration 192, loss = 0.00914660\n",
      "Iteration 193, loss = 0.00908196\n",
      "Iteration 194, loss = 0.00897721\n",
      "Iteration 195, loss = 0.00890074\n",
      "Iteration 196, loss = 0.00882243\n",
      "Iteration 197, loss = 0.00878548\n",
      "Iteration 198, loss = 0.00864687\n",
      "Iteration 199, loss = 0.00855933\n",
      "Iteration 200, loss = 0.00847455\n",
      "Iteration 201, loss = 0.00840071\n",
      "Iteration 202, loss = 0.00832679\n",
      "Iteration 203, loss = 0.00824985\n",
      "Iteration 204, loss = 0.00818304\n",
      "Iteration 205, loss = 0.00810459\n",
      "Iteration 206, loss = 0.00804014\n",
      "Iteration 207, loss = 0.00795634\n",
      "Iteration 208, loss = 0.00791676\n",
      "Iteration 209, loss = 0.00782043\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42676963\n",
      "Iteration 2, loss = 0.39345846\n",
      "Iteration 3, loss = 0.39194434\n",
      "Iteration 4, loss = 0.39113872\n",
      "Iteration 5, loss = 0.38978726\n",
      "Iteration 6, loss = 0.38900435\n",
      "Iteration 7, loss = 0.38894710\n",
      "Iteration 8, loss = 0.38727866\n",
      "Iteration 9, loss = 0.38719805\n",
      "Iteration 10, loss = 0.38576281\n",
      "Iteration 11, loss = 0.38416196\n",
      "Iteration 12, loss = 0.38277747\n",
      "Iteration 13, loss = 0.38134164\n",
      "Iteration 14, loss = 0.37944293\n",
      "Iteration 15, loss = 0.37793002\n",
      "Iteration 16, loss = 0.37552308\n",
      "Iteration 17, loss = 0.37334152\n",
      "Iteration 18, loss = 0.36997204\n",
      "Iteration 19, loss = 0.36689767\n",
      "Iteration 20, loss = 0.36284671\n",
      "Iteration 21, loss = 0.35934842\n",
      "Iteration 22, loss = 0.35381305\n",
      "Iteration 23, loss = 0.34809993\n",
      "Iteration 24, loss = 0.34169028\n",
      "Iteration 25, loss = 0.33521683\n",
      "Iteration 26, loss = 0.32743727\n",
      "Iteration 27, loss = 0.31878363\n",
      "Iteration 28, loss = 0.30970684\n",
      "Iteration 29, loss = 0.29987110\n",
      "Iteration 30, loss = 0.28866826\n",
      "Iteration 31, loss = 0.27757210\n",
      "Iteration 32, loss = 0.26556883\n",
      "Iteration 33, loss = 0.25387357\n",
      "Iteration 34, loss = 0.24129393\n",
      "Iteration 35, loss = 0.22942148\n",
      "Iteration 36, loss = 0.21754722\n",
      "Iteration 37, loss = 0.20598632\n",
      "Iteration 38, loss = 0.19500608\n",
      "Iteration 39, loss = 0.18492145\n",
      "Iteration 40, loss = 0.17562030\n",
      "Iteration 41, loss = 0.16713139\n",
      "Iteration 42, loss = 0.15826620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.15053428\n",
      "Iteration 44, loss = 0.14448444\n",
      "Iteration 45, loss = 0.13777296\n",
      "Iteration 46, loss = 0.13144773\n",
      "Iteration 47, loss = 0.12611528\n",
      "Iteration 48, loss = 0.12080363\n",
      "Iteration 49, loss = 0.11653013\n",
      "Iteration 50, loss = 0.11222615\n",
      "Iteration 51, loss = 0.10766197\n",
      "Iteration 52, loss = 0.10397992\n",
      "Iteration 53, loss = 0.10033255\n",
      "Iteration 54, loss = 0.09696742\n",
      "Iteration 55, loss = 0.09372306\n",
      "Iteration 56, loss = 0.09112665\n",
      "Iteration 57, loss = 0.08811972\n",
      "Iteration 58, loss = 0.08538769\n",
      "Iteration 59, loss = 0.08298072\n",
      "Iteration 60, loss = 0.08050789\n",
      "Iteration 61, loss = 0.07837040\n",
      "Iteration 62, loss = 0.07623802\n",
      "Iteration 63, loss = 0.07420103\n",
      "Iteration 64, loss = 0.07195963\n",
      "Iteration 65, loss = 0.07017222\n",
      "Iteration 66, loss = 0.06834547\n",
      "Iteration 67, loss = 0.06661308\n",
      "Iteration 68, loss = 0.06526048\n",
      "Iteration 69, loss = 0.06354415\n",
      "Iteration 70, loss = 0.06214424\n",
      "Iteration 71, loss = 0.06047215\n",
      "Iteration 72, loss = 0.05902576\n",
      "Iteration 73, loss = 0.05792550\n",
      "Iteration 74, loss = 0.05647038\n",
      "Iteration 75, loss = 0.05494189\n",
      "Iteration 76, loss = 0.05409795\n",
      "Iteration 77, loss = 0.05268894\n",
      "Iteration 78, loss = 0.05173389\n",
      "Iteration 79, loss = 0.05036000\n",
      "Iteration 80, loss = 0.04959291\n",
      "Iteration 81, loss = 0.04823201\n",
      "Iteration 82, loss = 0.04719792\n",
      "Iteration 83, loss = 0.04610724\n",
      "Iteration 84, loss = 0.04540230\n",
      "Iteration 85, loss = 0.04432961\n",
      "Iteration 86, loss = 0.04342166\n",
      "Iteration 87, loss = 0.04257203\n",
      "Iteration 88, loss = 0.04166329\n",
      "Iteration 89, loss = 0.04084002\n",
      "Iteration 90, loss = 0.04006037\n",
      "Iteration 91, loss = 0.03921716\n",
      "Iteration 92, loss = 0.03857654\n",
      "Iteration 93, loss = 0.03768642\n",
      "Iteration 94, loss = 0.03707857\n",
      "Iteration 95, loss = 0.03627244\n",
      "Iteration 96, loss = 0.03555627\n",
      "Iteration 97, loss = 0.03481150\n",
      "Iteration 98, loss = 0.03448289\n",
      "Iteration 99, loss = 0.03363635\n",
      "Iteration 100, loss = 0.03293708\n",
      "Iteration 101, loss = 0.03243857\n",
      "Iteration 102, loss = 0.03174483\n",
      "Iteration 103, loss = 0.03117226\n",
      "Iteration 104, loss = 0.03053127\n",
      "Iteration 105, loss = 0.03000740\n",
      "Iteration 106, loss = 0.02946404\n",
      "Iteration 107, loss = 0.02893049\n",
      "Iteration 108, loss = 0.02842628\n",
      "Iteration 109, loss = 0.02797291\n",
      "Iteration 110, loss = 0.02751278\n",
      "Iteration 111, loss = 0.02698180\n",
      "Iteration 112, loss = 0.02644587\n",
      "Iteration 113, loss = 0.02615557\n",
      "Iteration 114, loss = 0.02564094\n",
      "Iteration 115, loss = 0.02516955\n",
      "Iteration 116, loss = 0.02482720\n",
      "Iteration 117, loss = 0.02429089\n",
      "Iteration 118, loss = 0.02387631\n",
      "Iteration 119, loss = 0.02351405\n",
      "Iteration 120, loss = 0.02311901\n",
      "Iteration 121, loss = 0.02271399\n",
      "Iteration 122, loss = 0.02236472\n",
      "Iteration 123, loss = 0.02201183\n",
      "Iteration 124, loss = 0.02168528\n",
      "Iteration 125, loss = 0.02125059\n",
      "Iteration 126, loss = 0.02093654\n",
      "Iteration 127, loss = 0.02065100\n",
      "Iteration 128, loss = 0.02030673\n",
      "Iteration 129, loss = 0.01994894\n",
      "Iteration 130, loss = 0.01976601\n",
      "Iteration 131, loss = 0.01935158\n",
      "Iteration 132, loss = 0.01907215\n",
      "Iteration 133, loss = 0.01879947\n",
      "Iteration 134, loss = 0.01852256\n",
      "Iteration 135, loss = 0.01828138\n",
      "Iteration 136, loss = 0.01793995\n",
      "Iteration 137, loss = 0.01775294\n",
      "Iteration 138, loss = 0.01745946\n",
      "Iteration 139, loss = 0.01718313\n",
      "Iteration 140, loss = 0.01698021\n",
      "Iteration 141, loss = 0.01669813\n",
      "Iteration 142, loss = 0.01648294\n",
      "Iteration 143, loss = 0.01623655\n",
      "Iteration 144, loss = 0.01608282\n",
      "Iteration 145, loss = 0.01583457\n",
      "Iteration 146, loss = 0.01558930\n",
      "Iteration 147, loss = 0.01540976\n",
      "Iteration 148, loss = 0.01524326\n",
      "Iteration 149, loss = 0.01500673\n",
      "Iteration 150, loss = 0.01481558\n",
      "Iteration 151, loss = 0.01462471\n",
      "Iteration 152, loss = 0.01446310\n",
      "Iteration 153, loss = 0.01420421\n",
      "Iteration 154, loss = 0.01414339\n",
      "Iteration 155, loss = 0.01389097\n",
      "Iteration 156, loss = 0.01367498\n",
      "Iteration 157, loss = 0.01355616\n",
      "Iteration 158, loss = 0.01336142\n",
      "Iteration 159, loss = 0.01317484\n",
      "Iteration 160, loss = 0.01302252\n",
      "Iteration 161, loss = 0.01287856\n",
      "Iteration 162, loss = 0.01269970\n",
      "Iteration 163, loss = 0.01256937\n",
      "Iteration 164, loss = 0.01239044\n",
      "Iteration 165, loss = 0.01226160\n",
      "Iteration 166, loss = 0.01208389\n",
      "Iteration 167, loss = 0.01196569\n",
      "Iteration 168, loss = 0.01181461\n",
      "Iteration 169, loss = 0.01169626\n",
      "Iteration 170, loss = 0.01154708\n",
      "Iteration 171, loss = 0.01141282\n",
      "Iteration 172, loss = 0.01130806\n",
      "Iteration 173, loss = 0.01115317\n",
      "Iteration 174, loss = 0.01102245\n",
      "Iteration 175, loss = 0.01091735\n",
      "Iteration 176, loss = 0.01077089\n",
      "Iteration 177, loss = 0.01065061\n",
      "Iteration 178, loss = 0.01054388\n",
      "Iteration 179, loss = 0.01043321\n",
      "Iteration 180, loss = 0.01032435\n",
      "Iteration 181, loss = 0.01021783\n",
      "Iteration 182, loss = 0.01010554\n",
      "Iteration 183, loss = 0.01001251\n",
      "Iteration 184, loss = 0.00991207\n",
      "Iteration 185, loss = 0.00985377\n",
      "Iteration 186, loss = 0.00971190\n",
      "Iteration 187, loss = 0.00963729\n",
      "Iteration 188, loss = 0.00951642\n",
      "Iteration 189, loss = 0.00941981\n",
      "Iteration 190, loss = 0.00932474\n",
      "Iteration 191, loss = 0.00923472\n",
      "Iteration 192, loss = 0.00914331\n",
      "Iteration 193, loss = 0.00906142\n",
      "Iteration 194, loss = 0.00898062\n",
      "Iteration 195, loss = 0.00888803\n",
      "Iteration 196, loss = 0.00883285\n",
      "Iteration 197, loss = 0.00873538\n",
      "Iteration 198, loss = 0.00863607\n",
      "Iteration 199, loss = 0.00857845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42298761\n",
      "Iteration 2, loss = 0.38759526\n",
      "Iteration 3, loss = 0.38645492\n",
      "Iteration 4, loss = 0.38616411\n",
      "Iteration 5, loss = 0.38571388\n",
      "Iteration 6, loss = 0.38495523\n",
      "Iteration 7, loss = 0.38365548\n",
      "Iteration 8, loss = 0.38316407\n",
      "Iteration 9, loss = 0.38204840\n",
      "Iteration 10, loss = 0.38106333\n",
      "Iteration 11, loss = 0.37988215\n",
      "Iteration 12, loss = 0.37852907\n",
      "Iteration 13, loss = 0.37735184\n",
      "Iteration 14, loss = 0.37624897\n",
      "Iteration 15, loss = 0.37388876\n",
      "Iteration 16, loss = 0.37158828\n",
      "Iteration 17, loss = 0.36998939\n",
      "Iteration 18, loss = 0.36666678\n",
      "Iteration 19, loss = 0.36368788\n",
      "Iteration 20, loss = 0.35991926\n",
      "Iteration 21, loss = 0.35680702\n",
      "Iteration 22, loss = 0.35173713\n",
      "Iteration 23, loss = 0.34653951\n",
      "Iteration 24, loss = 0.34085335\n",
      "Iteration 25, loss = 0.33530499\n",
      "Iteration 26, loss = 0.32747844\n",
      "Iteration 27, loss = 0.31987484\n",
      "Iteration 28, loss = 0.31155213\n",
      "Iteration 29, loss = 0.30178270\n",
      "Iteration 30, loss = 0.29149086\n",
      "Iteration 31, loss = 0.28195780\n",
      "Iteration 32, loss = 0.27094387\n",
      "Iteration 33, loss = 0.25882911\n",
      "Iteration 34, loss = 0.24730839\n",
      "Iteration 35, loss = 0.23570797\n",
      "Iteration 36, loss = 0.22407710\n",
      "Iteration 37, loss = 0.21310863\n",
      "Iteration 38, loss = 0.20244558\n",
      "Iteration 39, loss = 0.19235941\n",
      "Iteration 40, loss = 0.18302960\n",
      "Iteration 41, loss = 0.17397602\n",
      "Iteration 42, loss = 0.16594215\n",
      "Iteration 43, loss = 0.15781051\n",
      "Iteration 44, loss = 0.15086626\n",
      "Iteration 45, loss = 0.14401270\n",
      "Iteration 46, loss = 0.13781199\n",
      "Iteration 47, loss = 0.13200181\n",
      "Iteration 48, loss = 0.12717131\n",
      "Iteration 49, loss = 0.12251688\n",
      "Iteration 50, loss = 0.11752589\n",
      "Iteration 51, loss = 0.11316651\n",
      "Iteration 52, loss = 0.10939588\n",
      "Iteration 53, loss = 0.10552244\n",
      "Iteration 54, loss = 0.10193837\n",
      "Iteration 55, loss = 0.09860609\n",
      "Iteration 56, loss = 0.09574982\n",
      "Iteration 57, loss = 0.09250490\n",
      "Iteration 58, loss = 0.08990922\n",
      "Iteration 59, loss = 0.08724387\n",
      "Iteration 60, loss = 0.08462673\n",
      "Iteration 61, loss = 0.08228823\n",
      "Iteration 62, loss = 0.08029914\n",
      "Iteration 63, loss = 0.07773721\n",
      "Iteration 64, loss = 0.07567174\n",
      "Iteration 65, loss = 0.07381674\n",
      "Iteration 66, loss = 0.07159465\n",
      "Iteration 67, loss = 0.06979651\n",
      "Iteration 68, loss = 0.06803899\n",
      "Iteration 69, loss = 0.06634830\n",
      "Iteration 70, loss = 0.06464277\n",
      "Iteration 71, loss = 0.06326771\n",
      "Iteration 72, loss = 0.06159375\n",
      "Iteration 73, loss = 0.06041539\n",
      "Iteration 74, loss = 0.05870823\n",
      "Iteration 75, loss = 0.05738466\n",
      "Iteration 76, loss = 0.05599847\n",
      "Iteration 77, loss = 0.05511683\n",
      "Iteration 78, loss = 0.05359165\n",
      "Iteration 79, loss = 0.05243204\n",
      "Iteration 80, loss = 0.05123420\n",
      "Iteration 81, loss = 0.05020124\n",
      "Iteration 82, loss = 0.04899862\n",
      "Iteration 83, loss = 0.04800883\n",
      "Iteration 84, loss = 0.04691570\n",
      "Iteration 85, loss = 0.04622039\n",
      "Iteration 86, loss = 0.04500101\n",
      "Iteration 87, loss = 0.04415035\n",
      "Iteration 88, loss = 0.04314505\n",
      "Iteration 89, loss = 0.04228126\n",
      "Iteration 90, loss = 0.04135681\n",
      "Iteration 91, loss = 0.04058007\n",
      "Iteration 92, loss = 0.03969839\n",
      "Iteration 93, loss = 0.03896505\n",
      "Iteration 94, loss = 0.03826235\n",
      "Iteration 95, loss = 0.03752728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 96, loss = 0.03669372\n",
      "Iteration 97, loss = 0.03612510\n",
      "Iteration 98, loss = 0.03555505\n",
      "Iteration 99, loss = 0.03480135\n",
      "Iteration 100, loss = 0.03397031\n",
      "Iteration 101, loss = 0.03343612\n",
      "Iteration 102, loss = 0.03272065\n",
      "Iteration 103, loss = 0.03262137\n",
      "Iteration 104, loss = 0.03149592\n",
      "Iteration 105, loss = 0.03099573\n",
      "Iteration 106, loss = 0.03038735\n",
      "Iteration 107, loss = 0.02977484\n",
      "Iteration 108, loss = 0.02926713\n",
      "Iteration 109, loss = 0.02875550\n",
      "Iteration 110, loss = 0.02838310\n",
      "Iteration 111, loss = 0.02773930\n",
      "Iteration 112, loss = 0.02731201\n",
      "Iteration 113, loss = 0.02687664\n",
      "Iteration 114, loss = 0.02631522\n",
      "Iteration 115, loss = 0.02582235\n",
      "Iteration 116, loss = 0.02540720\n",
      "Iteration 117, loss = 0.02495583\n",
      "Iteration 118, loss = 0.02449829\n",
      "Iteration 119, loss = 0.02418542\n",
      "Iteration 120, loss = 0.02372077\n",
      "Iteration 121, loss = 0.02341825\n",
      "Iteration 122, loss = 0.02295811\n",
      "Iteration 123, loss = 0.02250185\n",
      "Iteration 124, loss = 0.02216133\n",
      "Iteration 125, loss = 0.02177877\n",
      "Iteration 126, loss = 0.02147120\n",
      "Iteration 127, loss = 0.02117283\n",
      "Iteration 128, loss = 0.02087748\n",
      "Iteration 129, loss = 0.02045588\n",
      "Iteration 130, loss = 0.02013872\n",
      "Iteration 131, loss = 0.01999106\n",
      "Iteration 132, loss = 0.01953990\n",
      "Iteration 133, loss = 0.01920503\n",
      "Iteration 134, loss = 0.01892971\n",
      "Iteration 135, loss = 0.01862378\n",
      "Iteration 136, loss = 0.01837475\n",
      "Iteration 137, loss = 0.01809318\n",
      "Iteration 138, loss = 0.01782708\n",
      "Iteration 139, loss = 0.01759040\n",
      "Iteration 140, loss = 0.01728538\n",
      "Iteration 141, loss = 0.01710128\n",
      "Iteration 142, loss = 0.01679066\n",
      "Iteration 143, loss = 0.01657363\n",
      "Iteration 144, loss = 0.01633638\n",
      "Iteration 145, loss = 0.01611971\n",
      "Iteration 146, loss = 0.01587432\n",
      "Iteration 147, loss = 0.01564869\n",
      "Iteration 148, loss = 0.01545097\n",
      "Iteration 149, loss = 0.01525558\n",
      "Iteration 150, loss = 0.01501777\n",
      "Iteration 151, loss = 0.01481580\n",
      "Iteration 152, loss = 0.01461422\n",
      "Iteration 153, loss = 0.01439213\n",
      "Iteration 154, loss = 0.01423219\n",
      "Iteration 155, loss = 0.01403683\n",
      "Iteration 156, loss = 0.01395668\n",
      "Iteration 157, loss = 0.01376903\n",
      "Iteration 158, loss = 0.01352470\n",
      "Iteration 159, loss = 0.01331880\n",
      "Iteration 160, loss = 0.01316575\n",
      "Iteration 161, loss = 0.01299433\n",
      "Iteration 162, loss = 0.01283182\n",
      "Iteration 163, loss = 0.01268192\n",
      "Iteration 164, loss = 0.01254602\n",
      "Iteration 165, loss = 0.01241932\n",
      "Iteration 166, loss = 0.01221622\n",
      "Iteration 167, loss = 0.01206220\n",
      "Iteration 168, loss = 0.01193569\n",
      "Iteration 169, loss = 0.01178494\n",
      "Iteration 170, loss = 0.01165842\n",
      "Iteration 171, loss = 0.01151687\n",
      "Iteration 172, loss = 0.01139943\n",
      "Iteration 173, loss = 0.01125255\n",
      "Iteration 174, loss = 0.01112014\n",
      "Iteration 175, loss = 0.01100376\n",
      "Iteration 176, loss = 0.01087789\n",
      "Iteration 177, loss = 0.01083872\n",
      "Iteration 178, loss = 0.01064885\n",
      "Iteration 179, loss = 0.01054772\n",
      "Iteration 180, loss = 0.01042263\n",
      "Iteration 181, loss = 0.01029974\n",
      "Iteration 182, loss = 0.01019117\n",
      "Iteration 183, loss = 0.01008768\n",
      "Iteration 184, loss = 0.00999450\n",
      "Iteration 185, loss = 0.00988583\n",
      "Iteration 186, loss = 0.00984220\n",
      "Iteration 187, loss = 0.00967413\n",
      "Iteration 188, loss = 0.00960024\n",
      "Iteration 189, loss = 0.00948030\n",
      "Iteration 190, loss = 0.00939190\n",
      "Iteration 191, loss = 0.00929560\n",
      "Iteration 192, loss = 0.00919980\n",
      "Iteration 193, loss = 0.00910851\n",
      "Iteration 194, loss = 0.00902262\n",
      "Iteration 195, loss = 0.00893527\n",
      "Iteration 196, loss = 0.00883847\n",
      "Iteration 197, loss = 0.00876051\n",
      "Iteration 198, loss = 0.00867625\n",
      "Iteration 199, loss = 0.00859539\n",
      "Iteration 200, loss = 0.00851846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42792311\n",
      "Iteration 2, loss = 0.39790658\n",
      "Iteration 3, loss = 0.39700202\n",
      "Iteration 4, loss = 0.39753295\n",
      "Iteration 5, loss = 0.39598525\n",
      "Iteration 6, loss = 0.39489672\n",
      "Iteration 7, loss = 0.39435339\n",
      "Iteration 8, loss = 0.39336391\n",
      "Iteration 9, loss = 0.39250204\n",
      "Iteration 10, loss = 0.39169602\n",
      "Iteration 11, loss = 0.39025813\n",
      "Iteration 12, loss = 0.38913360\n",
      "Iteration 13, loss = 0.38759770\n",
      "Iteration 14, loss = 0.38564612\n",
      "Iteration 15, loss = 0.38429003\n",
      "Iteration 16, loss = 0.38172299\n",
      "Iteration 17, loss = 0.37897471\n",
      "Iteration 18, loss = 0.37617242\n",
      "Iteration 19, loss = 0.37240222\n",
      "Iteration 20, loss = 0.36877326\n",
      "Iteration 21, loss = 0.36411274\n",
      "Iteration 22, loss = 0.35982978\n",
      "Iteration 23, loss = 0.35398268\n",
      "Iteration 24, loss = 0.34763541\n",
      "Iteration 25, loss = 0.34122387\n",
      "Iteration 26, loss = 0.33295783\n",
      "Iteration 27, loss = 0.32404331\n",
      "Iteration 28, loss = 0.31488604\n",
      "Iteration 29, loss = 0.30483851\n",
      "Iteration 30, loss = 0.29397084\n",
      "Iteration 31, loss = 0.28289279\n",
      "Iteration 32, loss = 0.27051595\n",
      "Iteration 33, loss = 0.25857582\n",
      "Iteration 34, loss = 0.24644961\n",
      "Iteration 35, loss = 0.23464125\n",
      "Iteration 36, loss = 0.22266695\n",
      "Iteration 37, loss = 0.21163529\n",
      "Iteration 38, loss = 0.20063599\n",
      "Iteration 39, loss = 0.19098894\n",
      "Iteration 40, loss = 0.18111420\n",
      "Iteration 41, loss = 0.17232803\n",
      "Iteration 42, loss = 0.16463618\n",
      "Iteration 43, loss = 0.15691996\n",
      "Iteration 44, loss = 0.14987018\n",
      "Iteration 45, loss = 0.14328072\n",
      "Iteration 46, loss = 0.13745288\n",
      "Iteration 47, loss = 0.13177763\n",
      "Iteration 48, loss = 0.12672108\n",
      "Iteration 49, loss = 0.12198899\n",
      "Iteration 50, loss = 0.11774160\n",
      "Iteration 51, loss = 0.11324194\n",
      "Iteration 52, loss = 0.10941847\n",
      "Iteration 53, loss = 0.10578727\n",
      "Iteration 54, loss = 0.10257538\n",
      "Iteration 55, loss = 0.09916007\n",
      "Iteration 56, loss = 0.09613884\n",
      "Iteration 57, loss = 0.09317072\n",
      "Iteration 58, loss = 0.09057453\n",
      "Iteration 59, loss = 0.08789381\n",
      "Iteration 60, loss = 0.08541400\n",
      "Iteration 61, loss = 0.08370457\n",
      "Iteration 62, loss = 0.08156558\n",
      "Iteration 63, loss = 0.07889206\n",
      "Iteration 64, loss = 0.07673191\n",
      "Iteration 65, loss = 0.07464026\n",
      "Iteration 66, loss = 0.07278179\n",
      "Iteration 67, loss = 0.07094552\n",
      "Iteration 68, loss = 0.06909204\n",
      "Iteration 69, loss = 0.06740395\n",
      "Iteration 70, loss = 0.06632365\n",
      "Iteration 71, loss = 0.06432593\n",
      "Iteration 72, loss = 0.06267486\n",
      "Iteration 73, loss = 0.06123761\n",
      "Iteration 74, loss = 0.05992421\n",
      "Iteration 75, loss = 0.05863159\n",
      "Iteration 76, loss = 0.05732186\n",
      "Iteration 77, loss = 0.05607583\n",
      "Iteration 78, loss = 0.05463749\n",
      "Iteration 79, loss = 0.05350347\n",
      "Iteration 80, loss = 0.05247330\n",
      "Iteration 81, loss = 0.05121142\n",
      "Iteration 82, loss = 0.05003912\n",
      "Iteration 83, loss = 0.04900266\n",
      "Iteration 84, loss = 0.04801051\n",
      "Iteration 85, loss = 0.04689668\n",
      "Iteration 86, loss = 0.04647962\n",
      "Iteration 87, loss = 0.04514660\n",
      "Iteration 88, loss = 0.04421683\n",
      "Iteration 89, loss = 0.04322939\n",
      "Iteration 90, loss = 0.04236689\n",
      "Iteration 91, loss = 0.04159923\n",
      "Iteration 92, loss = 0.04089877\n",
      "Iteration 93, loss = 0.04003093\n",
      "Iteration 94, loss = 0.03921769\n",
      "Iteration 95, loss = 0.03842959\n",
      "Iteration 96, loss = 0.03764343\n",
      "Iteration 97, loss = 0.03737187\n",
      "Iteration 98, loss = 0.03626657\n",
      "Iteration 99, loss = 0.03561925\n",
      "Iteration 100, loss = 0.03489188\n",
      "Iteration 101, loss = 0.03419463\n",
      "Iteration 102, loss = 0.03361196\n",
      "Iteration 103, loss = 0.03296909\n",
      "Iteration 104, loss = 0.03236733\n",
      "Iteration 105, loss = 0.03178169\n",
      "Iteration 106, loss = 0.03132444\n",
      "Iteration 107, loss = 0.03066523\n",
      "Iteration 108, loss = 0.03008337\n",
      "Iteration 109, loss = 0.02966814\n",
      "Iteration 110, loss = 0.02911925\n",
      "Iteration 111, loss = 0.02855928\n",
      "Iteration 112, loss = 0.02803029\n",
      "Iteration 113, loss = 0.02759420\n",
      "Iteration 114, loss = 0.02722964\n",
      "Iteration 115, loss = 0.02667894\n",
      "Iteration 116, loss = 0.02624411\n",
      "Iteration 117, loss = 0.02589087\n",
      "Iteration 118, loss = 0.02547243\n",
      "Iteration 119, loss = 0.02489192\n",
      "Iteration 120, loss = 0.02458766\n",
      "Iteration 121, loss = 0.02417088\n",
      "Iteration 122, loss = 0.02370433\n",
      "Iteration 123, loss = 0.02332618\n",
      "Iteration 124, loss = 0.02295918\n",
      "Iteration 125, loss = 0.02258253\n",
      "Iteration 126, loss = 0.02226158\n",
      "Iteration 127, loss = 0.02185581\n",
      "Iteration 128, loss = 0.02152195\n",
      "Iteration 129, loss = 0.02118339\n",
      "Iteration 130, loss = 0.02092154\n",
      "Iteration 131, loss = 0.02057518\n",
      "Iteration 132, loss = 0.02028556\n",
      "Iteration 133, loss = 0.01994685\n",
      "Iteration 134, loss = 0.01971218\n",
      "Iteration 135, loss = 0.01935275\n",
      "Iteration 136, loss = 0.01912711\n",
      "Iteration 137, loss = 0.01886572\n",
      "Iteration 138, loss = 0.01853958\n",
      "Iteration 139, loss = 0.01831274\n",
      "Iteration 140, loss = 0.01814368\n",
      "Iteration 141, loss = 0.01783576\n",
      "Iteration 142, loss = 0.01763898\n",
      "Iteration 143, loss = 0.01727833\n",
      "Iteration 144, loss = 0.01705038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 145, loss = 0.01677209\n",
      "Iteration 146, loss = 0.01652914\n",
      "Iteration 147, loss = 0.01631801\n",
      "Iteration 148, loss = 0.01613669\n",
      "Iteration 149, loss = 0.01588578\n",
      "Iteration 150, loss = 0.01565499\n",
      "Iteration 151, loss = 0.01546891\n",
      "Iteration 152, loss = 0.01527957\n",
      "Iteration 153, loss = 0.01514381\n",
      "Iteration 154, loss = 0.01488513\n",
      "Iteration 155, loss = 0.01471897\n",
      "Iteration 156, loss = 0.01455224\n",
      "Iteration 157, loss = 0.01432038\n",
      "Iteration 158, loss = 0.01417926\n",
      "Iteration 159, loss = 0.01420907\n",
      "Iteration 160, loss = 0.01386139\n",
      "Iteration 161, loss = 0.01362813\n",
      "Iteration 162, loss = 0.01347646\n",
      "Iteration 163, loss = 0.01331559\n",
      "Iteration 164, loss = 0.01316342\n",
      "Iteration 165, loss = 0.01297093\n",
      "Iteration 166, loss = 0.01281828\n",
      "Iteration 167, loss = 0.01268339\n",
      "Iteration 168, loss = 0.01253982\n",
      "Iteration 169, loss = 0.01239743\n",
      "Iteration 170, loss = 0.01224194\n",
      "Iteration 171, loss = 0.01210036\n",
      "Iteration 172, loss = 0.01196727\n",
      "Iteration 173, loss = 0.01181976\n",
      "Iteration 174, loss = 0.01169488\n",
      "Iteration 175, loss = 0.01156769\n",
      "Iteration 176, loss = 0.01143816\n",
      "Iteration 177, loss = 0.01132479\n",
      "Iteration 178, loss = 0.01119616\n",
      "Iteration 179, loss = 0.01106486\n",
      "Iteration 180, loss = 0.01093482\n",
      "Iteration 181, loss = 0.01083428\n",
      "Iteration 182, loss = 0.01073090\n",
      "Iteration 183, loss = 0.01062125\n",
      "Iteration 184, loss = 0.01050826\n",
      "Iteration 185, loss = 0.01038531\n",
      "Iteration 186, loss = 0.01028346\n",
      "Iteration 187, loss = 0.01018132\n",
      "Iteration 188, loss = 0.01007315\n",
      "Iteration 189, loss = 0.00997757\n",
      "Iteration 190, loss = 0.00987050\n",
      "Iteration 191, loss = 0.00977350\n",
      "Iteration 192, loss = 0.00967815\n",
      "Iteration 193, loss = 0.00959742\n",
      "Iteration 194, loss = 0.00948249\n",
      "Iteration 195, loss = 0.00940542\n",
      "Iteration 196, loss = 0.00931573\n",
      "Iteration 197, loss = 0.00922877\n",
      "Iteration 198, loss = 0.00915562\n",
      "Iteration 199, loss = 0.00905916\n",
      "Iteration 200, loss = 0.00897067\n",
      "Iteration 201, loss = 0.00889442\n",
      "Iteration 202, loss = 0.00881000\n",
      "Iteration 203, loss = 0.00872401\n",
      "Iteration 204, loss = 0.00865448\n",
      "Iteration 205, loss = 0.00857810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42762754\n",
      "Iteration 2, loss = 0.39760617\n",
      "Iteration 3, loss = 0.39505619\n",
      "Iteration 4, loss = 0.39498337\n",
      "Iteration 5, loss = 0.39459893\n",
      "Iteration 6, loss = 0.39300961\n",
      "Iteration 7, loss = 0.39261832\n",
      "Iteration 8, loss = 0.39150020\n",
      "Iteration 9, loss = 0.39043610\n",
      "Iteration 10, loss = 0.39012838\n",
      "Iteration 11, loss = 0.38906001\n",
      "Iteration 12, loss = 0.38698809\n",
      "Iteration 13, loss = 0.38576493\n",
      "Iteration 14, loss = 0.38369594\n",
      "Iteration 15, loss = 0.38137034\n",
      "Iteration 16, loss = 0.37936742\n",
      "Iteration 17, loss = 0.37627157\n",
      "Iteration 18, loss = 0.37338263\n",
      "Iteration 19, loss = 0.36968601\n",
      "Iteration 20, loss = 0.36578629\n",
      "Iteration 21, loss = 0.36144195\n",
      "Iteration 22, loss = 0.35589611\n",
      "Iteration 23, loss = 0.35027547\n",
      "Iteration 24, loss = 0.34358826\n",
      "Iteration 25, loss = 0.33621072\n",
      "Iteration 26, loss = 0.32829753\n",
      "Iteration 27, loss = 0.31906895\n",
      "Iteration 28, loss = 0.30945027\n",
      "Iteration 29, loss = 0.29880413\n",
      "Iteration 30, loss = 0.28778922\n",
      "Iteration 31, loss = 0.27574706\n",
      "Iteration 32, loss = 0.26335650\n",
      "Iteration 33, loss = 0.25086084\n",
      "Iteration 34, loss = 0.23860459\n",
      "Iteration 35, loss = 0.22664890\n",
      "Iteration 36, loss = 0.21467474\n",
      "Iteration 37, loss = 0.20361032\n",
      "Iteration 38, loss = 0.19280248\n",
      "Iteration 39, loss = 0.18303660\n",
      "Iteration 40, loss = 0.17339509\n",
      "Iteration 41, loss = 0.16466564\n",
      "Iteration 42, loss = 0.15693051\n",
      "Iteration 43, loss = 0.14951551\n",
      "Iteration 44, loss = 0.14306918\n",
      "Iteration 45, loss = 0.13660258\n",
      "Iteration 46, loss = 0.13096478\n",
      "Iteration 47, loss = 0.12578869\n",
      "Iteration 48, loss = 0.12049442\n",
      "Iteration 49, loss = 0.11584978\n",
      "Iteration 50, loss = 0.11172551\n",
      "Iteration 51, loss = 0.10768887\n",
      "Iteration 52, loss = 0.10438130\n",
      "Iteration 53, loss = 0.10069597\n",
      "Iteration 54, loss = 0.09720644\n",
      "Iteration 55, loss = 0.09458735\n",
      "Iteration 56, loss = 0.09125092\n",
      "Iteration 57, loss = 0.08825763\n",
      "Iteration 58, loss = 0.08587962\n",
      "Iteration 59, loss = 0.08317837\n",
      "Iteration 60, loss = 0.08071890\n",
      "Iteration 61, loss = 0.07872053\n",
      "Iteration 62, loss = 0.07645800\n",
      "Iteration 63, loss = 0.07450783\n",
      "Iteration 64, loss = 0.07235197\n",
      "Iteration 65, loss = 0.07055810\n",
      "Iteration 66, loss = 0.06870755\n",
      "Iteration 67, loss = 0.06700759\n",
      "Iteration 68, loss = 0.06547487\n",
      "Iteration 69, loss = 0.06410846\n",
      "Iteration 70, loss = 0.06243327\n",
      "Iteration 71, loss = 0.06074188\n",
      "Iteration 72, loss = 0.05907519\n",
      "Iteration 73, loss = 0.05787295\n",
      "Iteration 74, loss = 0.05667603\n",
      "Iteration 75, loss = 0.05520760\n",
      "Iteration 76, loss = 0.05404512\n",
      "Iteration 77, loss = 0.05288135\n",
      "Iteration 78, loss = 0.05157849\n",
      "Iteration 79, loss = 0.05084412\n",
      "Iteration 80, loss = 0.04924240\n",
      "Iteration 81, loss = 0.04829903\n",
      "Iteration 82, loss = 0.04732947\n",
      "Iteration 83, loss = 0.04635399\n",
      "Iteration 84, loss = 0.04528412\n",
      "Iteration 85, loss = 0.04433076\n",
      "Iteration 86, loss = 0.04331792\n",
      "Iteration 87, loss = 0.04255415\n",
      "Iteration 88, loss = 0.04154656\n",
      "Iteration 89, loss = 0.04068228\n",
      "Iteration 90, loss = 0.03994146\n",
      "Iteration 91, loss = 0.03916310\n",
      "Iteration 92, loss = 0.03838353\n",
      "Iteration 93, loss = 0.03759820\n",
      "Iteration 94, loss = 0.03686643\n",
      "Iteration 95, loss = 0.03609145\n",
      "Iteration 96, loss = 0.03546709\n",
      "Iteration 97, loss = 0.03479498\n",
      "Iteration 98, loss = 0.03414938\n",
      "Iteration 99, loss = 0.03351244\n",
      "Iteration 100, loss = 0.03284177\n",
      "Iteration 101, loss = 0.03238953\n",
      "Iteration 102, loss = 0.03172321\n",
      "Iteration 103, loss = 0.03103441\n",
      "Iteration 104, loss = 0.03048441\n",
      "Iteration 105, loss = 0.02994236\n",
      "Iteration 106, loss = 0.02934155\n",
      "Iteration 107, loss = 0.02896703\n",
      "Iteration 108, loss = 0.02823766\n",
      "Iteration 109, loss = 0.02778920\n",
      "Iteration 110, loss = 0.02731260\n",
      "Iteration 111, loss = 0.02687192\n",
      "Iteration 112, loss = 0.02633075\n",
      "Iteration 113, loss = 0.02591466\n",
      "Iteration 114, loss = 0.02550750\n",
      "Iteration 115, loss = 0.02503348\n",
      "Iteration 116, loss = 0.02456498\n",
      "Iteration 117, loss = 0.02417984\n",
      "Iteration 118, loss = 0.02395514\n",
      "Iteration 119, loss = 0.02339838\n",
      "Iteration 120, loss = 0.02294643\n",
      "Iteration 121, loss = 0.02259391\n",
      "Iteration 122, loss = 0.02224830\n",
      "Iteration 123, loss = 0.02178854\n",
      "Iteration 124, loss = 0.02154923\n",
      "Iteration 125, loss = 0.02112025\n",
      "Iteration 126, loss = 0.02080978\n",
      "Iteration 127, loss = 0.02053287\n",
      "Iteration 128, loss = 0.02018605\n",
      "Iteration 129, loss = 0.02000032\n",
      "Iteration 130, loss = 0.01952013\n",
      "Iteration 131, loss = 0.01925503\n",
      "Iteration 132, loss = 0.01897776\n",
      "Iteration 133, loss = 0.01865813\n",
      "Iteration 134, loss = 0.01843557\n",
      "Iteration 135, loss = 0.01816372\n",
      "Iteration 136, loss = 0.01787784\n",
      "Iteration 137, loss = 0.01756026\n",
      "Iteration 138, loss = 0.01734443\n",
      "Iteration 139, loss = 0.01710175\n",
      "Iteration 140, loss = 0.01685107\n",
      "Iteration 141, loss = 0.01659241\n",
      "Iteration 142, loss = 0.01638583\n",
      "Iteration 143, loss = 0.01616990\n",
      "Iteration 144, loss = 0.01595823\n",
      "Iteration 145, loss = 0.01573840\n",
      "Iteration 146, loss = 0.01550539\n",
      "Iteration 147, loss = 0.01528404\n",
      "Iteration 148, loss = 0.01512152\n",
      "Iteration 149, loss = 0.01492375\n",
      "Iteration 150, loss = 0.01469550\n",
      "Iteration 151, loss = 0.01452869\n",
      "Iteration 152, loss = 0.01437753\n",
      "Iteration 153, loss = 0.01413826\n",
      "Iteration 154, loss = 0.01393781\n",
      "Iteration 155, loss = 0.01389083\n",
      "Iteration 156, loss = 0.01359244\n",
      "Iteration 157, loss = 0.01343243\n",
      "Iteration 158, loss = 0.01331168\n",
      "Iteration 159, loss = 0.01312197\n",
      "Iteration 160, loss = 0.01293308\n",
      "Iteration 161, loss = 0.01282168\n",
      "Iteration 162, loss = 0.01262799\n",
      "Iteration 163, loss = 0.01247216\n",
      "Iteration 164, loss = 0.01232305\n",
      "Iteration 165, loss = 0.01216262\n",
      "Iteration 166, loss = 0.01205439\n",
      "Iteration 167, loss = 0.01191480\n",
      "Iteration 168, loss = 0.01176326\n",
      "Iteration 169, loss = 0.01162509\n",
      "Iteration 170, loss = 0.01150532\n",
      "Iteration 171, loss = 0.01137886\n",
      "Iteration 172, loss = 0.01125434\n",
      "Iteration 173, loss = 0.01113828\n",
      "Iteration 174, loss = 0.01099681\n",
      "Iteration 175, loss = 0.01088932\n",
      "Iteration 176, loss = 0.01076223\n",
      "Iteration 177, loss = 0.01065755\n",
      "Iteration 178, loss = 0.01052122\n",
      "Iteration 179, loss = 0.01042374\n",
      "Iteration 180, loss = 0.01031777\n",
      "Iteration 181, loss = 0.01020328\n",
      "Iteration 182, loss = 0.01010395\n",
      "Iteration 183, loss = 0.01003614\n",
      "Iteration 184, loss = 0.00991543\n",
      "Iteration 185, loss = 0.00979274\n",
      "Iteration 186, loss = 0.00968715\n",
      "Iteration 187, loss = 0.00959565\n",
      "Iteration 188, loss = 0.00949823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 189, loss = 0.00940560\n",
      "Iteration 190, loss = 0.00932107\n",
      "Iteration 191, loss = 0.00922363\n",
      "Iteration 192, loss = 0.00914042\n",
      "Iteration 193, loss = 0.00905133\n",
      "Iteration 194, loss = 0.00896200\n",
      "Iteration 195, loss = 0.00887909\n",
      "Iteration 196, loss = 0.00880392\n",
      "Iteration 197, loss = 0.00871359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43371822\n",
      "Iteration 2, loss = 0.39604335\n",
      "Iteration 3, loss = 0.39140950\n",
      "Iteration 4, loss = 0.39083030\n",
      "Iteration 5, loss = 0.38978659\n",
      "Iteration 6, loss = 0.38927902\n",
      "Iteration 7, loss = 0.38796659\n",
      "Iteration 8, loss = 0.38808665\n",
      "Iteration 9, loss = 0.38658979\n",
      "Iteration 10, loss = 0.38533398\n",
      "Iteration 11, loss = 0.38426487\n",
      "Iteration 12, loss = 0.38309179\n",
      "Iteration 13, loss = 0.38115560\n",
      "Iteration 14, loss = 0.37937554\n",
      "Iteration 15, loss = 0.37747541\n",
      "Iteration 16, loss = 0.37523091\n",
      "Iteration 17, loss = 0.37306111\n",
      "Iteration 18, loss = 0.37024478\n",
      "Iteration 19, loss = 0.36670510\n",
      "Iteration 20, loss = 0.36290861\n",
      "Iteration 21, loss = 0.35881872\n",
      "Iteration 22, loss = 0.35371390\n",
      "Iteration 23, loss = 0.34811413\n",
      "Iteration 24, loss = 0.34169336\n",
      "Iteration 25, loss = 0.33466932\n",
      "Iteration 26, loss = 0.32726341\n",
      "Iteration 27, loss = 0.31853929\n",
      "Iteration 28, loss = 0.30923724\n",
      "Iteration 29, loss = 0.29873975\n",
      "Iteration 30, loss = 0.28770160\n",
      "Iteration 31, loss = 0.27671048\n",
      "Iteration 32, loss = 0.26399213\n",
      "Iteration 33, loss = 0.25210212\n",
      "Iteration 34, loss = 0.23934835\n",
      "Iteration 35, loss = 0.22713934\n",
      "Iteration 36, loss = 0.21505423\n",
      "Iteration 37, loss = 0.20325329\n",
      "Iteration 38, loss = 0.19256705\n",
      "Iteration 39, loss = 0.18224039\n",
      "Iteration 40, loss = 0.17270252\n",
      "Iteration 41, loss = 0.16364386\n",
      "Iteration 42, loss = 0.15548720\n",
      "Iteration 43, loss = 0.14776957\n",
      "Iteration 44, loss = 0.14062558\n",
      "Iteration 45, loss = 0.13417159\n",
      "Iteration 46, loss = 0.12858746\n",
      "Iteration 47, loss = 0.12302340\n",
      "Iteration 48, loss = 0.11761309\n",
      "Iteration 49, loss = 0.11306827\n",
      "Iteration 50, loss = 0.10848661\n",
      "Iteration 51, loss = 0.10450370\n",
      "Iteration 52, loss = 0.10062985\n",
      "Iteration 53, loss = 0.09726226\n",
      "Iteration 54, loss = 0.09394058\n",
      "Iteration 55, loss = 0.09075012\n",
      "Iteration 56, loss = 0.08853855\n",
      "Iteration 57, loss = 0.08499846\n",
      "Iteration 58, loss = 0.08238570\n",
      "Iteration 59, loss = 0.07994562\n",
      "Iteration 60, loss = 0.07769021\n",
      "Iteration 61, loss = 0.07536548\n",
      "Iteration 62, loss = 0.07336260\n",
      "Iteration 63, loss = 0.07119894\n",
      "Iteration 64, loss = 0.06959633\n",
      "Iteration 65, loss = 0.06738975\n",
      "Iteration 66, loss = 0.06566563\n",
      "Iteration 67, loss = 0.06408586\n",
      "Iteration 68, loss = 0.06238812\n",
      "Iteration 69, loss = 0.06090407\n",
      "Iteration 70, loss = 0.05965803\n",
      "Iteration 71, loss = 0.05819012\n",
      "Iteration 72, loss = 0.05682645\n",
      "Iteration 73, loss = 0.05521615\n",
      "Iteration 74, loss = 0.05428871\n",
      "Iteration 75, loss = 0.05281928\n",
      "Iteration 76, loss = 0.05151806\n",
      "Iteration 77, loss = 0.05044973\n",
      "Iteration 78, loss = 0.04935045\n",
      "Iteration 79, loss = 0.04809387\n",
      "Iteration 80, loss = 0.04703806\n",
      "Iteration 81, loss = 0.04611974\n",
      "Iteration 82, loss = 0.04507244\n",
      "Iteration 83, loss = 0.04406913\n",
      "Iteration 84, loss = 0.04313211\n",
      "Iteration 85, loss = 0.04221064\n",
      "Iteration 86, loss = 0.04165699\n",
      "Iteration 87, loss = 0.04060746\n",
      "Iteration 88, loss = 0.03983199\n",
      "Iteration 89, loss = 0.03888324\n",
      "Iteration 90, loss = 0.03808212\n",
      "Iteration 91, loss = 0.03733099\n",
      "Iteration 92, loss = 0.03670344\n",
      "Iteration 93, loss = 0.03581791\n",
      "Iteration 94, loss = 0.03521856\n",
      "Iteration 95, loss = 0.03452474\n",
      "Iteration 96, loss = 0.03381957\n",
      "Iteration 97, loss = 0.03311741\n",
      "Iteration 98, loss = 0.03254912\n",
      "Iteration 99, loss = 0.03191069\n",
      "Iteration 100, loss = 0.03150256\n",
      "Iteration 101, loss = 0.03072971\n",
      "Iteration 102, loss = 0.03012416\n",
      "Iteration 103, loss = 0.02965396\n",
      "Iteration 104, loss = 0.02909307\n",
      "Iteration 105, loss = 0.02854469\n",
      "Iteration 106, loss = 0.02799595\n",
      "Iteration 107, loss = 0.02747699\n",
      "Iteration 108, loss = 0.02699517\n",
      "Iteration 109, loss = 0.02649938\n",
      "Iteration 110, loss = 0.02608734\n",
      "Iteration 111, loss = 0.02559012\n",
      "Iteration 112, loss = 0.02513716\n",
      "Iteration 113, loss = 0.02470035\n",
      "Iteration 114, loss = 0.02432136\n",
      "Iteration 115, loss = 0.02386901\n",
      "Iteration 116, loss = 0.02350026\n",
      "Iteration 117, loss = 0.02315379\n",
      "Iteration 118, loss = 0.02271965\n",
      "Iteration 119, loss = 0.02235082\n",
      "Iteration 120, loss = 0.02201919\n",
      "Iteration 121, loss = 0.02161594\n",
      "Iteration 122, loss = 0.02144192\n",
      "Iteration 123, loss = 0.02094692\n",
      "Iteration 124, loss = 0.02072443\n",
      "Iteration 125, loss = 0.02025554\n",
      "Iteration 126, loss = 0.01998454\n",
      "Iteration 127, loss = 0.01960576\n",
      "Iteration 128, loss = 0.01935806\n",
      "Iteration 129, loss = 0.01909125\n",
      "Iteration 130, loss = 0.01904498\n",
      "Iteration 131, loss = 0.01852052\n",
      "Iteration 132, loss = 0.01823904\n",
      "Iteration 133, loss = 0.01788496\n",
      "Iteration 134, loss = 0.01766882\n",
      "Iteration 135, loss = 0.01749573\n",
      "Iteration 136, loss = 0.01724292\n",
      "Iteration 137, loss = 0.01690010\n",
      "Iteration 138, loss = 0.01667554\n",
      "Iteration 139, loss = 0.01643010\n",
      "Iteration 140, loss = 0.01616233\n",
      "Iteration 141, loss = 0.01595676\n",
      "Iteration 142, loss = 0.01575297\n",
      "Iteration 143, loss = 0.01550349\n",
      "Iteration 144, loss = 0.01526837\n",
      "Iteration 145, loss = 0.01506583\n",
      "Iteration 146, loss = 0.01486199\n",
      "Iteration 147, loss = 0.01466691\n",
      "Iteration 148, loss = 0.01447645\n",
      "Iteration 149, loss = 0.01427325\n",
      "Iteration 150, loss = 0.01415604\n",
      "Iteration 151, loss = 0.01395731\n",
      "Iteration 152, loss = 0.01372657\n",
      "Iteration 153, loss = 0.01362650\n",
      "Iteration 154, loss = 0.01346542\n",
      "Iteration 155, loss = 0.01328511\n",
      "Iteration 156, loss = 0.01307068\n",
      "Iteration 157, loss = 0.01287840\n",
      "Iteration 158, loss = 0.01271892\n",
      "Iteration 159, loss = 0.01259112\n",
      "Iteration 160, loss = 0.01240809\n",
      "Iteration 161, loss = 0.01224625\n",
      "Iteration 162, loss = 0.01211717\n",
      "Iteration 163, loss = 0.01199361\n",
      "Iteration 164, loss = 0.01184325\n",
      "Iteration 165, loss = 0.01167482\n",
      "Iteration 166, loss = 0.01153965\n",
      "Iteration 167, loss = 0.01140926\n",
      "Iteration 168, loss = 0.01127944\n",
      "Iteration 169, loss = 0.01116755\n",
      "Iteration 170, loss = 0.01105587\n",
      "Iteration 171, loss = 0.01092287\n",
      "Iteration 172, loss = 0.01080013\n",
      "Iteration 173, loss = 0.01065851\n",
      "Iteration 174, loss = 0.01056897\n",
      "Iteration 175, loss = 0.01047006\n",
      "Iteration 176, loss = 0.01033206\n",
      "Iteration 177, loss = 0.01021066\n",
      "Iteration 178, loss = 0.01010199\n",
      "Iteration 179, loss = 0.01001036\n",
      "Iteration 180, loss = 0.00990764\n",
      "Iteration 181, loss = 0.00979431\n",
      "Iteration 182, loss = 0.00969275\n",
      "Iteration 183, loss = 0.00962972\n",
      "Iteration 184, loss = 0.00949168\n",
      "Iteration 185, loss = 0.00940869\n",
      "Iteration 186, loss = 0.00930193\n",
      "Iteration 187, loss = 0.00920905\n",
      "Iteration 188, loss = 0.00913037\n",
      "Iteration 189, loss = 0.00904235\n",
      "Iteration 190, loss = 0.00894476\n",
      "Iteration 191, loss = 0.00887268\n",
      "Iteration 192, loss = 0.00879279\n",
      "Iteration 193, loss = 0.00868581\n",
      "Iteration 194, loss = 0.00860684\n",
      "Iteration 195, loss = 0.00852670\n",
      "Iteration 196, loss = 0.00847721\n",
      "Iteration 197, loss = 0.00836938\n",
      "Iteration 198, loss = 0.00828585\n",
      "Iteration 199, loss = 0.00822239\n",
      "Iteration 200, loss = 0.00814351\n",
      "Iteration 201, loss = 0.00807532\n",
      "Iteration 202, loss = 0.00799390\n",
      "Iteration 203, loss = 0.00791739\n",
      "Iteration 204, loss = 0.00786450\n",
      "Iteration 205, loss = 0.00777709\n",
      "Iteration 206, loss = 0.00774302\n",
      "Iteration 207, loss = 0.00764760\n",
      "Iteration 208, loss = 0.00758811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43237810\n",
      "Iteration 2, loss = 0.39737654\n",
      "Iteration 3, loss = 0.39584610\n",
      "Iteration 4, loss = 0.39529231\n",
      "Iteration 5, loss = 0.39426271\n",
      "Iteration 6, loss = 0.39423468\n",
      "Iteration 7, loss = 0.39289210\n",
      "Iteration 8, loss = 0.39196465\n",
      "Iteration 9, loss = 0.39081119\n",
      "Iteration 10, loss = 0.38986442\n",
      "Iteration 11, loss = 0.38841763\n",
      "Iteration 12, loss = 0.38804699\n",
      "Iteration 13, loss = 0.38560889\n",
      "Iteration 14, loss = 0.38418930\n",
      "Iteration 15, loss = 0.38193716\n",
      "Iteration 16, loss = 0.37979224\n",
      "Iteration 17, loss = 0.37705994\n",
      "Iteration 18, loss = 0.37505198\n",
      "Iteration 19, loss = 0.37075320\n",
      "Iteration 20, loss = 0.36671926\n",
      "Iteration 21, loss = 0.36265754\n",
      "Iteration 22, loss = 0.35691683\n",
      "Iteration 23, loss = 0.35169287\n",
      "Iteration 24, loss = 0.34500399\n",
      "Iteration 25, loss = 0.33767038\n",
      "Iteration 26, loss = 0.32949325\n",
      "Iteration 27, loss = 0.32078510\n",
      "Iteration 28, loss = 0.31082539\n",
      "Iteration 29, loss = 0.30000741\n",
      "Iteration 30, loss = 0.28917744\n",
      "Iteration 31, loss = 0.27810171\n",
      "Iteration 32, loss = 0.26607356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.25306910\n",
      "Iteration 34, loss = 0.24002548\n",
      "Iteration 35, loss = 0.22786958\n",
      "Iteration 36, loss = 0.21601902\n",
      "Iteration 37, loss = 0.20419841\n",
      "Iteration 38, loss = 0.19379209\n",
      "Iteration 39, loss = 0.18310370\n",
      "Iteration 40, loss = 0.17385110\n",
      "Iteration 41, loss = 0.16461911\n",
      "Iteration 42, loss = 0.15644672\n",
      "Iteration 43, loss = 0.14893780\n",
      "Iteration 44, loss = 0.14193801\n",
      "Iteration 45, loss = 0.13548616\n",
      "Iteration 46, loss = 0.13001199\n",
      "Iteration 47, loss = 0.12405523\n",
      "Iteration 48, loss = 0.11924971\n",
      "Iteration 49, loss = 0.11452581\n",
      "Iteration 50, loss = 0.11037260\n",
      "Iteration 51, loss = 0.10629147\n",
      "Iteration 52, loss = 0.10219456\n",
      "Iteration 53, loss = 0.09900707\n",
      "Iteration 54, loss = 0.09562528\n",
      "Iteration 55, loss = 0.09259751\n",
      "Iteration 56, loss = 0.08993486\n",
      "Iteration 57, loss = 0.08717391\n",
      "Iteration 58, loss = 0.08446135\n",
      "Iteration 59, loss = 0.08187494\n",
      "Iteration 60, loss = 0.07997559\n",
      "Iteration 61, loss = 0.07753340\n",
      "Iteration 62, loss = 0.07540773\n",
      "Iteration 63, loss = 0.07332842\n",
      "Iteration 64, loss = 0.07147487\n",
      "Iteration 65, loss = 0.06950357\n",
      "Iteration 66, loss = 0.06771664\n",
      "Iteration 67, loss = 0.06605738\n",
      "Iteration 68, loss = 0.06441625\n",
      "Iteration 69, loss = 0.06287833\n",
      "Iteration 70, loss = 0.06121255\n",
      "Iteration 71, loss = 0.05977145\n",
      "Iteration 72, loss = 0.05848149\n",
      "Iteration 73, loss = 0.05705772\n",
      "Iteration 74, loss = 0.05587913\n",
      "Iteration 75, loss = 0.05474364\n",
      "Iteration 76, loss = 0.05386620\n",
      "Iteration 77, loss = 0.05205456\n",
      "Iteration 78, loss = 0.05088101\n",
      "Iteration 79, loss = 0.04983136\n",
      "Iteration 80, loss = 0.04881453\n",
      "Iteration 81, loss = 0.04771361\n",
      "Iteration 82, loss = 0.04665993\n",
      "Iteration 83, loss = 0.04570591\n",
      "Iteration 84, loss = 0.04479193\n",
      "Iteration 85, loss = 0.04377229\n",
      "Iteration 86, loss = 0.04290930\n",
      "Iteration 87, loss = 0.04214986\n",
      "Iteration 88, loss = 0.04121166\n",
      "Iteration 89, loss = 0.04041812\n",
      "Iteration 90, loss = 0.03950945\n",
      "Iteration 91, loss = 0.03887626\n",
      "Iteration 92, loss = 0.03801845\n",
      "Iteration 93, loss = 0.03716392\n",
      "Iteration 94, loss = 0.03657347\n",
      "Iteration 95, loss = 0.03599624\n",
      "Iteration 96, loss = 0.03529915\n",
      "Iteration 97, loss = 0.03439785\n",
      "Iteration 98, loss = 0.03374354\n",
      "Iteration 99, loss = 0.03308514\n",
      "Iteration 100, loss = 0.03264545\n",
      "Iteration 101, loss = 0.03185459\n",
      "Iteration 102, loss = 0.03122606\n",
      "Iteration 103, loss = 0.03063814\n",
      "Iteration 104, loss = 0.03002067\n",
      "Iteration 105, loss = 0.02959757\n",
      "Iteration 106, loss = 0.02897147\n",
      "Iteration 107, loss = 0.02866898\n",
      "Iteration 108, loss = 0.02799972\n",
      "Iteration 109, loss = 0.02757594\n",
      "Iteration 110, loss = 0.02699936\n",
      "Iteration 111, loss = 0.02646927\n",
      "Iteration 112, loss = 0.02595569\n",
      "Iteration 113, loss = 0.02548740\n",
      "Iteration 114, loss = 0.02514367\n",
      "Iteration 115, loss = 0.02463601\n",
      "Iteration 116, loss = 0.02431387\n",
      "Iteration 117, loss = 0.02383878\n",
      "Iteration 118, loss = 0.02335875\n",
      "Iteration 119, loss = 0.02308751\n",
      "Iteration 120, loss = 0.02257578\n",
      "Iteration 121, loss = 0.02227436\n",
      "Iteration 122, loss = 0.02181451\n",
      "Iteration 123, loss = 0.02146996\n",
      "Iteration 124, loss = 0.02113107\n",
      "Iteration 125, loss = 0.02083830\n",
      "Iteration 126, loss = 0.02043240\n",
      "Iteration 127, loss = 0.02018369\n",
      "Iteration 128, loss = 0.01979949\n",
      "Iteration 129, loss = 0.01951689\n",
      "Iteration 130, loss = 0.01926846\n",
      "Iteration 131, loss = 0.01894597\n",
      "Iteration 132, loss = 0.01870012\n",
      "Iteration 133, loss = 0.01828859\n",
      "Iteration 134, loss = 0.01803629\n",
      "Iteration 135, loss = 0.01781406\n",
      "Iteration 136, loss = 0.01754239\n",
      "Iteration 137, loss = 0.01724697\n",
      "Iteration 138, loss = 0.01702827\n",
      "Iteration 139, loss = 0.01682266\n",
      "Iteration 140, loss = 0.01652013\n",
      "Iteration 141, loss = 0.01624772\n",
      "Iteration 142, loss = 0.01601019\n",
      "Iteration 143, loss = 0.01579674\n",
      "Iteration 144, loss = 0.01555879\n",
      "Iteration 145, loss = 0.01541596\n",
      "Iteration 146, loss = 0.01512914\n",
      "Iteration 147, loss = 0.01493439\n",
      "Iteration 148, loss = 0.01473252\n",
      "Iteration 149, loss = 0.01460852\n",
      "Iteration 150, loss = 0.01434313\n",
      "Iteration 151, loss = 0.01415555\n",
      "Iteration 152, loss = 0.01394333\n",
      "Iteration 153, loss = 0.01377886\n",
      "Iteration 154, loss = 0.01357898\n",
      "Iteration 155, loss = 0.01340430\n",
      "Iteration 156, loss = 0.01322911\n",
      "Iteration 157, loss = 0.01306041\n",
      "Iteration 158, loss = 0.01289659\n",
      "Iteration 159, loss = 0.01273108\n",
      "Iteration 160, loss = 0.01257955\n",
      "Iteration 161, loss = 0.01245547\n",
      "Iteration 162, loss = 0.01230449\n",
      "Iteration 163, loss = 0.01217290\n",
      "Iteration 164, loss = 0.01198079\n",
      "Iteration 165, loss = 0.01183951\n",
      "Iteration 166, loss = 0.01172492\n",
      "Iteration 167, loss = 0.01159079\n",
      "Iteration 168, loss = 0.01143027\n",
      "Iteration 169, loss = 0.01130517\n",
      "Iteration 170, loss = 0.01119691\n",
      "Iteration 171, loss = 0.01104378\n",
      "Iteration 172, loss = 0.01094331\n",
      "Iteration 173, loss = 0.01083376\n",
      "Iteration 174, loss = 0.01068538\n",
      "Iteration 175, loss = 0.01055225\n",
      "Iteration 176, loss = 0.01044949\n",
      "Iteration 177, loss = 0.01033739\n",
      "Iteration 178, loss = 0.01022356\n",
      "Iteration 179, loss = 0.01010690\n",
      "Iteration 180, loss = 0.01000483\n",
      "Iteration 181, loss = 0.00989637\n",
      "Iteration 182, loss = 0.00983424\n",
      "Iteration 183, loss = 0.00972438\n",
      "Iteration 184, loss = 0.00960375\n",
      "Iteration 185, loss = 0.00949010\n",
      "Iteration 186, loss = 0.00942364\n",
      "Iteration 187, loss = 0.00934435\n",
      "Iteration 188, loss = 0.00920840\n",
      "Iteration 189, loss = 0.00913576\n",
      "Iteration 190, loss = 0.00904695\n",
      "Iteration 191, loss = 0.00895005\n",
      "Iteration 192, loss = 0.00885424\n",
      "Iteration 193, loss = 0.00877978\n",
      "Iteration 194, loss = 0.00868217\n",
      "Iteration 195, loss = 0.00862176\n",
      "Iteration 196, loss = 0.00851292\n",
      "Iteration 197, loss = 0.00845386\n",
      "Iteration 198, loss = 0.00838467\n",
      "Iteration 199, loss = 0.00830076\n",
      "Iteration 200, loss = 0.00824975\n",
      "Iteration 201, loss = 0.00814692\n",
      "Iteration 202, loss = 0.00808437\n",
      "Iteration 203, loss = 0.00799518\n",
      "Iteration 204, loss = 0.00792484\n",
      "Iteration 205, loss = 0.00784085\n",
      "Iteration 206, loss = 0.00777822\n",
      "Iteration 207, loss = 0.00772114\n",
      "Iteration 208, loss = 0.00764265\n",
      "Iteration 209, loss = 0.00759778\n",
      "Iteration 210, loss = 0.00752122\n",
      "Iteration 211, loss = 0.00743999\n",
      "Iteration 212, loss = 0.00740161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42816109\n",
      "Iteration 2, loss = 0.39339999\n",
      "Iteration 3, loss = 0.39113050\n",
      "Iteration 4, loss = 0.39018810\n",
      "Iteration 5, loss = 0.38988805\n",
      "Iteration 6, loss = 0.38892219\n",
      "Iteration 7, loss = 0.38752055\n",
      "Iteration 8, loss = 0.38788924\n",
      "Iteration 9, loss = 0.38585814\n",
      "Iteration 10, loss = 0.38511517\n",
      "Iteration 11, loss = 0.38388044\n",
      "Iteration 12, loss = 0.38228533\n",
      "Iteration 13, loss = 0.38086829\n",
      "Iteration 14, loss = 0.37910061\n",
      "Iteration 15, loss = 0.37711370\n",
      "Iteration 16, loss = 0.37525207\n",
      "Iteration 17, loss = 0.37287649\n",
      "Iteration 18, loss = 0.36934632\n",
      "Iteration 19, loss = 0.36708400\n",
      "Iteration 20, loss = 0.36323484\n",
      "Iteration 21, loss = 0.35842093\n",
      "Iteration 22, loss = 0.35338335\n",
      "Iteration 23, loss = 0.34804372\n",
      "Iteration 24, loss = 0.34156788\n",
      "Iteration 25, loss = 0.33490576\n",
      "Iteration 26, loss = 0.32749406\n",
      "Iteration 27, loss = 0.31875647\n",
      "Iteration 28, loss = 0.30954956\n",
      "Iteration 29, loss = 0.29923563\n",
      "Iteration 30, loss = 0.28907419\n",
      "Iteration 31, loss = 0.27737059\n",
      "Iteration 32, loss = 0.26605201\n",
      "Iteration 33, loss = 0.25391322\n",
      "Iteration 34, loss = 0.24146022\n",
      "Iteration 35, loss = 0.22954346\n",
      "Iteration 36, loss = 0.21797739\n",
      "Iteration 37, loss = 0.20688818\n",
      "Iteration 38, loss = 0.19607701\n",
      "Iteration 39, loss = 0.18606217\n",
      "Iteration 40, loss = 0.17655251\n",
      "Iteration 41, loss = 0.16743193\n",
      "Iteration 42, loss = 0.15961860\n",
      "Iteration 43, loss = 0.15180411\n",
      "Iteration 44, loss = 0.14507839\n",
      "Iteration 45, loss = 0.13868787\n",
      "Iteration 46, loss = 0.13230425\n",
      "Iteration 47, loss = 0.12707934\n",
      "Iteration 48, loss = 0.12201555\n",
      "Iteration 49, loss = 0.11682485\n",
      "Iteration 50, loss = 0.11258481\n",
      "Iteration 51, loss = 0.10865578\n",
      "Iteration 52, loss = 0.10438821\n",
      "Iteration 53, loss = 0.10078883\n",
      "Iteration 54, loss = 0.09731579\n",
      "Iteration 55, loss = 0.09427496\n",
      "Iteration 56, loss = 0.09120646\n",
      "Iteration 57, loss = 0.08832080\n",
      "Iteration 58, loss = 0.08556244\n",
      "Iteration 59, loss = 0.08289601\n",
      "Iteration 60, loss = 0.08056651\n",
      "Iteration 61, loss = 0.07851307\n",
      "Iteration 62, loss = 0.07657287\n",
      "Iteration 63, loss = 0.07389627\n",
      "Iteration 64, loss = 0.07175548\n",
      "Iteration 65, loss = 0.06983738\n",
      "Iteration 66, loss = 0.06787345\n",
      "Iteration 67, loss = 0.06657492\n",
      "Iteration 68, loss = 0.06451500\n",
      "Iteration 69, loss = 0.06289890\n",
      "Iteration 70, loss = 0.06135510\n",
      "Iteration 71, loss = 0.05978225\n",
      "Iteration 72, loss = 0.05845019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73, loss = 0.05703376\n",
      "Iteration 74, loss = 0.05563605\n",
      "Iteration 75, loss = 0.05410842\n",
      "Iteration 76, loss = 0.05283099\n",
      "Iteration 77, loss = 0.05167594\n",
      "Iteration 78, loss = 0.05056155\n",
      "Iteration 79, loss = 0.04929412\n",
      "Iteration 80, loss = 0.04839322\n",
      "Iteration 81, loss = 0.04717133\n",
      "Iteration 82, loss = 0.04624118\n",
      "Iteration 83, loss = 0.04530241\n",
      "Iteration 84, loss = 0.04406068\n",
      "Iteration 85, loss = 0.04319079\n",
      "Iteration 86, loss = 0.04244645\n",
      "Iteration 87, loss = 0.04138945\n",
      "Iteration 88, loss = 0.04053359\n",
      "Iteration 89, loss = 0.03982821\n",
      "Iteration 90, loss = 0.03911676\n",
      "Iteration 91, loss = 0.03810136\n",
      "Iteration 92, loss = 0.03764778\n",
      "Iteration 93, loss = 0.03654642\n",
      "Iteration 94, loss = 0.03581725\n",
      "Iteration 95, loss = 0.03510330\n",
      "Iteration 96, loss = 0.03437197\n",
      "Iteration 97, loss = 0.03397593\n",
      "Iteration 98, loss = 0.03329354\n",
      "Iteration 99, loss = 0.03235617\n",
      "Iteration 100, loss = 0.03191215\n",
      "Iteration 101, loss = 0.03123775\n",
      "Iteration 102, loss = 0.03076107\n",
      "Iteration 103, loss = 0.02998710\n",
      "Iteration 104, loss = 0.02954795\n",
      "Iteration 105, loss = 0.02894612\n",
      "Iteration 106, loss = 0.02840733\n",
      "Iteration 107, loss = 0.02799256\n",
      "Iteration 108, loss = 0.02744124\n",
      "Iteration 109, loss = 0.02692217\n",
      "Iteration 110, loss = 0.02639985\n",
      "Iteration 111, loss = 0.02594619\n",
      "Iteration 112, loss = 0.02544319\n",
      "Iteration 113, loss = 0.02497008\n",
      "Iteration 114, loss = 0.02477708\n",
      "Iteration 115, loss = 0.02415989\n",
      "Iteration 116, loss = 0.02370659\n",
      "Iteration 117, loss = 0.02331476\n",
      "Iteration 118, loss = 0.02296092\n",
      "Iteration 119, loss = 0.02260939\n",
      "Iteration 120, loss = 0.02226507\n",
      "Iteration 121, loss = 0.02177434\n",
      "Iteration 122, loss = 0.02158562\n",
      "Iteration 123, loss = 0.02110678\n",
      "Iteration 124, loss = 0.02081121\n",
      "Iteration 125, loss = 0.02045636\n",
      "Iteration 126, loss = 0.02012821\n",
      "Iteration 127, loss = 0.01978394\n",
      "Iteration 128, loss = 0.01944788\n",
      "Iteration 129, loss = 0.01912962\n",
      "Iteration 130, loss = 0.01885867\n",
      "Iteration 131, loss = 0.01855468\n",
      "Iteration 132, loss = 0.01829811\n",
      "Iteration 133, loss = 0.01797813\n",
      "Iteration 134, loss = 0.01774526\n",
      "Iteration 135, loss = 0.01747088\n",
      "Iteration 136, loss = 0.01722525\n",
      "Iteration 137, loss = 0.01694367\n",
      "Iteration 138, loss = 0.01670196\n",
      "Iteration 139, loss = 0.01647391\n",
      "Iteration 140, loss = 0.01621586\n",
      "Iteration 141, loss = 0.01598949\n",
      "Iteration 142, loss = 0.01575512\n",
      "Iteration 143, loss = 0.01553131\n",
      "Iteration 144, loss = 0.01534027\n",
      "Iteration 145, loss = 0.01508838\n",
      "Iteration 146, loss = 0.01489888\n",
      "Iteration 147, loss = 0.01468982\n",
      "Iteration 148, loss = 0.01450005\n",
      "Iteration 149, loss = 0.01431077\n",
      "Iteration 150, loss = 0.01411277\n",
      "Iteration 151, loss = 0.01394429\n",
      "Iteration 152, loss = 0.01375776\n",
      "Iteration 153, loss = 0.01356965\n",
      "Iteration 154, loss = 0.01345608\n",
      "Iteration 155, loss = 0.01324219\n",
      "Iteration 156, loss = 0.01305811\n",
      "Iteration 157, loss = 0.01297497\n",
      "Iteration 158, loss = 0.01272913\n",
      "Iteration 159, loss = 0.01259449\n",
      "Iteration 160, loss = 0.01241794\n",
      "Iteration 161, loss = 0.01226141\n",
      "Iteration 162, loss = 0.01213053\n",
      "Iteration 163, loss = 0.01198207\n",
      "Iteration 164, loss = 0.01183018\n",
      "Iteration 165, loss = 0.01170127\n",
      "Iteration 166, loss = 0.01156196\n",
      "Iteration 167, loss = 0.01142413\n",
      "Iteration 168, loss = 0.01129255\n",
      "Iteration 169, loss = 0.01115960\n",
      "Iteration 170, loss = 0.01102726\n",
      "Iteration 171, loss = 0.01091422\n",
      "Iteration 172, loss = 0.01079776\n",
      "Iteration 173, loss = 0.01068257\n",
      "Iteration 174, loss = 0.01054898\n",
      "Iteration 175, loss = 0.01046455\n",
      "Iteration 176, loss = 0.01033446\n",
      "Iteration 177, loss = 0.01023459\n",
      "Iteration 178, loss = 0.01011488\n",
      "Iteration 179, loss = 0.01000516\n",
      "Iteration 180, loss = 0.00989420\n",
      "Iteration 181, loss = 0.00978696\n",
      "Iteration 182, loss = 0.00968834\n",
      "Iteration 183, loss = 0.00959494\n",
      "Iteration 184, loss = 0.00948107\n",
      "Iteration 185, loss = 0.00938181\n",
      "Iteration 186, loss = 0.00928964\n",
      "Iteration 187, loss = 0.00920179\n",
      "Iteration 188, loss = 0.00909784\n",
      "Iteration 189, loss = 0.00902074\n",
      "Iteration 190, loss = 0.00892936\n",
      "Iteration 191, loss = 0.00884813\n",
      "Iteration 192, loss = 0.00877369\n",
      "Iteration 193, loss = 0.00870467\n",
      "Iteration 194, loss = 0.00858218\n",
      "Iteration 195, loss = 0.00852067\n",
      "Iteration 196, loss = 0.00842457\n",
      "Iteration 197, loss = 0.00835775\n",
      "Iteration 198, loss = 0.00826545\n",
      "Iteration 199, loss = 0.00820297\n",
      "Iteration 200, loss = 0.00813802\n",
      "Iteration 201, loss = 0.00806904\n",
      "Iteration 202, loss = 0.00800150\n",
      "Iteration 203, loss = 0.00789983\n",
      "Iteration 204, loss = 0.00784037\n",
      "Iteration 205, loss = 0.00776835\n",
      "Iteration 206, loss = 0.00769541\n",
      "Iteration 207, loss = 0.00763372\n",
      "Iteration 208, loss = 0.00757717\n",
      "Iteration 209, loss = 0.00750378\n",
      "Iteration 210, loss = 0.00744634\n",
      "Iteration 211, loss = 0.00738091\n",
      "Iteration 212, loss = 0.00732235\n",
      "Iteration 213, loss = 0.00725904\n",
      "Iteration 214, loss = 0.00719707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41999135\n",
      "Iteration 2, loss = 0.38783108\n",
      "Iteration 3, loss = 0.38699659\n",
      "Iteration 4, loss = 0.38640092\n",
      "Iteration 5, loss = 0.38523151\n",
      "Iteration 6, loss = 0.38416444\n",
      "Iteration 7, loss = 0.38372463\n",
      "Iteration 8, loss = 0.38257425\n",
      "Iteration 9, loss = 0.38178452\n",
      "Iteration 10, loss = 0.38134748\n",
      "Iteration 11, loss = 0.37999718\n",
      "Iteration 12, loss = 0.37819909\n",
      "Iteration 13, loss = 0.37763142\n",
      "Iteration 14, loss = 0.37497003\n",
      "Iteration 15, loss = 0.37344177\n",
      "Iteration 16, loss = 0.37088515\n",
      "Iteration 17, loss = 0.36836855\n",
      "Iteration 18, loss = 0.36551942\n",
      "Iteration 19, loss = 0.36257692\n",
      "Iteration 20, loss = 0.35891801\n",
      "Iteration 21, loss = 0.35442411\n",
      "Iteration 22, loss = 0.35000340\n",
      "Iteration 23, loss = 0.34413899\n",
      "Iteration 24, loss = 0.33828603\n",
      "Iteration 25, loss = 0.33131626\n",
      "Iteration 26, loss = 0.32458402\n",
      "Iteration 27, loss = 0.31666133\n",
      "Iteration 28, loss = 0.30661968\n",
      "Iteration 29, loss = 0.29652636\n",
      "Iteration 30, loss = 0.28633546\n",
      "Iteration 31, loss = 0.27503421\n",
      "Iteration 32, loss = 0.26321502\n",
      "Iteration 33, loss = 0.25106689\n",
      "Iteration 34, loss = 0.23954918\n",
      "Iteration 35, loss = 0.22768829\n",
      "Iteration 36, loss = 0.21648633\n",
      "Iteration 37, loss = 0.20474908\n",
      "Iteration 38, loss = 0.19460116\n",
      "Iteration 39, loss = 0.18426479\n",
      "Iteration 40, loss = 0.17483515\n",
      "Iteration 41, loss = 0.16649567\n",
      "Iteration 42, loss = 0.15821132\n",
      "Iteration 43, loss = 0.15051302\n",
      "Iteration 44, loss = 0.14381911\n",
      "Iteration 45, loss = 0.13800491\n",
      "Iteration 46, loss = 0.13136623\n",
      "Iteration 47, loss = 0.12610246\n",
      "Iteration 48, loss = 0.12090965\n",
      "Iteration 49, loss = 0.11640310\n",
      "Iteration 50, loss = 0.11177688\n",
      "Iteration 51, loss = 0.10751971\n",
      "Iteration 52, loss = 0.10374278\n",
      "Iteration 53, loss = 0.10007694\n",
      "Iteration 54, loss = 0.09645318\n",
      "Iteration 55, loss = 0.09388827\n",
      "Iteration 56, loss = 0.09052416\n",
      "Iteration 57, loss = 0.08783487\n",
      "Iteration 58, loss = 0.08500799\n",
      "Iteration 59, loss = 0.08238270\n",
      "Iteration 60, loss = 0.07984136\n",
      "Iteration 61, loss = 0.07753322\n",
      "Iteration 62, loss = 0.07533574\n",
      "Iteration 63, loss = 0.07327895\n",
      "Iteration 64, loss = 0.07137993\n",
      "Iteration 65, loss = 0.06931050\n",
      "Iteration 66, loss = 0.06755295\n",
      "Iteration 67, loss = 0.06584777\n",
      "Iteration 68, loss = 0.06411201\n",
      "Iteration 69, loss = 0.06253636\n",
      "Iteration 70, loss = 0.06076122\n",
      "Iteration 71, loss = 0.05943342\n",
      "Iteration 72, loss = 0.05792836\n",
      "Iteration 73, loss = 0.05663566\n",
      "Iteration 74, loss = 0.05508661\n",
      "Iteration 75, loss = 0.05373481\n",
      "Iteration 76, loss = 0.05250642\n",
      "Iteration 77, loss = 0.05142089\n",
      "Iteration 78, loss = 0.05017427\n",
      "Iteration 79, loss = 0.04928937\n",
      "Iteration 80, loss = 0.04781468\n",
      "Iteration 81, loss = 0.04681028\n",
      "Iteration 82, loss = 0.04576296\n",
      "Iteration 83, loss = 0.04469547\n",
      "Iteration 84, loss = 0.04391783\n",
      "Iteration 85, loss = 0.04311343\n",
      "Iteration 86, loss = 0.04201266\n",
      "Iteration 87, loss = 0.04100607\n",
      "Iteration 88, loss = 0.04019522\n",
      "Iteration 89, loss = 0.03926692\n",
      "Iteration 90, loss = 0.03836780\n",
      "Iteration 91, loss = 0.03771065\n",
      "Iteration 92, loss = 0.03696366\n",
      "Iteration 93, loss = 0.03634354\n",
      "Iteration 94, loss = 0.03542887\n",
      "Iteration 95, loss = 0.03477601\n",
      "Iteration 96, loss = 0.03405696\n",
      "Iteration 97, loss = 0.03335407\n",
      "Iteration 98, loss = 0.03262508\n",
      "Iteration 99, loss = 0.03201276\n",
      "Iteration 100, loss = 0.03153562\n",
      "Iteration 101, loss = 0.03078690\n",
      "Iteration 102, loss = 0.03010395\n",
      "Iteration 103, loss = 0.02954319\n",
      "Iteration 104, loss = 0.02900828\n",
      "Iteration 105, loss = 0.02848632\n",
      "Iteration 106, loss = 0.02799896\n",
      "Iteration 107, loss = 0.02743117\n",
      "Iteration 108, loss = 0.02691262\n",
      "Iteration 109, loss = 0.02645185\n",
      "Iteration 110, loss = 0.02591285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 111, loss = 0.02545824\n",
      "Iteration 112, loss = 0.02495936\n",
      "Iteration 113, loss = 0.02449445\n",
      "Iteration 114, loss = 0.02406243\n",
      "Iteration 115, loss = 0.02365945\n",
      "Iteration 116, loss = 0.02323815\n",
      "Iteration 117, loss = 0.02291179\n",
      "Iteration 118, loss = 0.02240313\n",
      "Iteration 119, loss = 0.02203017\n",
      "Iteration 120, loss = 0.02167626\n",
      "Iteration 121, loss = 0.02128357\n",
      "Iteration 122, loss = 0.02090325\n",
      "Iteration 123, loss = 0.02073097\n",
      "Iteration 124, loss = 0.02033885\n",
      "Iteration 125, loss = 0.01991892\n",
      "Iteration 126, loss = 0.01969241\n",
      "Iteration 127, loss = 0.01930677\n",
      "Iteration 128, loss = 0.01897373\n",
      "Iteration 129, loss = 0.01869713\n",
      "Iteration 130, loss = 0.01837753\n",
      "Iteration 131, loss = 0.01812764\n",
      "Iteration 132, loss = 0.01795163\n",
      "Iteration 133, loss = 0.01762763\n",
      "Iteration 134, loss = 0.01743093\n",
      "Iteration 135, loss = 0.01711808\n",
      "Iteration 136, loss = 0.01673732\n",
      "Iteration 137, loss = 0.01647725\n",
      "Iteration 138, loss = 0.01645494\n",
      "Iteration 139, loss = 0.01608896\n",
      "Iteration 140, loss = 0.01581593\n",
      "Iteration 141, loss = 0.01554425\n",
      "Iteration 142, loss = 0.01530085\n",
      "Iteration 143, loss = 0.01515944\n",
      "Iteration 144, loss = 0.01487806\n",
      "Iteration 145, loss = 0.01466674\n",
      "Iteration 146, loss = 0.01446816\n",
      "Iteration 147, loss = 0.01427474\n",
      "Iteration 148, loss = 0.01412126\n",
      "Iteration 149, loss = 0.01390765\n",
      "Iteration 150, loss = 0.01373172\n",
      "Iteration 151, loss = 0.01360052\n",
      "Iteration 152, loss = 0.01336270\n",
      "Iteration 153, loss = 0.01316642\n",
      "Iteration 154, loss = 0.01300430\n",
      "Iteration 155, loss = 0.01281981\n",
      "Iteration 156, loss = 0.01265199\n",
      "Iteration 157, loss = 0.01253072\n",
      "Iteration 158, loss = 0.01235369\n",
      "Iteration 159, loss = 0.01218751\n",
      "Iteration 160, loss = 0.01204287\n",
      "Iteration 161, loss = 0.01190365\n",
      "Iteration 162, loss = 0.01174709\n",
      "Iteration 163, loss = 0.01161356\n",
      "Iteration 164, loss = 0.01145946\n",
      "Iteration 165, loss = 0.01133460\n",
      "Iteration 166, loss = 0.01121623\n",
      "Iteration 167, loss = 0.01106437\n",
      "Iteration 168, loss = 0.01093111\n",
      "Iteration 169, loss = 0.01081227\n",
      "Iteration 170, loss = 0.01069218\n",
      "Iteration 171, loss = 0.01056478\n",
      "Iteration 172, loss = 0.01044581\n",
      "Iteration 173, loss = 0.01033625\n",
      "Iteration 174, loss = 0.01021363\n",
      "Iteration 175, loss = 0.01012900\n",
      "Iteration 176, loss = 0.00998497\n",
      "Iteration 177, loss = 0.00988258\n",
      "Iteration 178, loss = 0.00979125\n",
      "Iteration 179, loss = 0.00971103\n",
      "Iteration 180, loss = 0.00964369\n",
      "Iteration 181, loss = 0.00946238\n",
      "Iteration 182, loss = 0.00938921\n",
      "Iteration 183, loss = 0.00930010\n",
      "Iteration 184, loss = 0.00916766\n",
      "Iteration 185, loss = 0.00908844\n",
      "Iteration 186, loss = 0.00902312\n",
      "Iteration 187, loss = 0.00890015\n",
      "Iteration 188, loss = 0.00882109\n",
      "Iteration 189, loss = 0.00872431\n",
      "Iteration 190, loss = 0.00863230\n",
      "Iteration 191, loss = 0.00854883\n",
      "Iteration 192, loss = 0.00847097\n",
      "Iteration 193, loss = 0.00838628\n",
      "Iteration 194, loss = 0.00830069\n",
      "Iteration 195, loss = 0.00822892\n",
      "Iteration 196, loss = 0.00816018\n",
      "Iteration 197, loss = 0.00807953\n",
      "Iteration 198, loss = 0.00800841\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42801105\n",
      "Iteration 2, loss = 0.39074648\n",
      "Iteration 3, loss = 0.38816968\n",
      "Iteration 4, loss = 0.38782530\n",
      "Iteration 5, loss = 0.38672652\n",
      "Iteration 6, loss = 0.38707970\n",
      "Iteration 7, loss = 0.38627609\n",
      "Iteration 8, loss = 0.38458873\n",
      "Iteration 9, loss = 0.38336007\n",
      "Iteration 10, loss = 0.38299339\n",
      "Iteration 11, loss = 0.38153835\n",
      "Iteration 12, loss = 0.38018994\n",
      "Iteration 13, loss = 0.37898285\n",
      "Iteration 14, loss = 0.37718791\n",
      "Iteration 15, loss = 0.37545113\n",
      "Iteration 16, loss = 0.37365108\n",
      "Iteration 17, loss = 0.37103968\n",
      "Iteration 18, loss = 0.36893685\n",
      "Iteration 19, loss = 0.36547412\n",
      "Iteration 20, loss = 0.36237907\n",
      "Iteration 21, loss = 0.35782307\n",
      "Iteration 22, loss = 0.35359839\n",
      "Iteration 23, loss = 0.34906845\n",
      "Iteration 24, loss = 0.34275897\n",
      "Iteration 25, loss = 0.33640308\n",
      "Iteration 26, loss = 0.32919514\n",
      "Iteration 27, loss = 0.32166605\n",
      "Iteration 28, loss = 0.31255594\n",
      "Iteration 29, loss = 0.30377974\n",
      "Iteration 30, loss = 0.29263442\n",
      "Iteration 31, loss = 0.28226763\n",
      "Iteration 32, loss = 0.27047347\n",
      "Iteration 33, loss = 0.25847356\n",
      "Iteration 34, loss = 0.24687801\n",
      "Iteration 35, loss = 0.23491477\n",
      "Iteration 36, loss = 0.22316227\n",
      "Iteration 37, loss = 0.21198550\n",
      "Iteration 38, loss = 0.20037486\n",
      "Iteration 39, loss = 0.19072547\n",
      "Iteration 40, loss = 0.18042405\n",
      "Iteration 41, loss = 0.17112126\n",
      "Iteration 42, loss = 0.16269399\n",
      "Iteration 43, loss = 0.15516857\n",
      "Iteration 44, loss = 0.14685612\n",
      "Iteration 45, loss = 0.14084455\n",
      "Iteration 46, loss = 0.13440998\n",
      "Iteration 47, loss = 0.12843734\n",
      "Iteration 48, loss = 0.12267262\n",
      "Iteration 49, loss = 0.11780019\n",
      "Iteration 50, loss = 0.11334899\n",
      "Iteration 51, loss = 0.10918709\n",
      "Iteration 52, loss = 0.10493680\n",
      "Iteration 53, loss = 0.10102897\n",
      "Iteration 54, loss = 0.09751494\n",
      "Iteration 55, loss = 0.09431302\n",
      "Iteration 56, loss = 0.09122777\n",
      "Iteration 57, loss = 0.08864928\n",
      "Iteration 58, loss = 0.08540338\n",
      "Iteration 59, loss = 0.08281311\n",
      "Iteration 60, loss = 0.08035218\n",
      "Iteration 61, loss = 0.07817308\n",
      "Iteration 62, loss = 0.07577857\n",
      "Iteration 63, loss = 0.07362038\n",
      "Iteration 64, loss = 0.07146937\n",
      "Iteration 65, loss = 0.06974374\n",
      "Iteration 66, loss = 0.06771203\n",
      "Iteration 67, loss = 0.06607243\n",
      "Iteration 68, loss = 0.06411630\n",
      "Iteration 69, loss = 0.06265079\n",
      "Iteration 70, loss = 0.06102335\n",
      "Iteration 71, loss = 0.05965128\n",
      "Iteration 72, loss = 0.05803978\n",
      "Iteration 73, loss = 0.05679545\n",
      "Iteration 74, loss = 0.05507829\n",
      "Iteration 75, loss = 0.05405855\n",
      "Iteration 76, loss = 0.05284099\n",
      "Iteration 77, loss = 0.05133434\n",
      "Iteration 78, loss = 0.05025382\n",
      "Iteration 79, loss = 0.04906526\n",
      "Iteration 80, loss = 0.04797325\n",
      "Iteration 81, loss = 0.04684548\n",
      "Iteration 82, loss = 0.04581773\n",
      "Iteration 83, loss = 0.04478407\n",
      "Iteration 84, loss = 0.04380669\n",
      "Iteration 85, loss = 0.04299147\n",
      "Iteration 86, loss = 0.04196065\n",
      "Iteration 87, loss = 0.04113484\n",
      "Iteration 88, loss = 0.04037113\n",
      "Iteration 89, loss = 0.03930226\n",
      "Iteration 90, loss = 0.03858482\n",
      "Iteration 91, loss = 0.03769949\n",
      "Iteration 92, loss = 0.03702896\n",
      "Iteration 93, loss = 0.03627914\n",
      "Iteration 94, loss = 0.03551442\n",
      "Iteration 95, loss = 0.03482818\n",
      "Iteration 96, loss = 0.03415133\n",
      "Iteration 97, loss = 0.03330407\n",
      "Iteration 98, loss = 0.03282944\n",
      "Iteration 99, loss = 0.03200060\n",
      "Iteration 100, loss = 0.03146510\n",
      "Iteration 101, loss = 0.03086909\n",
      "Iteration 102, loss = 0.03026983\n",
      "Iteration 103, loss = 0.02965301\n",
      "Iteration 104, loss = 0.02909212\n",
      "Iteration 105, loss = 0.02859592\n",
      "Iteration 106, loss = 0.02797119\n",
      "Iteration 107, loss = 0.02778431\n",
      "Iteration 108, loss = 0.02691083\n",
      "Iteration 109, loss = 0.02655196\n",
      "Iteration 110, loss = 0.02591971\n",
      "Iteration 111, loss = 0.02548842\n",
      "Iteration 112, loss = 0.02503748\n",
      "Iteration 113, loss = 0.02457006\n",
      "Iteration 114, loss = 0.02407567\n",
      "Iteration 115, loss = 0.02372297\n",
      "Iteration 116, loss = 0.02327775\n",
      "Iteration 117, loss = 0.02284973\n",
      "Iteration 118, loss = 0.02246395\n",
      "Iteration 119, loss = 0.02206258\n",
      "Iteration 120, loss = 0.02167173\n",
      "Iteration 121, loss = 0.02136099\n",
      "Iteration 122, loss = 0.02101653\n",
      "Iteration 123, loss = 0.02066392\n",
      "Iteration 124, loss = 0.02033911\n",
      "Iteration 125, loss = 0.01996471\n",
      "Iteration 126, loss = 0.01969290\n",
      "Iteration 127, loss = 0.01943854\n",
      "Iteration 128, loss = 0.01912603\n",
      "Iteration 129, loss = 0.01873923\n",
      "Iteration 130, loss = 0.01846699\n",
      "Iteration 131, loss = 0.01816439\n",
      "Iteration 132, loss = 0.01784156\n",
      "Iteration 133, loss = 0.01756511\n",
      "Iteration 134, loss = 0.01733816\n",
      "Iteration 135, loss = 0.01705620\n",
      "Iteration 136, loss = 0.01678312\n",
      "Iteration 137, loss = 0.01656174\n",
      "Iteration 138, loss = 0.01633535\n",
      "Iteration 139, loss = 0.01604625\n",
      "Iteration 140, loss = 0.01582256\n",
      "Iteration 141, loss = 0.01563687\n",
      "Iteration 142, loss = 0.01535921\n",
      "Iteration 143, loss = 0.01512696\n",
      "Iteration 144, loss = 0.01492775\n",
      "Iteration 145, loss = 0.01482877\n",
      "Iteration 146, loss = 0.01455384\n",
      "Iteration 147, loss = 0.01431055\n",
      "Iteration 148, loss = 0.01413334\n",
      "Iteration 149, loss = 0.01392662\n",
      "Iteration 150, loss = 0.01374980\n",
      "Iteration 151, loss = 0.01356282\n",
      "Iteration 152, loss = 0.01338507\n",
      "Iteration 153, loss = 0.01320703\n",
      "Iteration 154, loss = 0.01302765\n",
      "Iteration 155, loss = 0.01287980\n",
      "Iteration 156, loss = 0.01269380\n",
      "Iteration 157, loss = 0.01257060\n",
      "Iteration 158, loss = 0.01238268\n",
      "Iteration 159, loss = 0.01225977\n",
      "Iteration 160, loss = 0.01209155\n",
      "Iteration 161, loss = 0.01199289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 162, loss = 0.01178453\n",
      "Iteration 163, loss = 0.01164439\n",
      "Iteration 164, loss = 0.01152057\n",
      "Iteration 165, loss = 0.01139116\n",
      "Iteration 166, loss = 0.01124668\n",
      "Iteration 167, loss = 0.01111749\n",
      "Iteration 168, loss = 0.01098454\n",
      "Iteration 169, loss = 0.01084369\n",
      "Iteration 170, loss = 0.01072217\n",
      "Iteration 171, loss = 0.01058567\n",
      "Iteration 172, loss = 0.01047784\n",
      "Iteration 173, loss = 0.01037444\n",
      "Iteration 174, loss = 0.01024511\n",
      "Iteration 175, loss = 0.01013744\n",
      "Iteration 176, loss = 0.01001719\n",
      "Iteration 177, loss = 0.00993351\n",
      "Iteration 178, loss = 0.00983733\n",
      "Iteration 179, loss = 0.00973109\n",
      "Iteration 180, loss = 0.00961597\n",
      "Iteration 181, loss = 0.00948954\n",
      "Iteration 182, loss = 0.00939283\n",
      "Iteration 183, loss = 0.00930157\n",
      "Iteration 184, loss = 0.00919546\n",
      "Iteration 185, loss = 0.00909656\n",
      "Iteration 186, loss = 0.00901042\n",
      "Iteration 187, loss = 0.00895068\n",
      "Iteration 188, loss = 0.00884129\n",
      "Iteration 189, loss = 0.00873892\n",
      "Iteration 190, loss = 0.00865236\n",
      "Iteration 191, loss = 0.00858464\n",
      "Iteration 192, loss = 0.00847998\n",
      "Iteration 193, loss = 0.00840334\n",
      "Iteration 194, loss = 0.00833003\n",
      "Iteration 195, loss = 0.00824219\n",
      "Iteration 196, loss = 0.00816202\n",
      "Iteration 197, loss = 0.00808966\n",
      "Iteration 198, loss = 0.00802312\n",
      "Iteration 199, loss = 0.00793880\n",
      "Iteration 200, loss = 0.00788053\n",
      "Iteration 201, loss = 0.00781894\n",
      "Iteration 202, loss = 0.00773216\n",
      "Iteration 203, loss = 0.00768591\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Temps d'execution = 163.202459 seconde\n",
      "\n",
      "Cross-validation F1 ham scores: [0.98972251 0.98658411 0.99059561 0.99234303 0.99131323 0.98757764\n",
      " 0.98878695 0.9881137  0.9849037  0.98600311]\n",
      "Iteration 1, loss = 0.45872420\n",
      "Iteration 2, loss = 0.39619118\n",
      "Iteration 3, loss = 0.39363119\n",
      "Iteration 4, loss = 0.39303067\n",
      "Iteration 5, loss = 0.39228793\n",
      "Iteration 6, loss = 0.39292603\n",
      "Iteration 7, loss = 0.39100021\n",
      "Iteration 8, loss = 0.39060778\n",
      "Iteration 9, loss = 0.38947677\n",
      "Iteration 10, loss = 0.38917004\n",
      "Iteration 11, loss = 0.38745952\n",
      "Iteration 12, loss = 0.38682733\n",
      "Iteration 13, loss = 0.38525163\n",
      "Iteration 14, loss = 0.38425306\n",
      "Iteration 15, loss = 0.38268216\n",
      "Iteration 16, loss = 0.38019235\n",
      "Iteration 17, loss = 0.37754462\n",
      "Iteration 18, loss = 0.37493312\n",
      "Iteration 19, loss = 0.37245562\n",
      "Iteration 20, loss = 0.36820730\n",
      "Iteration 21, loss = 0.36418075\n",
      "Iteration 22, loss = 0.35969325\n",
      "Iteration 23, loss = 0.35457524\n",
      "Iteration 24, loss = 0.34843343\n",
      "Iteration 25, loss = 0.34235710\n",
      "Iteration 26, loss = 0.33424280\n",
      "Iteration 27, loss = 0.32576634\n",
      "Iteration 28, loss = 0.31722652\n",
      "Iteration 29, loss = 0.30700355\n",
      "Iteration 30, loss = 0.29551247\n",
      "Iteration 31, loss = 0.28421522\n",
      "Iteration 32, loss = 0.27226572\n",
      "Iteration 33, loss = 0.26031247\n",
      "Iteration 34, loss = 0.24789700\n",
      "Iteration 35, loss = 0.23609894\n",
      "Iteration 36, loss = 0.22362707\n",
      "Iteration 37, loss = 0.21213292\n",
      "Iteration 38, loss = 0.20150688\n",
      "Iteration 39, loss = 0.19043516\n",
      "Iteration 40, loss = 0.18113668\n",
      "Iteration 41, loss = 0.17194365\n",
      "Iteration 42, loss = 0.16314399\n",
      "Iteration 43, loss = 0.15529604\n",
      "Iteration 44, loss = 0.14821680\n",
      "Iteration 45, loss = 0.14152257\n",
      "Iteration 46, loss = 0.13644472\n",
      "Iteration 47, loss = 0.13041130\n",
      "Iteration 48, loss = 0.12464468\n",
      "Iteration 49, loss = 0.11971399\n",
      "Iteration 50, loss = 0.11511958\n",
      "Iteration 51, loss = 0.11161695\n",
      "Iteration 52, loss = 0.10690717\n",
      "Iteration 53, loss = 0.10344628\n",
      "Iteration 54, loss = 0.09983477\n",
      "Iteration 55, loss = 0.09674040\n",
      "Iteration 56, loss = 0.09348419\n",
      "Iteration 57, loss = 0.09087580\n",
      "Iteration 58, loss = 0.08797783\n",
      "Iteration 59, loss = 0.08530707\n",
      "Iteration 60, loss = 0.08282744\n",
      "Iteration 61, loss = 0.08069439\n",
      "Iteration 62, loss = 0.07829193\n",
      "Iteration 63, loss = 0.07600832\n",
      "Iteration 64, loss = 0.07375732\n",
      "Iteration 65, loss = 0.07226896\n",
      "Iteration 66, loss = 0.06997986\n",
      "Iteration 67, loss = 0.06818818\n",
      "Iteration 68, loss = 0.06638909\n",
      "Iteration 69, loss = 0.06509817\n",
      "Iteration 70, loss = 0.06321922\n",
      "Iteration 71, loss = 0.06177347\n",
      "Iteration 72, loss = 0.06017707\n",
      "Iteration 73, loss = 0.05900821\n",
      "Iteration 74, loss = 0.05748938\n",
      "Iteration 75, loss = 0.05609759\n",
      "Iteration 76, loss = 0.05488316\n",
      "Iteration 77, loss = 0.05354440\n",
      "Iteration 78, loss = 0.05236448\n",
      "Iteration 79, loss = 0.05115482\n",
      "Iteration 80, loss = 0.05014247\n",
      "Iteration 81, loss = 0.04896650\n",
      "Iteration 82, loss = 0.04807977\n",
      "Iteration 83, loss = 0.04681579\n",
      "Iteration 84, loss = 0.04591870\n",
      "Iteration 85, loss = 0.04488504\n",
      "Iteration 86, loss = 0.04387558\n",
      "Iteration 87, loss = 0.04293260\n",
      "Iteration 88, loss = 0.04212605\n",
      "Iteration 89, loss = 0.04136767\n",
      "Iteration 90, loss = 0.04025707\n",
      "Iteration 91, loss = 0.03956755\n",
      "Iteration 92, loss = 0.03866569\n",
      "Iteration 93, loss = 0.03790873\n",
      "Iteration 94, loss = 0.03712247\n",
      "Iteration 95, loss = 0.03641819\n",
      "Iteration 96, loss = 0.03573053\n",
      "Iteration 97, loss = 0.03500984\n",
      "Iteration 98, loss = 0.03446683\n",
      "Iteration 99, loss = 0.03366271\n",
      "Iteration 100, loss = 0.03307299\n",
      "Iteration 101, loss = 0.03239296\n",
      "Iteration 102, loss = 0.03185862\n",
      "Iteration 103, loss = 0.03114205\n",
      "Iteration 104, loss = 0.03085585\n",
      "Iteration 105, loss = 0.03000140\n",
      "Iteration 106, loss = 0.02951000\n",
      "Iteration 107, loss = 0.02897282\n",
      "Iteration 108, loss = 0.02848028\n",
      "Iteration 109, loss = 0.02788185\n",
      "Iteration 110, loss = 0.02735952\n",
      "Iteration 111, loss = 0.02696290\n",
      "Iteration 112, loss = 0.02635321\n",
      "Iteration 113, loss = 0.02597320\n",
      "Iteration 114, loss = 0.02553470\n",
      "Iteration 115, loss = 0.02503385\n",
      "Iteration 116, loss = 0.02466233\n",
      "Iteration 117, loss = 0.02417166\n",
      "Iteration 118, loss = 0.02378471\n",
      "Iteration 119, loss = 0.02346128\n",
      "Iteration 120, loss = 0.02301161\n",
      "Iteration 121, loss = 0.02261498\n",
      "Iteration 122, loss = 0.02219994\n",
      "Iteration 123, loss = 0.02184002\n",
      "Iteration 124, loss = 0.02151144\n",
      "Iteration 125, loss = 0.02132227\n",
      "Iteration 126, loss = 0.02084960\n",
      "Iteration 127, loss = 0.02049112\n",
      "Iteration 128, loss = 0.02011203\n",
      "Iteration 129, loss = 0.01995794\n",
      "Iteration 130, loss = 0.01953116\n",
      "Iteration 131, loss = 0.01929148\n",
      "Iteration 132, loss = 0.01892998\n",
      "Iteration 133, loss = 0.01863024\n",
      "Iteration 134, loss = 0.01834977\n",
      "Iteration 135, loss = 0.01814119\n",
      "Iteration 136, loss = 0.01782586\n",
      "Iteration 137, loss = 0.01751869\n",
      "Iteration 138, loss = 0.01728719\n",
      "Iteration 139, loss = 0.01702850\n",
      "Iteration 140, loss = 0.01681336\n",
      "Iteration 141, loss = 0.01655738\n",
      "Iteration 142, loss = 0.01631734\n",
      "Iteration 143, loss = 0.01604995\n",
      "Iteration 144, loss = 0.01587829\n",
      "Iteration 145, loss = 0.01562596\n",
      "Iteration 146, loss = 0.01541129\n",
      "Iteration 147, loss = 0.01521303\n",
      "Iteration 148, loss = 0.01502375\n",
      "Iteration 149, loss = 0.01481855\n",
      "Iteration 150, loss = 0.01457367\n",
      "Iteration 151, loss = 0.01448161\n",
      "Iteration 152, loss = 0.01426208\n",
      "Iteration 153, loss = 0.01400697\n",
      "Iteration 154, loss = 0.01381004\n",
      "Iteration 155, loss = 0.01368135\n",
      "Iteration 156, loss = 0.01345592\n",
      "Iteration 157, loss = 0.01330476\n",
      "Iteration 158, loss = 0.01313499\n",
      "Iteration 159, loss = 0.01298229\n",
      "Iteration 160, loss = 0.01279684\n",
      "Iteration 161, loss = 0.01266425\n",
      "Iteration 162, loss = 0.01254568\n",
      "Iteration 163, loss = 0.01237142\n",
      "Iteration 164, loss = 0.01228027\n",
      "Iteration 165, loss = 0.01204590\n",
      "Iteration 166, loss = 0.01189464\n",
      "Iteration 167, loss = 0.01175337\n",
      "Iteration 168, loss = 0.01163585\n",
      "Iteration 169, loss = 0.01159709\n",
      "Iteration 170, loss = 0.01137463\n",
      "Iteration 171, loss = 0.01123084\n",
      "Iteration 172, loss = 0.01109096\n",
      "Iteration 173, loss = 0.01097989\n",
      "Iteration 174, loss = 0.01086218\n",
      "Iteration 175, loss = 0.01078309\n",
      "Iteration 176, loss = 0.01061038\n",
      "Iteration 177, loss = 0.01051452\n",
      "Iteration 178, loss = 0.01041863\n",
      "Iteration 179, loss = 0.01028882\n",
      "Iteration 180, loss = 0.01018002\n",
      "Iteration 181, loss = 0.01006973\n",
      "Iteration 182, loss = 0.00996802\n",
      "Iteration 183, loss = 0.00984536\n",
      "Iteration 184, loss = 0.00977637\n",
      "Iteration 185, loss = 0.00964839\n",
      "Iteration 186, loss = 0.00954936\n",
      "Iteration 187, loss = 0.00946575\n",
      "Iteration 188, loss = 0.00937596\n",
      "Iteration 189, loss = 0.00927139\n",
      "Iteration 190, loss = 0.00917160\n",
      "Iteration 191, loss = 0.00909718\n",
      "Iteration 192, loss = 0.00898342\n",
      "Iteration 193, loss = 0.00891979\n",
      "Iteration 194, loss = 0.00881791\n",
      "Iteration 195, loss = 0.00874296\n",
      "Iteration 196, loss = 0.00866657\n",
      "Iteration 197, loss = 0.00863033\n",
      "Iteration 198, loss = 0.00849391\n",
      "Iteration 199, loss = 0.00840804\n",
      "Iteration 200, loss = 0.00832510\n",
      "Iteration 201, loss = 0.00825292\n",
      "Iteration 202, loss = 0.00818030\n",
      "Iteration 203, loss = 0.00810488\n",
      "Iteration 204, loss = 0.00803963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 205, loss = 0.00796242\n",
      "Iteration 206, loss = 0.00789957\n",
      "Iteration 207, loss = 0.00781705\n",
      "Iteration 208, loss = 0.00777839\n",
      "Iteration 209, loss = 0.00768393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45249319\n",
      "Iteration 2, loss = 0.39434839\n",
      "Iteration 3, loss = 0.39219741\n",
      "Iteration 4, loss = 0.39140486\n",
      "Iteration 5, loss = 0.39024451\n",
      "Iteration 6, loss = 0.38959137\n",
      "Iteration 7, loss = 0.38965507\n",
      "Iteration 8, loss = 0.38813613\n",
      "Iteration 9, loss = 0.38817740\n",
      "Iteration 10, loss = 0.38689604\n",
      "Iteration 11, loss = 0.38545273\n",
      "Iteration 12, loss = 0.38421407\n",
      "Iteration 13, loss = 0.38293409\n",
      "Iteration 14, loss = 0.38118962\n",
      "Iteration 15, loss = 0.37982477\n",
      "Iteration 16, loss = 0.37759453\n",
      "Iteration 17, loss = 0.37556196\n",
      "Iteration 18, loss = 0.37235978\n",
      "Iteration 19, loss = 0.36945302\n",
      "Iteration 20, loss = 0.36555062\n",
      "Iteration 21, loss = 0.36219631\n",
      "Iteration 22, loss = 0.35680567\n",
      "Iteration 23, loss = 0.35119974\n",
      "Iteration 24, loss = 0.34487585\n",
      "Iteration 25, loss = 0.33845953\n",
      "Iteration 26, loss = 0.33069673\n",
      "Iteration 27, loss = 0.32201297\n",
      "Iteration 28, loss = 0.31286015\n",
      "Iteration 29, loss = 0.30288646\n",
      "Iteration 30, loss = 0.29149812\n",
      "Iteration 31, loss = 0.28016133\n",
      "Iteration 32, loss = 0.26786723\n",
      "Iteration 33, loss = 0.25586719\n",
      "Iteration 34, loss = 0.24295944\n",
      "Iteration 35, loss = 0.23074485\n",
      "Iteration 36, loss = 0.21855501\n",
      "Iteration 37, loss = 0.20670068\n",
      "Iteration 38, loss = 0.19544723\n",
      "Iteration 39, loss = 0.18513774\n",
      "Iteration 40, loss = 0.17563926\n",
      "Iteration 41, loss = 0.16696334\n",
      "Iteration 42, loss = 0.15797909\n",
      "Iteration 43, loss = 0.15012181\n",
      "Iteration 44, loss = 0.14399359\n",
      "Iteration 45, loss = 0.13720701\n",
      "Iteration 46, loss = 0.13081371\n",
      "Iteration 47, loss = 0.12543868\n",
      "Iteration 48, loss = 0.12008141\n",
      "Iteration 49, loss = 0.11577967\n",
      "Iteration 50, loss = 0.11145361\n",
      "Iteration 51, loss = 0.10686618\n",
      "Iteration 52, loss = 0.10317467\n",
      "Iteration 53, loss = 0.09951897\n",
      "Iteration 54, loss = 0.09614617\n",
      "Iteration 55, loss = 0.09289335\n",
      "Iteration 56, loss = 0.09029294\n",
      "Iteration 57, loss = 0.08728432\n",
      "Iteration 58, loss = 0.08455131\n",
      "Iteration 59, loss = 0.08215134\n",
      "Iteration 60, loss = 0.07967643\n",
      "Iteration 61, loss = 0.07754103\n",
      "Iteration 62, loss = 0.07541212\n",
      "Iteration 63, loss = 0.07338594\n",
      "Iteration 64, loss = 0.07114089\n",
      "Iteration 65, loss = 0.06935876\n",
      "Iteration 66, loss = 0.06753914\n",
      "Iteration 67, loss = 0.06580361\n",
      "Iteration 68, loss = 0.06446644\n",
      "Iteration 69, loss = 0.06275700\n",
      "Iteration 70, loss = 0.06135888\n",
      "Iteration 71, loss = 0.05970058\n",
      "Iteration 72, loss = 0.05825033\n",
      "Iteration 73, loss = 0.05716808\n",
      "Iteration 74, loss = 0.05570436\n",
      "Iteration 75, loss = 0.05419304\n",
      "Iteration 76, loss = 0.05335356\n",
      "Iteration 77, loss = 0.05195400\n",
      "Iteration 78, loss = 0.05100672\n",
      "Iteration 79, loss = 0.04963555\n",
      "Iteration 80, loss = 0.04887411\n",
      "Iteration 81, loss = 0.04751792\n",
      "Iteration 82, loss = 0.04649342\n",
      "Iteration 83, loss = 0.04540898\n",
      "Iteration 84, loss = 0.04470342\n",
      "Iteration 85, loss = 0.04364632\n",
      "Iteration 86, loss = 0.04274635\n",
      "Iteration 87, loss = 0.04190578\n",
      "Iteration 88, loss = 0.04100327\n",
      "Iteration 89, loss = 0.04018697\n",
      "Iteration 90, loss = 0.03941551\n",
      "Iteration 91, loss = 0.03857875\n",
      "Iteration 92, loss = 0.03794076\n",
      "Iteration 93, loss = 0.03706007\n",
      "Iteration 94, loss = 0.03645071\n",
      "Iteration 95, loss = 0.03566032\n",
      "Iteration 96, loss = 0.03495159\n",
      "Iteration 97, loss = 0.03421511\n",
      "Iteration 98, loss = 0.03389214\n",
      "Iteration 99, loss = 0.03305199\n",
      "Iteration 100, loss = 0.03236390\n",
      "Iteration 101, loss = 0.03187229\n",
      "Iteration 102, loss = 0.03118342\n",
      "Iteration 103, loss = 0.03061729\n",
      "Iteration 104, loss = 0.02998162\n",
      "Iteration 105, loss = 0.02946277\n",
      "Iteration 106, loss = 0.02892898\n",
      "Iteration 107, loss = 0.02840014\n",
      "Iteration 108, loss = 0.02790250\n",
      "Iteration 109, loss = 0.02745313\n",
      "Iteration 110, loss = 0.02699929\n",
      "Iteration 111, loss = 0.02647938\n",
      "Iteration 112, loss = 0.02594811\n",
      "Iteration 113, loss = 0.02566158\n",
      "Iteration 114, loss = 0.02515650\n",
      "Iteration 115, loss = 0.02469293\n",
      "Iteration 116, loss = 0.02435777\n",
      "Iteration 117, loss = 0.02382452\n",
      "Iteration 118, loss = 0.02341692\n",
      "Iteration 119, loss = 0.02305894\n",
      "Iteration 120, loss = 0.02267215\n",
      "Iteration 121, loss = 0.02227205\n",
      "Iteration 122, loss = 0.02192747\n",
      "Iteration 123, loss = 0.02157779\n",
      "Iteration 124, loss = 0.02126033\n",
      "Iteration 125, loss = 0.02083267\n",
      "Iteration 126, loss = 0.02052311\n",
      "Iteration 127, loss = 0.02024227\n",
      "Iteration 128, loss = 0.01990537\n",
      "Iteration 129, loss = 0.01955231\n",
      "Iteration 130, loss = 0.01937184\n",
      "Iteration 131, loss = 0.01896768\n",
      "Iteration 132, loss = 0.01869286\n",
      "Iteration 133, loss = 0.01842405\n",
      "Iteration 134, loss = 0.01815173\n",
      "Iteration 135, loss = 0.01791465\n",
      "Iteration 136, loss = 0.01758031\n",
      "Iteration 137, loss = 0.01739717\n",
      "Iteration 138, loss = 0.01710801\n",
      "Iteration 139, loss = 0.01683701\n",
      "Iteration 140, loss = 0.01663984\n",
      "Iteration 141, loss = 0.01636185\n",
      "Iteration 142, loss = 0.01615126\n",
      "Iteration 143, loss = 0.01590892\n",
      "Iteration 144, loss = 0.01575678\n",
      "Iteration 145, loss = 0.01551555\n",
      "Iteration 146, loss = 0.01527409\n",
      "Iteration 147, loss = 0.01509678\n",
      "Iteration 148, loss = 0.01493504\n",
      "Iteration 149, loss = 0.01470486\n",
      "Iteration 150, loss = 0.01451711\n",
      "Iteration 151, loss = 0.01433041\n",
      "Iteration 152, loss = 0.01417233\n",
      "Iteration 153, loss = 0.01391681\n",
      "Iteration 154, loss = 0.01385529\n",
      "Iteration 155, loss = 0.01361267\n",
      "Iteration 156, loss = 0.01339917\n",
      "Iteration 157, loss = 0.01328331\n",
      "Iteration 158, loss = 0.01309121\n",
      "Iteration 159, loss = 0.01290854\n",
      "Iteration 160, loss = 0.01275867\n",
      "Iteration 161, loss = 0.01261712\n",
      "Iteration 162, loss = 0.01244380\n",
      "Iteration 163, loss = 0.01231657\n",
      "Iteration 164, loss = 0.01214092\n",
      "Iteration 165, loss = 0.01201450\n",
      "Iteration 166, loss = 0.01184074\n",
      "Iteration 167, loss = 0.01172417\n",
      "Iteration 168, loss = 0.01157714\n",
      "Iteration 169, loss = 0.01146069\n",
      "Iteration 170, loss = 0.01131491\n",
      "Iteration 171, loss = 0.01118432\n",
      "Iteration 172, loss = 0.01108156\n",
      "Iteration 173, loss = 0.01092928\n",
      "Iteration 174, loss = 0.01080136\n",
      "Iteration 175, loss = 0.01069837\n",
      "Iteration 176, loss = 0.01055573\n",
      "Iteration 177, loss = 0.01043802\n",
      "Iteration 178, loss = 0.01033393\n",
      "Iteration 179, loss = 0.01022541\n",
      "Iteration 180, loss = 0.01011870\n",
      "Iteration 181, loss = 0.01001513\n",
      "Iteration 182, loss = 0.00990482\n",
      "Iteration 183, loss = 0.00981400\n",
      "Iteration 184, loss = 0.00971580\n",
      "Iteration 185, loss = 0.00965929\n",
      "Iteration 186, loss = 0.00952023\n",
      "Iteration 187, loss = 0.00944795\n",
      "Iteration 188, loss = 0.00932897\n",
      "Iteration 189, loss = 0.00923452\n",
      "Iteration 190, loss = 0.00914142\n",
      "Iteration 191, loss = 0.00905324\n",
      "Iteration 192, loss = 0.00896401\n",
      "Iteration 193, loss = 0.00888375\n",
      "Iteration 194, loss = 0.00880491\n",
      "Iteration 195, loss = 0.00871458\n",
      "Iteration 196, loss = 0.00866133\n",
      "Iteration 197, loss = 0.00856518\n",
      "Iteration 198, loss = 0.00846812\n",
      "Iteration 199, loss = 0.00841162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44896314\n",
      "Iteration 2, loss = 0.38801499\n",
      "Iteration 3, loss = 0.38666657\n",
      "Iteration 4, loss = 0.38648695\n",
      "Iteration 5, loss = 0.38615072\n",
      "Iteration 6, loss = 0.38551034\n",
      "Iteration 7, loss = 0.38437532\n",
      "Iteration 8, loss = 0.38402247\n",
      "Iteration 9, loss = 0.38304286\n",
      "Iteration 10, loss = 0.38219654\n",
      "Iteration 11, loss = 0.38116108\n",
      "Iteration 12, loss = 0.37996156\n",
      "Iteration 13, loss = 0.37893409\n",
      "Iteration 14, loss = 0.37799499\n",
      "Iteration 15, loss = 0.37578487\n",
      "Iteration 16, loss = 0.37366730\n",
      "Iteration 17, loss = 0.37222990\n",
      "Iteration 18, loss = 0.36909708\n",
      "Iteration 19, loss = 0.36627444\n",
      "Iteration 20, loss = 0.36268578\n",
      "Iteration 21, loss = 0.35974615\n",
      "Iteration 22, loss = 0.35482380\n",
      "Iteration 23, loss = 0.34977787\n",
      "Iteration 24, loss = 0.34422248\n",
      "Iteration 25, loss = 0.33876310\n",
      "Iteration 26, loss = 0.33101267\n",
      "Iteration 27, loss = 0.32343654\n",
      "Iteration 28, loss = 0.31511011\n",
      "Iteration 29, loss = 0.30526221\n",
      "Iteration 30, loss = 0.29483822\n",
      "Iteration 31, loss = 0.28514791\n",
      "Iteration 32, loss = 0.27390186\n",
      "Iteration 33, loss = 0.26150565\n",
      "Iteration 34, loss = 0.24967503\n",
      "Iteration 35, loss = 0.23775120\n",
      "Iteration 36, loss = 0.22579554\n",
      "Iteration 37, loss = 0.21450911\n",
      "Iteration 38, loss = 0.20355403\n",
      "Iteration 39, loss = 0.19319691\n",
      "Iteration 40, loss = 0.18362523\n",
      "Iteration 41, loss = 0.17437770\n",
      "Iteration 42, loss = 0.16615519\n",
      "Iteration 43, loss = 0.15787106\n",
      "Iteration 44, loss = 0.15079599\n",
      "Iteration 45, loss = 0.14383116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.13753948\n",
      "Iteration 47, loss = 0.13165743\n",
      "Iteration 48, loss = 0.12676666\n",
      "Iteration 49, loss = 0.12206960\n",
      "Iteration 50, loss = 0.11702380\n",
      "Iteration 51, loss = 0.11263324\n",
      "Iteration 52, loss = 0.10883414\n",
      "Iteration 53, loss = 0.10493889\n",
      "Iteration 54, loss = 0.10132716\n",
      "Iteration 55, loss = 0.09798216\n",
      "Iteration 56, loss = 0.09511332\n",
      "Iteration 57, loss = 0.09185097\n",
      "Iteration 58, loss = 0.08925176\n",
      "Iteration 59, loss = 0.08657059\n",
      "Iteration 60, loss = 0.08394984\n",
      "Iteration 61, loss = 0.08160460\n",
      "Iteration 62, loss = 0.07960727\n",
      "Iteration 63, loss = 0.07704624\n",
      "Iteration 64, loss = 0.07499284\n",
      "Iteration 65, loss = 0.07312132\n",
      "Iteration 66, loss = 0.07090034\n",
      "Iteration 67, loss = 0.06910455\n",
      "Iteration 68, loss = 0.06734360\n",
      "Iteration 69, loss = 0.06565088\n",
      "Iteration 70, loss = 0.06395062\n",
      "Iteration 71, loss = 0.06257697\n",
      "Iteration 72, loss = 0.06090787\n",
      "Iteration 73, loss = 0.05973655\n",
      "Iteration 74, loss = 0.05802474\n",
      "Iteration 75, loss = 0.05670539\n",
      "Iteration 76, loss = 0.05532122\n",
      "Iteration 77, loss = 0.05444524\n",
      "Iteration 78, loss = 0.05292616\n",
      "Iteration 79, loss = 0.05176762\n",
      "Iteration 80, loss = 0.05057533\n",
      "Iteration 81, loss = 0.04954752\n",
      "Iteration 82, loss = 0.04834832\n",
      "Iteration 83, loss = 0.04736938\n",
      "Iteration 84, loss = 0.04627340\n",
      "Iteration 85, loss = 0.04558260\n",
      "Iteration 86, loss = 0.04436928\n",
      "Iteration 87, loss = 0.04353097\n",
      "Iteration 88, loss = 0.04252622\n",
      "Iteration 89, loss = 0.04166592\n",
      "Iteration 90, loss = 0.04074750\n",
      "Iteration 91, loss = 0.03997824\n",
      "Iteration 92, loss = 0.03910101\n",
      "Iteration 93, loss = 0.03837449\n",
      "Iteration 94, loss = 0.03767660\n",
      "Iteration 95, loss = 0.03695409\n",
      "Iteration 96, loss = 0.03611985\n",
      "Iteration 97, loss = 0.03556076\n",
      "Iteration 98, loss = 0.03498719\n",
      "Iteration 99, loss = 0.03423970\n",
      "Iteration 100, loss = 0.03341834\n",
      "Iteration 101, loss = 0.03289708\n",
      "Iteration 102, loss = 0.03217987\n",
      "Iteration 103, loss = 0.03209043\n",
      "Iteration 104, loss = 0.03096389\n",
      "Iteration 105, loss = 0.03046991\n",
      "Iteration 106, loss = 0.02986811\n",
      "Iteration 107, loss = 0.02926269\n",
      "Iteration 108, loss = 0.02876056\n",
      "Iteration 109, loss = 0.02825349\n",
      "Iteration 110, loss = 0.02789051\n",
      "Iteration 111, loss = 0.02724293\n",
      "Iteration 112, loss = 0.02682769\n",
      "Iteration 113, loss = 0.02639658\n",
      "Iteration 114, loss = 0.02584535\n",
      "Iteration 115, loss = 0.02535332\n",
      "Iteration 116, loss = 0.02494124\n",
      "Iteration 117, loss = 0.02449740\n",
      "Iteration 118, loss = 0.02404677\n",
      "Iteration 119, loss = 0.02373581\n",
      "Iteration 120, loss = 0.02328149\n",
      "Iteration 121, loss = 0.02298193\n",
      "Iteration 122, loss = 0.02253096\n",
      "Iteration 123, loss = 0.02207826\n",
      "Iteration 124, loss = 0.02174239\n",
      "Iteration 125, loss = 0.02136663\n",
      "Iteration 126, loss = 0.02106242\n",
      "Iteration 127, loss = 0.02076747\n",
      "Iteration 128, loss = 0.02047564\n",
      "Iteration 129, loss = 0.02006323\n",
      "Iteration 130, loss = 0.01975040\n",
      "Iteration 131, loss = 0.01960471\n",
      "Iteration 132, loss = 0.01916213\n",
      "Iteration 133, loss = 0.01883241\n",
      "Iteration 134, loss = 0.01856096\n",
      "Iteration 135, loss = 0.01826099\n",
      "Iteration 136, loss = 0.01801601\n",
      "Iteration 137, loss = 0.01773991\n",
      "Iteration 138, loss = 0.01747627\n",
      "Iteration 139, loss = 0.01724331\n",
      "Iteration 140, loss = 0.01694502\n",
      "Iteration 141, loss = 0.01676531\n",
      "Iteration 142, loss = 0.01645792\n",
      "Iteration 143, loss = 0.01624538\n",
      "Iteration 144, loss = 0.01601501\n",
      "Iteration 145, loss = 0.01580167\n",
      "Iteration 146, loss = 0.01556047\n",
      "Iteration 147, loss = 0.01533936\n",
      "Iteration 148, loss = 0.01514626\n",
      "Iteration 149, loss = 0.01495179\n",
      "Iteration 150, loss = 0.01471938\n",
      "Iteration 151, loss = 0.01452246\n",
      "Iteration 152, loss = 0.01432481\n",
      "Iteration 153, loss = 0.01410594\n",
      "Iteration 154, loss = 0.01394929\n",
      "Iteration 155, loss = 0.01375814\n",
      "Iteration 156, loss = 0.01367704\n",
      "Iteration 157, loss = 0.01349794\n",
      "Iteration 158, loss = 0.01325880\n",
      "Iteration 159, loss = 0.01305505\n",
      "Iteration 160, loss = 0.01290458\n",
      "Iteration 161, loss = 0.01273665\n",
      "Iteration 162, loss = 0.01257728\n",
      "Iteration 163, loss = 0.01243021\n",
      "Iteration 164, loss = 0.01229624\n",
      "Iteration 165, loss = 0.01217259\n",
      "Iteration 166, loss = 0.01197463\n",
      "Iteration 167, loss = 0.01182322\n",
      "Iteration 168, loss = 0.01169953\n",
      "Iteration 169, loss = 0.01155248\n",
      "Iteration 170, loss = 0.01142859\n",
      "Iteration 171, loss = 0.01128983\n",
      "Iteration 172, loss = 0.01117576\n",
      "Iteration 173, loss = 0.01103076\n",
      "Iteration 174, loss = 0.01090140\n",
      "Iteration 175, loss = 0.01078694\n",
      "Iteration 176, loss = 0.01066398\n",
      "Iteration 177, loss = 0.01062541\n",
      "Iteration 178, loss = 0.01044032\n",
      "Iteration 179, loss = 0.01034151\n",
      "Iteration 180, loss = 0.01021921\n",
      "Iteration 181, loss = 0.01009871\n",
      "Iteration 182, loss = 0.00999218\n",
      "Iteration 183, loss = 0.00989104\n",
      "Iteration 184, loss = 0.00979970\n",
      "Iteration 185, loss = 0.00969340\n",
      "Iteration 186, loss = 0.00965008\n",
      "Iteration 187, loss = 0.00948650\n",
      "Iteration 188, loss = 0.00941370\n",
      "Iteration 189, loss = 0.00929678\n",
      "Iteration 190, loss = 0.00921068\n",
      "Iteration 191, loss = 0.00911594\n",
      "Iteration 192, loss = 0.00902195\n",
      "Iteration 193, loss = 0.00893308\n",
      "Iteration 194, loss = 0.00884929\n",
      "Iteration 195, loss = 0.00876366\n",
      "Iteration 196, loss = 0.00866887\n",
      "Iteration 197, loss = 0.00859275\n",
      "Iteration 198, loss = 0.00850997\n",
      "Iteration 199, loss = 0.00843089\n",
      "Iteration 200, loss = 0.00835606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45186431\n",
      "Iteration 2, loss = 0.39840405\n",
      "Iteration 3, loss = 0.39716178\n",
      "Iteration 4, loss = 0.39785692\n",
      "Iteration 5, loss = 0.39642646\n",
      "Iteration 6, loss = 0.39548634\n",
      "Iteration 7, loss = 0.39507471\n",
      "Iteration 8, loss = 0.39422918\n",
      "Iteration 9, loss = 0.39350298\n",
      "Iteration 10, loss = 0.39282917\n",
      "Iteration 11, loss = 0.39155831\n",
      "Iteration 12, loss = 0.39057956\n",
      "Iteration 13, loss = 0.38919270\n",
      "Iteration 14, loss = 0.38741425\n",
      "Iteration 15, loss = 0.38620860\n",
      "Iteration 16, loss = 0.38381149\n",
      "Iteration 17, loss = 0.38124914\n",
      "Iteration 18, loss = 0.37863074\n",
      "Iteration 19, loss = 0.37501802\n",
      "Iteration 20, loss = 0.37155626\n",
      "Iteration 21, loss = 0.36705856\n",
      "Iteration 22, loss = 0.36292563\n",
      "Iteration 23, loss = 0.35720689\n",
      "Iteration 24, loss = 0.35097285\n",
      "Iteration 25, loss = 0.34464358\n",
      "Iteration 26, loss = 0.33641363\n",
      "Iteration 27, loss = 0.32748851\n",
      "Iteration 28, loss = 0.31828216\n",
      "Iteration 29, loss = 0.30811910\n",
      "Iteration 30, loss = 0.29708667\n",
      "Iteration 31, loss = 0.28579408\n",
      "Iteration 32, loss = 0.27314522\n",
      "Iteration 33, loss = 0.26089479\n",
      "Iteration 34, loss = 0.24845571\n",
      "Iteration 35, loss = 0.23632677\n",
      "Iteration 36, loss = 0.22402686\n",
      "Iteration 37, loss = 0.21270122\n",
      "Iteration 38, loss = 0.20143045\n",
      "Iteration 39, loss = 0.19154590\n",
      "Iteration 40, loss = 0.18145499\n",
      "Iteration 41, loss = 0.17248977\n",
      "Iteration 42, loss = 0.16465059\n",
      "Iteration 43, loss = 0.15681462\n",
      "Iteration 44, loss = 0.14965399\n",
      "Iteration 45, loss = 0.14297866\n",
      "Iteration 46, loss = 0.13707193\n",
      "Iteration 47, loss = 0.13134008\n",
      "Iteration 48, loss = 0.12623590\n",
      "Iteration 49, loss = 0.12147373\n",
      "Iteration 50, loss = 0.11719010\n",
      "Iteration 51, loss = 0.11265834\n",
      "Iteration 52, loss = 0.10881210\n",
      "Iteration 53, loss = 0.10516677\n",
      "Iteration 54, loss = 0.10194633\n",
      "Iteration 55, loss = 0.09851952\n",
      "Iteration 56, loss = 0.09547889\n",
      "Iteration 57, loss = 0.09251047\n",
      "Iteration 58, loss = 0.08991001\n",
      "Iteration 59, loss = 0.08723016\n",
      "Iteration 60, loss = 0.08474271\n",
      "Iteration 61, loss = 0.08302337\n",
      "Iteration 62, loss = 0.08091428\n",
      "Iteration 63, loss = 0.07821425\n",
      "Iteration 64, loss = 0.07605368\n",
      "Iteration 65, loss = 0.07396205\n",
      "Iteration 66, loss = 0.07211383\n",
      "Iteration 67, loss = 0.07027732\n",
      "Iteration 68, loss = 0.06841946\n",
      "Iteration 69, loss = 0.06673199\n",
      "Iteration 70, loss = 0.06565409\n",
      "Iteration 71, loss = 0.06366301\n",
      "Iteration 72, loss = 0.06201365\n",
      "Iteration 73, loss = 0.06058302\n",
      "Iteration 74, loss = 0.05927240\n",
      "Iteration 75, loss = 0.05798204\n",
      "Iteration 76, loss = 0.05667592\n",
      "Iteration 77, loss = 0.05543882\n",
      "Iteration 78, loss = 0.05399948\n",
      "Iteration 79, loss = 0.05287998\n",
      "Iteration 80, loss = 0.05184652\n",
      "Iteration 81, loss = 0.05059322\n",
      "Iteration 82, loss = 0.04942507\n",
      "Iteration 83, loss = 0.04839015\n",
      "Iteration 84, loss = 0.04740572\n",
      "Iteration 85, loss = 0.04629924\n",
      "Iteration 86, loss = 0.04589165\n",
      "Iteration 87, loss = 0.04456244\n",
      "Iteration 88, loss = 0.04362927\n",
      "Iteration 89, loss = 0.04265565\n",
      "Iteration 90, loss = 0.04179570\n",
      "Iteration 91, loss = 0.04103894\n",
      "Iteration 92, loss = 0.04034731\n",
      "Iteration 93, loss = 0.03948037\n",
      "Iteration 94, loss = 0.03866854\n",
      "Iteration 95, loss = 0.03788638\n",
      "Iteration 96, loss = 0.03710383\n",
      "Iteration 97, loss = 0.03684059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 98, loss = 0.03574915\n",
      "Iteration 99, loss = 0.03510041\n",
      "Iteration 100, loss = 0.03437921\n",
      "Iteration 101, loss = 0.03368869\n",
      "Iteration 102, loss = 0.03311217\n",
      "Iteration 103, loss = 0.03247353\n",
      "Iteration 104, loss = 0.03187874\n",
      "Iteration 105, loss = 0.03129890\n",
      "Iteration 106, loss = 0.03084867\n",
      "Iteration 107, loss = 0.03019411\n",
      "Iteration 108, loss = 0.02961480\n",
      "Iteration 109, loss = 0.02920760\n",
      "Iteration 110, loss = 0.02866535\n",
      "Iteration 111, loss = 0.02810825\n",
      "Iteration 112, loss = 0.02758651\n",
      "Iteration 113, loss = 0.02715545\n",
      "Iteration 114, loss = 0.02679648\n",
      "Iteration 115, loss = 0.02625235\n",
      "Iteration 116, loss = 0.02582055\n",
      "Iteration 117, loss = 0.02547205\n",
      "Iteration 118, loss = 0.02506271\n",
      "Iteration 119, loss = 0.02448312\n",
      "Iteration 120, loss = 0.02418701\n",
      "Iteration 121, loss = 0.02377188\n",
      "Iteration 122, loss = 0.02330948\n",
      "Iteration 123, loss = 0.02293871\n",
      "Iteration 124, loss = 0.02257710\n",
      "Iteration 125, loss = 0.02220606\n",
      "Iteration 126, loss = 0.02188633\n",
      "Iteration 127, loss = 0.02148852\n",
      "Iteration 128, loss = 0.02116041\n",
      "Iteration 129, loss = 0.02082541\n",
      "Iteration 130, loss = 0.02056831\n",
      "Iteration 131, loss = 0.02022453\n",
      "Iteration 132, loss = 0.01994209\n",
      "Iteration 133, loss = 0.01960664\n",
      "Iteration 134, loss = 0.01937774\n",
      "Iteration 135, loss = 0.01902111\n",
      "Iteration 136, loss = 0.01879787\n",
      "Iteration 137, loss = 0.01853959\n",
      "Iteration 138, loss = 0.01821951\n",
      "Iteration 139, loss = 0.01799624\n",
      "Iteration 140, loss = 0.01783448\n",
      "Iteration 141, loss = 0.01752786\n",
      "Iteration 142, loss = 0.01733809\n",
      "Iteration 143, loss = 0.01697614\n",
      "Iteration 144, loss = 0.01675196\n",
      "Iteration 145, loss = 0.01647802\n",
      "Iteration 146, loss = 0.01623866\n",
      "Iteration 147, loss = 0.01603158\n",
      "Iteration 148, loss = 0.01585099\n",
      "Iteration 149, loss = 0.01560482\n",
      "Iteration 150, loss = 0.01537876\n",
      "Iteration 151, loss = 0.01519519\n",
      "Iteration 152, loss = 0.01500894\n",
      "Iteration 153, loss = 0.01487492\n",
      "Iteration 154, loss = 0.01462073\n",
      "Iteration 155, loss = 0.01445809\n",
      "Iteration 156, loss = 0.01429283\n",
      "Iteration 157, loss = 0.01406606\n",
      "Iteration 158, loss = 0.01392855\n",
      "Iteration 159, loss = 0.01395568\n",
      "Iteration 160, loss = 0.01361373\n",
      "Iteration 161, loss = 0.01338461\n",
      "Iteration 162, loss = 0.01323556\n",
      "Iteration 163, loss = 0.01307664\n",
      "Iteration 164, loss = 0.01292804\n",
      "Iteration 165, loss = 0.01273914\n",
      "Iteration 166, loss = 0.01258917\n",
      "Iteration 167, loss = 0.01245631\n",
      "Iteration 168, loss = 0.01231552\n",
      "Iteration 169, loss = 0.01217625\n",
      "Iteration 170, loss = 0.01202264\n",
      "Iteration 171, loss = 0.01188382\n",
      "Iteration 172, loss = 0.01175296\n",
      "Iteration 173, loss = 0.01160778\n",
      "Iteration 174, loss = 0.01148553\n",
      "Iteration 175, loss = 0.01136051\n",
      "Iteration 176, loss = 0.01123311\n",
      "Iteration 177, loss = 0.01112110\n",
      "Iteration 178, loss = 0.01099559\n",
      "Iteration 179, loss = 0.01086692\n",
      "Iteration 180, loss = 0.01073925\n",
      "Iteration 181, loss = 0.01064050\n",
      "Iteration 182, loss = 0.01053966\n",
      "Iteration 183, loss = 0.01043186\n",
      "Iteration 184, loss = 0.01032105\n",
      "Iteration 185, loss = 0.01019990\n",
      "Iteration 186, loss = 0.01009991\n",
      "Iteration 187, loss = 0.01000023\n",
      "Iteration 188, loss = 0.00989373\n",
      "Iteration 189, loss = 0.00979985\n",
      "Iteration 190, loss = 0.00969516\n",
      "Iteration 191, loss = 0.00959948\n",
      "Iteration 192, loss = 0.00950597\n",
      "Iteration 193, loss = 0.00942676\n",
      "Iteration 194, loss = 0.00931466\n",
      "Iteration 195, loss = 0.00923845\n",
      "Iteration 196, loss = 0.00915046\n",
      "Iteration 197, loss = 0.00906554\n",
      "Iteration 198, loss = 0.00899381\n",
      "Iteration 199, loss = 0.00889888\n",
      "Iteration 200, loss = 0.00881195\n",
      "Iteration 201, loss = 0.00873701\n",
      "Iteration 202, loss = 0.00865454\n",
      "Iteration 203, loss = 0.00857045\n",
      "Iteration 204, loss = 0.00850224\n",
      "Iteration 205, loss = 0.00842675\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45199388\n",
      "Iteration 2, loss = 0.39863381\n",
      "Iteration 3, loss = 0.39526287\n",
      "Iteration 4, loss = 0.39531838\n",
      "Iteration 5, loss = 0.39504702\n",
      "Iteration 6, loss = 0.39362373\n",
      "Iteration 7, loss = 0.39337008\n",
      "Iteration 8, loss = 0.39237599\n",
      "Iteration 9, loss = 0.39146219\n",
      "Iteration 10, loss = 0.39130107\n",
      "Iteration 11, loss = 0.39035841\n",
      "Iteration 12, loss = 0.38845710\n",
      "Iteration 13, loss = 0.38740030\n",
      "Iteration 14, loss = 0.38549291\n",
      "Iteration 15, loss = 0.38333675\n",
      "Iteration 16, loss = 0.38150738\n",
      "Iteration 17, loss = 0.37858166\n",
      "Iteration 18, loss = 0.37586814\n",
      "Iteration 19, loss = 0.37234149\n",
      "Iteration 20, loss = 0.36860461\n",
      "Iteration 21, loss = 0.36441882\n",
      "Iteration 22, loss = 0.35901104\n",
      "Iteration 23, loss = 0.35351451\n",
      "Iteration 24, loss = 0.34691811\n",
      "Iteration 25, loss = 0.33959965\n",
      "Iteration 26, loss = 0.33171033\n",
      "Iteration 27, loss = 0.32244535\n",
      "Iteration 28, loss = 0.31274535\n",
      "Iteration 29, loss = 0.30195863\n",
      "Iteration 30, loss = 0.29074364\n",
      "Iteration 31, loss = 0.27844305\n",
      "Iteration 32, loss = 0.26575209\n",
      "Iteration 33, loss = 0.25293416\n",
      "Iteration 34, loss = 0.24034441\n",
      "Iteration 35, loss = 0.22805683\n",
      "Iteration 36, loss = 0.21574573\n",
      "Iteration 37, loss = 0.20441800\n",
      "Iteration 38, loss = 0.19334134\n",
      "Iteration 39, loss = 0.18333035\n",
      "Iteration 40, loss = 0.17350148\n",
      "Iteration 41, loss = 0.16461091\n",
      "Iteration 42, loss = 0.15673983\n",
      "Iteration 43, loss = 0.14921622\n",
      "Iteration 44, loss = 0.14267905\n",
      "Iteration 45, loss = 0.13613763\n",
      "Iteration 46, loss = 0.13044356\n",
      "Iteration 47, loss = 0.12521557\n",
      "Iteration 48, loss = 0.11988703\n",
      "Iteration 49, loss = 0.11520846\n",
      "Iteration 50, loss = 0.11106115\n",
      "Iteration 51, loss = 0.10700977\n",
      "Iteration 52, loss = 0.10369082\n",
      "Iteration 53, loss = 0.09998345\n",
      "Iteration 54, loss = 0.09648915\n",
      "Iteration 55, loss = 0.09386925\n",
      "Iteration 56, loss = 0.09052698\n",
      "Iteration 57, loss = 0.08753227\n",
      "Iteration 58, loss = 0.08515619\n",
      "Iteration 59, loss = 0.08245361\n",
      "Iteration 60, loss = 0.07999520\n",
      "Iteration 61, loss = 0.07800281\n",
      "Iteration 62, loss = 0.07573529\n",
      "Iteration 63, loss = 0.07379161\n",
      "Iteration 64, loss = 0.07164157\n",
      "Iteration 65, loss = 0.06985257\n",
      "Iteration 66, loss = 0.06801053\n",
      "Iteration 67, loss = 0.06630880\n",
      "Iteration 68, loss = 0.06478078\n",
      "Iteration 69, loss = 0.06341470\n",
      "Iteration 70, loss = 0.06175766\n",
      "Iteration 71, loss = 0.06006304\n",
      "Iteration 72, loss = 0.05841084\n",
      "Iteration 73, loss = 0.05721039\n",
      "Iteration 74, loss = 0.05601756\n",
      "Iteration 75, loss = 0.05455995\n",
      "Iteration 76, loss = 0.05340376\n",
      "Iteration 77, loss = 0.05224680\n",
      "Iteration 78, loss = 0.05094656\n",
      "Iteration 79, loss = 0.05022487\n",
      "Iteration 80, loss = 0.04862633\n",
      "Iteration 81, loss = 0.04768946\n",
      "Iteration 82, loss = 0.04672236\n",
      "Iteration 83, loss = 0.04575349\n",
      "Iteration 84, loss = 0.04469451\n",
      "Iteration 85, loss = 0.04374367\n",
      "Iteration 86, loss = 0.04273864\n",
      "Iteration 87, loss = 0.04197836\n",
      "Iteration 88, loss = 0.04097850\n",
      "Iteration 89, loss = 0.04011723\n",
      "Iteration 90, loss = 0.03938544\n",
      "Iteration 91, loss = 0.03861625\n",
      "Iteration 92, loss = 0.03784157\n",
      "Iteration 93, loss = 0.03706278\n",
      "Iteration 94, loss = 0.03633151\n",
      "Iteration 95, loss = 0.03556720\n",
      "Iteration 96, loss = 0.03494756\n",
      "Iteration 97, loss = 0.03428373\n",
      "Iteration 98, loss = 0.03364303\n",
      "Iteration 99, loss = 0.03301496\n",
      "Iteration 100, loss = 0.03234891\n",
      "Iteration 101, loss = 0.03190378\n",
      "Iteration 102, loss = 0.03123554\n",
      "Iteration 103, loss = 0.03055588\n",
      "Iteration 104, loss = 0.03001477\n",
      "Iteration 105, loss = 0.02947534\n",
      "Iteration 106, loss = 0.02888334\n",
      "Iteration 107, loss = 0.02851523\n",
      "Iteration 108, loss = 0.02779150\n",
      "Iteration 109, loss = 0.02734693\n",
      "Iteration 110, loss = 0.02687769\n",
      "Iteration 111, loss = 0.02643743\n",
      "Iteration 112, loss = 0.02590507\n",
      "Iteration 113, loss = 0.02549346\n",
      "Iteration 114, loss = 0.02509274\n",
      "Iteration 115, loss = 0.02462078\n",
      "Iteration 116, loss = 0.02415916\n",
      "Iteration 117, loss = 0.02377635\n",
      "Iteration 118, loss = 0.02355592\n",
      "Iteration 119, loss = 0.02300621\n",
      "Iteration 120, loss = 0.02256360\n",
      "Iteration 121, loss = 0.02221426\n",
      "Iteration 122, loss = 0.02187503\n",
      "Iteration 123, loss = 0.02141829\n",
      "Iteration 124, loss = 0.02118503\n",
      "Iteration 125, loss = 0.02075867\n",
      "Iteration 126, loss = 0.02045229\n",
      "Iteration 127, loss = 0.02018269\n",
      "Iteration 128, loss = 0.01983773\n",
      "Iteration 129, loss = 0.01965746\n",
      "Iteration 130, loss = 0.01918226\n",
      "Iteration 131, loss = 0.01892001\n",
      "Iteration 132, loss = 0.01864950\n",
      "Iteration 133, loss = 0.01833162\n",
      "Iteration 134, loss = 0.01811174\n",
      "Iteration 135, loss = 0.01784559\n",
      "Iteration 136, loss = 0.01756380\n",
      "Iteration 137, loss = 0.01725161\n",
      "Iteration 138, loss = 0.01704065\n",
      "Iteration 139, loss = 0.01680038\n",
      "Iteration 140, loss = 0.01655412\n",
      "Iteration 141, loss = 0.01629892\n",
      "Iteration 142, loss = 0.01609464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 143, loss = 0.01588289\n",
      "Iteration 144, loss = 0.01567469\n",
      "Iteration 145, loss = 0.01545773\n",
      "Iteration 146, loss = 0.01522834\n",
      "Iteration 147, loss = 0.01501114\n",
      "Iteration 148, loss = 0.01485172\n",
      "Iteration 149, loss = 0.01465771\n",
      "Iteration 150, loss = 0.01443352\n",
      "Iteration 151, loss = 0.01426980\n",
      "Iteration 152, loss = 0.01411993\n",
      "Iteration 153, loss = 0.01388509\n",
      "Iteration 154, loss = 0.01368814\n",
      "Iteration 155, loss = 0.01364615\n",
      "Iteration 156, loss = 0.01334927\n",
      "Iteration 157, loss = 0.01319148\n",
      "Iteration 158, loss = 0.01307161\n",
      "Iteration 159, loss = 0.01288590\n",
      "Iteration 160, loss = 0.01270020\n",
      "Iteration 161, loss = 0.01259093\n",
      "Iteration 162, loss = 0.01240024\n",
      "Iteration 163, loss = 0.01224802\n",
      "Iteration 164, loss = 0.01210109\n",
      "Iteration 165, loss = 0.01194423\n",
      "Iteration 166, loss = 0.01183738\n",
      "Iteration 167, loss = 0.01170061\n",
      "Iteration 168, loss = 0.01155156\n",
      "Iteration 169, loss = 0.01141606\n",
      "Iteration 170, loss = 0.01129855\n",
      "Iteration 171, loss = 0.01117440\n",
      "Iteration 172, loss = 0.01105253\n",
      "Iteration 173, loss = 0.01093915\n",
      "Iteration 174, loss = 0.01079968\n",
      "Iteration 175, loss = 0.01069405\n",
      "Iteration 176, loss = 0.01056880\n",
      "Iteration 177, loss = 0.01046673\n",
      "Iteration 178, loss = 0.01033268\n",
      "Iteration 179, loss = 0.01023705\n",
      "Iteration 180, loss = 0.01013311\n",
      "Iteration 181, loss = 0.01002047\n",
      "Iteration 182, loss = 0.00992313\n",
      "Iteration 183, loss = 0.00985756\n",
      "Iteration 184, loss = 0.00973796\n",
      "Iteration 185, loss = 0.00961789\n",
      "Iteration 186, loss = 0.00951397\n",
      "Iteration 187, loss = 0.00942441\n",
      "Iteration 188, loss = 0.00932855\n",
      "Iteration 189, loss = 0.00923781\n",
      "Iteration 190, loss = 0.00915512\n",
      "Iteration 191, loss = 0.00905919\n",
      "Iteration 192, loss = 0.00897805\n",
      "Iteration 193, loss = 0.00889023\n",
      "Iteration 194, loss = 0.00880309\n",
      "Iteration 195, loss = 0.00872163\n",
      "Iteration 196, loss = 0.00864815\n",
      "Iteration 197, loss = 0.00855900\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46199167\n",
      "Iteration 2, loss = 0.39716289\n",
      "Iteration 3, loss = 0.39158872\n",
      "Iteration 4, loss = 0.39117476\n",
      "Iteration 5, loss = 0.39025827\n",
      "Iteration 6, loss = 0.38988946\n",
      "Iteration 7, loss = 0.38872326\n",
      "Iteration 8, loss = 0.38897366\n",
      "Iteration 9, loss = 0.38762154\n",
      "Iteration 10, loss = 0.38652017\n",
      "Iteration 11, loss = 0.38560659\n",
      "Iteration 12, loss = 0.38460833\n",
      "Iteration 13, loss = 0.38281947\n",
      "Iteration 14, loss = 0.38121299\n",
      "Iteration 15, loss = 0.37948591\n",
      "Iteration 16, loss = 0.37742488\n",
      "Iteration 17, loss = 0.37544282\n",
      "Iteration 18, loss = 0.37280627\n",
      "Iteration 19, loss = 0.36945612\n",
      "Iteration 20, loss = 0.36584659\n",
      "Iteration 21, loss = 0.36192899\n",
      "Iteration 22, loss = 0.35700025\n",
      "Iteration 23, loss = 0.35156829\n",
      "Iteration 24, loss = 0.34528225\n",
      "Iteration 25, loss = 0.33837048\n",
      "Iteration 26, loss = 0.33103145\n",
      "Iteration 27, loss = 0.32234266\n",
      "Iteration 28, loss = 0.31301816\n",
      "Iteration 29, loss = 0.30244858\n",
      "Iteration 30, loss = 0.29126955\n",
      "Iteration 31, loss = 0.28010288\n",
      "Iteration 32, loss = 0.26709894\n",
      "Iteration 33, loss = 0.25493403\n",
      "Iteration 34, loss = 0.24185170\n",
      "Iteration 35, loss = 0.22929620\n",
      "Iteration 36, loss = 0.21687559\n",
      "Iteration 37, loss = 0.20474297\n",
      "Iteration 38, loss = 0.19375591\n",
      "Iteration 39, loss = 0.18315553\n",
      "Iteration 40, loss = 0.17338640\n",
      "Iteration 41, loss = 0.16412769\n",
      "Iteration 42, loss = 0.15578983\n",
      "Iteration 43, loss = 0.14791904\n",
      "Iteration 44, loss = 0.14064816\n",
      "Iteration 45, loss = 0.13408589\n",
      "Iteration 46, loss = 0.12840248\n",
      "Iteration 47, loss = 0.12277612\n",
      "Iteration 48, loss = 0.11730729\n",
      "Iteration 49, loss = 0.11271223\n",
      "Iteration 50, loss = 0.10808834\n",
      "Iteration 51, loss = 0.10406920\n",
      "Iteration 52, loss = 0.10016354\n",
      "Iteration 53, loss = 0.09677661\n",
      "Iteration 54, loss = 0.09343609\n",
      "Iteration 55, loss = 0.09022425\n",
      "Iteration 56, loss = 0.08801380\n",
      "Iteration 57, loss = 0.08445378\n",
      "Iteration 58, loss = 0.08182476\n",
      "Iteration 59, loss = 0.07938017\n",
      "Iteration 60, loss = 0.07712368\n",
      "Iteration 61, loss = 0.07479082\n",
      "Iteration 62, loss = 0.07278253\n",
      "Iteration 63, loss = 0.07062014\n",
      "Iteration 64, loss = 0.06901424\n",
      "Iteration 65, loss = 0.06681205\n",
      "Iteration 66, loss = 0.06508575\n",
      "Iteration 67, loss = 0.06350759\n",
      "Iteration 68, loss = 0.06181127\n",
      "Iteration 69, loss = 0.06033196\n",
      "Iteration 70, loss = 0.05908837\n",
      "Iteration 71, loss = 0.05761797\n",
      "Iteration 72, loss = 0.05625523\n",
      "Iteration 73, loss = 0.05464961\n",
      "Iteration 74, loss = 0.05372261\n",
      "Iteration 75, loss = 0.05226140\n",
      "Iteration 76, loss = 0.05096435\n",
      "Iteration 77, loss = 0.04989259\n",
      "Iteration 78, loss = 0.04880278\n",
      "Iteration 79, loss = 0.04754726\n",
      "Iteration 80, loss = 0.04649539\n",
      "Iteration 81, loss = 0.04558379\n",
      "Iteration 82, loss = 0.04454072\n",
      "Iteration 83, loss = 0.04354079\n",
      "Iteration 84, loss = 0.04260882\n",
      "Iteration 85, loss = 0.04168988\n",
      "Iteration 86, loss = 0.04113432\n",
      "Iteration 87, loss = 0.04009240\n",
      "Iteration 88, loss = 0.03932368\n",
      "Iteration 89, loss = 0.03838278\n",
      "Iteration 90, loss = 0.03758791\n",
      "Iteration 91, loss = 0.03683889\n",
      "Iteration 92, loss = 0.03621428\n",
      "Iteration 93, loss = 0.03533482\n",
      "Iteration 94, loss = 0.03473593\n",
      "Iteration 95, loss = 0.03405271\n",
      "Iteration 96, loss = 0.03335081\n",
      "Iteration 97, loss = 0.03265349\n",
      "Iteration 98, loss = 0.03209442\n",
      "Iteration 99, loss = 0.03145566\n",
      "Iteration 100, loss = 0.03105297\n",
      "Iteration 101, loss = 0.03028498\n",
      "Iteration 102, loss = 0.02968412\n",
      "Iteration 103, loss = 0.02922130\n",
      "Iteration 104, loss = 0.02866521\n",
      "Iteration 105, loss = 0.02812068\n",
      "Iteration 106, loss = 0.02757468\n",
      "Iteration 107, loss = 0.02705859\n",
      "Iteration 108, loss = 0.02658366\n",
      "Iteration 109, loss = 0.02609214\n",
      "Iteration 110, loss = 0.02568414\n",
      "Iteration 111, loss = 0.02519155\n",
      "Iteration 112, loss = 0.02474323\n",
      "Iteration 113, loss = 0.02431262\n",
      "Iteration 114, loss = 0.02393630\n",
      "Iteration 115, loss = 0.02349089\n",
      "Iteration 116, loss = 0.02312515\n",
      "Iteration 117, loss = 0.02278185\n",
      "Iteration 118, loss = 0.02235305\n",
      "Iteration 119, loss = 0.02198911\n",
      "Iteration 120, loss = 0.02166254\n",
      "Iteration 121, loss = 0.02126245\n",
      "Iteration 122, loss = 0.02109161\n",
      "Iteration 123, loss = 0.02060218\n",
      "Iteration 124, loss = 0.02038533\n",
      "Iteration 125, loss = 0.01991808\n",
      "Iteration 126, loss = 0.01965368\n",
      "Iteration 127, loss = 0.01927777\n",
      "Iteration 128, loss = 0.01903549\n",
      "Iteration 129, loss = 0.01877161\n",
      "Iteration 130, loss = 0.01872430\n",
      "Iteration 131, loss = 0.01820834\n",
      "Iteration 132, loss = 0.01793161\n",
      "Iteration 133, loss = 0.01758067\n",
      "Iteration 134, loss = 0.01736807\n",
      "Iteration 135, loss = 0.01719837\n",
      "Iteration 136, loss = 0.01695007\n",
      "Iteration 137, loss = 0.01660949\n",
      "Iteration 138, loss = 0.01639070\n",
      "Iteration 139, loss = 0.01614819\n",
      "Iteration 140, loss = 0.01588318\n",
      "Iteration 141, loss = 0.01568068\n",
      "Iteration 142, loss = 0.01548088\n",
      "Iteration 143, loss = 0.01523454\n",
      "Iteration 144, loss = 0.01500284\n",
      "Iteration 145, loss = 0.01480328\n",
      "Iteration 146, loss = 0.01460276\n",
      "Iteration 147, loss = 0.01441090\n",
      "Iteration 148, loss = 0.01422356\n",
      "Iteration 149, loss = 0.01402386\n",
      "Iteration 150, loss = 0.01390873\n",
      "Iteration 151, loss = 0.01371203\n",
      "Iteration 152, loss = 0.01348543\n",
      "Iteration 153, loss = 0.01338874\n",
      "Iteration 154, loss = 0.01322917\n",
      "Iteration 155, loss = 0.01305029\n",
      "Iteration 156, loss = 0.01284001\n",
      "Iteration 157, loss = 0.01265186\n",
      "Iteration 158, loss = 0.01249510\n",
      "Iteration 159, loss = 0.01237115\n",
      "Iteration 160, loss = 0.01219092\n",
      "Iteration 161, loss = 0.01202995\n",
      "Iteration 162, loss = 0.01190341\n",
      "Iteration 163, loss = 0.01178252\n",
      "Iteration 164, loss = 0.01163384\n",
      "Iteration 165, loss = 0.01146836\n",
      "Iteration 166, loss = 0.01133665\n",
      "Iteration 167, loss = 0.01120775\n",
      "Iteration 168, loss = 0.01108178\n",
      "Iteration 169, loss = 0.01097052\n",
      "Iteration 170, loss = 0.01086071\n",
      "Iteration 171, loss = 0.01073040\n",
      "Iteration 172, loss = 0.01060996\n",
      "Iteration 173, loss = 0.01047055\n",
      "Iteration 174, loss = 0.01038332\n",
      "Iteration 175, loss = 0.01028607\n",
      "Iteration 176, loss = 0.01015069\n",
      "Iteration 177, loss = 0.01003117\n",
      "Iteration 178, loss = 0.00992410\n",
      "Iteration 179, loss = 0.00983522\n",
      "Iteration 180, loss = 0.00973306\n",
      "Iteration 181, loss = 0.00962260\n",
      "Iteration 182, loss = 0.00952251\n",
      "Iteration 183, loss = 0.00946142\n",
      "Iteration 184, loss = 0.00932517\n",
      "Iteration 185, loss = 0.00924447\n",
      "Iteration 186, loss = 0.00913920\n",
      "Iteration 187, loss = 0.00904800\n",
      "Iteration 188, loss = 0.00897067\n",
      "Iteration 189, loss = 0.00888445\n",
      "Iteration 190, loss = 0.00878877\n",
      "Iteration 191, loss = 0.00871774\n",
      "Iteration 192, loss = 0.00863867\n",
      "Iteration 193, loss = 0.00853463\n",
      "Iteration 194, loss = 0.00845712\n",
      "Iteration 195, loss = 0.00837851\n",
      "Iteration 196, loss = 0.00833000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 197, loss = 0.00822380\n",
      "Iteration 198, loss = 0.00814208\n",
      "Iteration 199, loss = 0.00808010\n",
      "Iteration 200, loss = 0.00800250\n",
      "Iteration 201, loss = 0.00793579\n",
      "Iteration 202, loss = 0.00785600\n",
      "Iteration 203, loss = 0.00778068\n",
      "Iteration 204, loss = 0.00772861\n",
      "Iteration 205, loss = 0.00764302\n",
      "Iteration 206, loss = 0.00760874\n",
      "Iteration 207, loss = 0.00751576\n",
      "Iteration 208, loss = 0.00745792\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45820022\n",
      "Iteration 2, loss = 0.39821992\n",
      "Iteration 3, loss = 0.39608748\n",
      "Iteration 4, loss = 0.39560804\n",
      "Iteration 5, loss = 0.39472888\n",
      "Iteration 6, loss = 0.39480947\n",
      "Iteration 7, loss = 0.39361189\n",
      "Iteration 8, loss = 0.39282515\n",
      "Iteration 9, loss = 0.39181928\n",
      "Iteration 10, loss = 0.39101695\n",
      "Iteration 11, loss = 0.38971824\n",
      "Iteration 12, loss = 0.38949259\n",
      "Iteration 13, loss = 0.38719070\n",
      "Iteration 14, loss = 0.38594534\n",
      "Iteration 15, loss = 0.38385126\n",
      "Iteration 16, loss = 0.38186940\n",
      "Iteration 17, loss = 0.37930084\n",
      "Iteration 18, loss = 0.37744738\n",
      "Iteration 19, loss = 0.37331514\n",
      "Iteration 20, loss = 0.36943748\n",
      "Iteration 21, loss = 0.36552898\n",
      "Iteration 22, loss = 0.35990323\n",
      "Iteration 23, loss = 0.35479623\n",
      "Iteration 24, loss = 0.34818356\n",
      "Iteration 25, loss = 0.34089320\n",
      "Iteration 26, loss = 0.33272504\n",
      "Iteration 27, loss = 0.32397888\n",
      "Iteration 28, loss = 0.31391550\n",
      "Iteration 29, loss = 0.30294218\n",
      "Iteration 30, loss = 0.29190852\n",
      "Iteration 31, loss = 0.28059859\n",
      "Iteration 32, loss = 0.26827743\n",
      "Iteration 33, loss = 0.25494072\n",
      "Iteration 34, loss = 0.24155678\n",
      "Iteration 35, loss = 0.22906935\n",
      "Iteration 36, loss = 0.21692476\n",
      "Iteration 37, loss = 0.20479452\n",
      "Iteration 38, loss = 0.19414104\n",
      "Iteration 39, loss = 0.18324037\n",
      "Iteration 40, loss = 0.17379939\n",
      "Iteration 41, loss = 0.16441068\n",
      "Iteration 42, loss = 0.15611349\n",
      "Iteration 43, loss = 0.14850274\n",
      "Iteration 44, loss = 0.14143095\n",
      "Iteration 45, loss = 0.13490881\n",
      "Iteration 46, loss = 0.12938893\n",
      "Iteration 47, loss = 0.12339924\n",
      "Iteration 48, loss = 0.11856508\n",
      "Iteration 49, loss = 0.11382125\n",
      "Iteration 50, loss = 0.10965145\n",
      "Iteration 51, loss = 0.10555667\n",
      "Iteration 52, loss = 0.10146124\n",
      "Iteration 53, loss = 0.09826134\n",
      "Iteration 54, loss = 0.09487868\n",
      "Iteration 55, loss = 0.09185599\n",
      "Iteration 56, loss = 0.08919203\n",
      "Iteration 57, loss = 0.08643642\n",
      "Iteration 58, loss = 0.08372572\n",
      "Iteration 59, loss = 0.08113896\n",
      "Iteration 60, loss = 0.07924795\n",
      "Iteration 61, loss = 0.07681124\n",
      "Iteration 62, loss = 0.07467501\n",
      "Iteration 63, loss = 0.07261904\n",
      "Iteration 64, loss = 0.07076310\n",
      "Iteration 65, loss = 0.06878882\n",
      "Iteration 66, loss = 0.06700402\n",
      "Iteration 67, loss = 0.06535695\n",
      "Iteration 68, loss = 0.06372219\n",
      "Iteration 69, loss = 0.06218459\n",
      "Iteration 70, loss = 0.06052350\n",
      "Iteration 71, loss = 0.05908411\n",
      "Iteration 72, loss = 0.05780515\n",
      "Iteration 73, loss = 0.05638057\n",
      "Iteration 74, loss = 0.05520396\n",
      "Iteration 75, loss = 0.05407407\n",
      "Iteration 76, loss = 0.05319447\n",
      "Iteration 77, loss = 0.05140113\n",
      "Iteration 78, loss = 0.05023335\n",
      "Iteration 79, loss = 0.04918542\n",
      "Iteration 80, loss = 0.04817620\n",
      "Iteration 81, loss = 0.04708248\n",
      "Iteration 82, loss = 0.04603446\n",
      "Iteration 83, loss = 0.04508517\n",
      "Iteration 84, loss = 0.04417741\n",
      "Iteration 85, loss = 0.04316262\n",
      "Iteration 86, loss = 0.04230377\n",
      "Iteration 87, loss = 0.04155080\n",
      "Iteration 88, loss = 0.04061688\n",
      "Iteration 89, loss = 0.03983326\n",
      "Iteration 90, loss = 0.03892728\n",
      "Iteration 91, loss = 0.03830317\n",
      "Iteration 92, loss = 0.03744738\n",
      "Iteration 93, loss = 0.03660237\n",
      "Iteration 94, loss = 0.03601039\n",
      "Iteration 95, loss = 0.03543984\n",
      "Iteration 96, loss = 0.03475532\n",
      "Iteration 97, loss = 0.03385802\n",
      "Iteration 98, loss = 0.03320850\n",
      "Iteration 99, loss = 0.03255606\n",
      "Iteration 100, loss = 0.03212231\n",
      "Iteration 101, loss = 0.03134019\n",
      "Iteration 102, loss = 0.03071157\n",
      "Iteration 103, loss = 0.03013160\n",
      "Iteration 104, loss = 0.02951853\n",
      "Iteration 105, loss = 0.02910585\n",
      "Iteration 106, loss = 0.02848120\n",
      "Iteration 107, loss = 0.02818746\n",
      "Iteration 108, loss = 0.02751951\n",
      "Iteration 109, loss = 0.02710540\n",
      "Iteration 110, loss = 0.02653259\n",
      "Iteration 111, loss = 0.02600698\n",
      "Iteration 112, loss = 0.02549805\n",
      "Iteration 113, loss = 0.02503741\n",
      "Iteration 114, loss = 0.02469649\n",
      "Iteration 115, loss = 0.02419551\n",
      "Iteration 116, loss = 0.02387852\n",
      "Iteration 117, loss = 0.02340720\n",
      "Iteration 118, loss = 0.02293468\n",
      "Iteration 119, loss = 0.02266679\n",
      "Iteration 120, loss = 0.02216300\n",
      "Iteration 121, loss = 0.02186253\n",
      "Iteration 122, loss = 0.02141205\n",
      "Iteration 123, loss = 0.02107286\n",
      "Iteration 124, loss = 0.02073763\n",
      "Iteration 125, loss = 0.02045242\n",
      "Iteration 126, loss = 0.02004871\n",
      "Iteration 127, loss = 0.01980379\n",
      "Iteration 128, loss = 0.01942728\n",
      "Iteration 129, loss = 0.01915148\n",
      "Iteration 130, loss = 0.01890314\n",
      "Iteration 131, loss = 0.01858976\n",
      "Iteration 132, loss = 0.01834525\n",
      "Iteration 133, loss = 0.01794040\n",
      "Iteration 134, loss = 0.01769241\n",
      "Iteration 135, loss = 0.01747577\n",
      "Iteration 136, loss = 0.01720696\n",
      "Iteration 137, loss = 0.01691709\n",
      "Iteration 138, loss = 0.01670316\n",
      "Iteration 139, loss = 0.01650209\n",
      "Iteration 140, loss = 0.01620416\n",
      "Iteration 141, loss = 0.01593533\n",
      "Iteration 142, loss = 0.01570172\n",
      "Iteration 143, loss = 0.01549272\n",
      "Iteration 144, loss = 0.01525868\n",
      "Iteration 145, loss = 0.01511804\n",
      "Iteration 146, loss = 0.01483746\n",
      "Iteration 147, loss = 0.01464576\n",
      "Iteration 148, loss = 0.01444804\n",
      "Iteration 149, loss = 0.01432652\n",
      "Iteration 150, loss = 0.01406553\n",
      "Iteration 151, loss = 0.01388209\n",
      "Iteration 152, loss = 0.01367355\n",
      "Iteration 153, loss = 0.01351316\n",
      "Iteration 154, loss = 0.01331707\n",
      "Iteration 155, loss = 0.01314575\n",
      "Iteration 156, loss = 0.01297407\n",
      "Iteration 157, loss = 0.01280802\n",
      "Iteration 158, loss = 0.01264866\n",
      "Iteration 159, loss = 0.01248580\n",
      "Iteration 160, loss = 0.01233684\n",
      "Iteration 161, loss = 0.01221692\n",
      "Iteration 162, loss = 0.01206550\n",
      "Iteration 163, loss = 0.01193841\n",
      "Iteration 164, loss = 0.01175069\n",
      "Iteration 165, loss = 0.01161192\n",
      "Iteration 166, loss = 0.01150044\n",
      "Iteration 167, loss = 0.01136859\n",
      "Iteration 168, loss = 0.01121120\n",
      "Iteration 169, loss = 0.01108907\n",
      "Iteration 170, loss = 0.01098328\n",
      "Iteration 171, loss = 0.01083327\n",
      "Iteration 172, loss = 0.01073484\n",
      "Iteration 173, loss = 0.01062691\n",
      "Iteration 174, loss = 0.01048181\n",
      "Iteration 175, loss = 0.01035193\n",
      "Iteration 176, loss = 0.01025160\n",
      "Iteration 177, loss = 0.01014138\n",
      "Iteration 178, loss = 0.01003040\n",
      "Iteration 179, loss = 0.00991654\n",
      "Iteration 180, loss = 0.00981681\n",
      "Iteration 181, loss = 0.00971003\n",
      "Iteration 182, loss = 0.00965028\n",
      "Iteration 183, loss = 0.00954252\n",
      "Iteration 184, loss = 0.00942383\n",
      "Iteration 185, loss = 0.00931208\n",
      "Iteration 186, loss = 0.00924727\n",
      "Iteration 187, loss = 0.00916975\n",
      "Iteration 188, loss = 0.00903636\n",
      "Iteration 189, loss = 0.00896561\n",
      "Iteration 190, loss = 0.00887830\n",
      "Iteration 191, loss = 0.00878348\n",
      "Iteration 192, loss = 0.00869027\n",
      "Iteration 193, loss = 0.00861679\n",
      "Iteration 194, loss = 0.00852160\n",
      "Iteration 195, loss = 0.00846285\n",
      "Iteration 196, loss = 0.00835556\n",
      "Iteration 197, loss = 0.00829820\n",
      "Iteration 198, loss = 0.00823055\n",
      "Iteration 199, loss = 0.00814798\n",
      "Iteration 200, loss = 0.00809720\n",
      "Iteration 201, loss = 0.00799765\n",
      "Iteration 202, loss = 0.00793600\n",
      "Iteration 203, loss = 0.00784928\n",
      "Iteration 204, loss = 0.00778050\n",
      "Iteration 205, loss = 0.00769813\n",
      "Iteration 206, loss = 0.00763672\n",
      "Iteration 207, loss = 0.00758147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45452930\n",
      "Iteration 2, loss = 0.39415865\n",
      "Iteration 3, loss = 0.39145995\n",
      "Iteration 4, loss = 0.39051190\n",
      "Iteration 5, loss = 0.39035913\n",
      "Iteration 6, loss = 0.38953245\n",
      "Iteration 7, loss = 0.38828053\n",
      "Iteration 8, loss = 0.38877886\n",
      "Iteration 9, loss = 0.38688525\n",
      "Iteration 10, loss = 0.38628839\n",
      "Iteration 11, loss = 0.38520461\n",
      "Iteration 12, loss = 0.38377406\n",
      "Iteration 13, loss = 0.38251226\n",
      "Iteration 14, loss = 0.38091433\n",
      "Iteration 15, loss = 0.37909610\n",
      "Iteration 16, loss = 0.37740821\n",
      "Iteration 17, loss = 0.37520673\n",
      "Iteration 18, loss = 0.37186790\n",
      "Iteration 19, loss = 0.36977382\n",
      "Iteration 20, loss = 0.36610284\n",
      "Iteration 21, loss = 0.36145687\n",
      "Iteration 22, loss = 0.35658866\n",
      "Iteration 23, loss = 0.35139492\n",
      "Iteration 24, loss = 0.34504123\n",
      "Iteration 25, loss = 0.33847701\n",
      "Iteration 26, loss = 0.33112833\n",
      "Iteration 27, loss = 0.32240191\n",
      "Iteration 28, loss = 0.31315592\n",
      "Iteration 29, loss = 0.30274688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.29243951\n",
      "Iteration 31, loss = 0.28052464\n",
      "Iteration 32, loss = 0.26896137\n",
      "Iteration 33, loss = 0.25652265\n",
      "Iteration 34, loss = 0.24374443\n",
      "Iteration 35, loss = 0.23148652\n",
      "Iteration 36, loss = 0.21959983\n",
      "Iteration 37, loss = 0.20819734\n",
      "Iteration 38, loss = 0.19710396\n",
      "Iteration 39, loss = 0.18682590\n",
      "Iteration 40, loss = 0.17708656\n",
      "Iteration 41, loss = 0.16777117\n",
      "Iteration 42, loss = 0.15978697\n",
      "Iteration 43, loss = 0.15183652\n",
      "Iteration 44, loss = 0.14498595\n",
      "Iteration 45, loss = 0.13851929\n",
      "Iteration 46, loss = 0.13203807\n",
      "Iteration 47, loss = 0.12675610\n",
      "Iteration 48, loss = 0.12163335\n",
      "Iteration 49, loss = 0.11639010\n",
      "Iteration 50, loss = 0.11212191\n",
      "Iteration 51, loss = 0.10815359\n",
      "Iteration 52, loss = 0.10385368\n",
      "Iteration 53, loss = 0.10023143\n",
      "Iteration 54, loss = 0.09674843\n",
      "Iteration 55, loss = 0.09368420\n",
      "Iteration 56, loss = 0.09060509\n",
      "Iteration 57, loss = 0.08770161\n",
      "Iteration 58, loss = 0.08493836\n",
      "Iteration 59, loss = 0.08226656\n",
      "Iteration 60, loss = 0.07992917\n",
      "Iteration 61, loss = 0.07787141\n",
      "Iteration 62, loss = 0.07591607\n",
      "Iteration 63, loss = 0.07324951\n",
      "Iteration 64, loss = 0.07110484\n",
      "Iteration 65, loss = 0.06918893\n",
      "Iteration 66, loss = 0.06722294\n",
      "Iteration 67, loss = 0.06591788\n",
      "Iteration 68, loss = 0.06386943\n",
      "Iteration 69, loss = 0.06225511\n",
      "Iteration 70, loss = 0.06070665\n",
      "Iteration 71, loss = 0.05914402\n",
      "Iteration 72, loss = 0.05782033\n",
      "Iteration 73, loss = 0.05639742\n",
      "Iteration 74, loss = 0.05500378\n",
      "Iteration 75, loss = 0.05347767\n",
      "Iteration 76, loss = 0.05220805\n",
      "Iteration 77, loss = 0.05105646\n",
      "Iteration 78, loss = 0.04994658\n",
      "Iteration 79, loss = 0.04868307\n",
      "Iteration 80, loss = 0.04778975\n",
      "Iteration 81, loss = 0.04657209\n",
      "Iteration 82, loss = 0.04563971\n",
      "Iteration 83, loss = 0.04471478\n",
      "Iteration 84, loss = 0.04347349\n",
      "Iteration 85, loss = 0.04261123\n",
      "Iteration 86, loss = 0.04186628\n",
      "Iteration 87, loss = 0.04081938\n",
      "Iteration 88, loss = 0.03997087\n",
      "Iteration 89, loss = 0.03926846\n",
      "Iteration 90, loss = 0.03856683\n",
      "Iteration 91, loss = 0.03755422\n",
      "Iteration 92, loss = 0.03709892\n",
      "Iteration 93, loss = 0.03600716\n",
      "Iteration 94, loss = 0.03528866\n",
      "Iteration 95, loss = 0.03457954\n",
      "Iteration 96, loss = 0.03385396\n",
      "Iteration 97, loss = 0.03345889\n",
      "Iteration 98, loss = 0.03278435\n",
      "Iteration 99, loss = 0.03185424\n",
      "Iteration 100, loss = 0.03141291\n",
      "Iteration 101, loss = 0.03074488\n",
      "Iteration 102, loss = 0.03027055\n",
      "Iteration 103, loss = 0.02950518\n",
      "Iteration 104, loss = 0.02907190\n",
      "Iteration 105, loss = 0.02847825\n",
      "Iteration 106, loss = 0.02794465\n",
      "Iteration 107, loss = 0.02753465\n",
      "Iteration 108, loss = 0.02699036\n",
      "Iteration 109, loss = 0.02647401\n",
      "Iteration 110, loss = 0.02595714\n",
      "Iteration 111, loss = 0.02551041\n",
      "Iteration 112, loss = 0.02501509\n",
      "Iteration 113, loss = 0.02454564\n",
      "Iteration 114, loss = 0.02435498\n",
      "Iteration 115, loss = 0.02374509\n",
      "Iteration 116, loss = 0.02329751\n",
      "Iteration 117, loss = 0.02290983\n",
      "Iteration 118, loss = 0.02256016\n",
      "Iteration 119, loss = 0.02221357\n",
      "Iteration 120, loss = 0.02187450\n",
      "Iteration 121, loss = 0.02138955\n",
      "Iteration 122, loss = 0.02119810\n",
      "Iteration 123, loss = 0.02073281\n",
      "Iteration 124, loss = 0.02043928\n",
      "Iteration 125, loss = 0.02008975\n",
      "Iteration 126, loss = 0.01976714\n",
      "Iteration 127, loss = 0.01942503\n",
      "Iteration 128, loss = 0.01909372\n",
      "Iteration 129, loss = 0.01878102\n",
      "Iteration 130, loss = 0.01851455\n",
      "Iteration 131, loss = 0.01821508\n",
      "Iteration 132, loss = 0.01796245\n",
      "Iteration 133, loss = 0.01764636\n",
      "Iteration 134, loss = 0.01741821\n",
      "Iteration 135, loss = 0.01714960\n",
      "Iteration 136, loss = 0.01690621\n",
      "Iteration 137, loss = 0.01663107\n",
      "Iteration 138, loss = 0.01639158\n",
      "Iteration 139, loss = 0.01616790\n",
      "Iteration 140, loss = 0.01591453\n",
      "Iteration 141, loss = 0.01569127\n",
      "Iteration 142, loss = 0.01546082\n",
      "Iteration 143, loss = 0.01524032\n",
      "Iteration 144, loss = 0.01505285\n",
      "Iteration 145, loss = 0.01480522\n",
      "Iteration 146, loss = 0.01461954\n",
      "Iteration 147, loss = 0.01441385\n",
      "Iteration 148, loss = 0.01422731\n",
      "Iteration 149, loss = 0.01404187\n",
      "Iteration 150, loss = 0.01384757\n",
      "Iteration 151, loss = 0.01368083\n",
      "Iteration 152, loss = 0.01349880\n",
      "Iteration 153, loss = 0.01331391\n",
      "Iteration 154, loss = 0.01320213\n",
      "Iteration 155, loss = 0.01299241\n",
      "Iteration 156, loss = 0.01281236\n",
      "Iteration 157, loss = 0.01272732\n",
      "Iteration 158, loss = 0.01248986\n",
      "Iteration 159, loss = 0.01235801\n",
      "Iteration 160, loss = 0.01218413\n",
      "Iteration 161, loss = 0.01203125\n",
      "Iteration 162, loss = 0.01190181\n",
      "Iteration 163, loss = 0.01175767\n",
      "Iteration 164, loss = 0.01160834\n",
      "Iteration 165, loss = 0.01148144\n",
      "Iteration 166, loss = 0.01134569\n",
      "Iteration 167, loss = 0.01121043\n",
      "Iteration 168, loss = 0.01108107\n",
      "Iteration 169, loss = 0.01095076\n",
      "Iteration 170, loss = 0.01082038\n",
      "Iteration 171, loss = 0.01071000\n",
      "Iteration 172, loss = 0.01059671\n",
      "Iteration 173, loss = 0.01048238\n",
      "Iteration 174, loss = 0.01035240\n",
      "Iteration 175, loss = 0.01026945\n",
      "Iteration 176, loss = 0.01014177\n",
      "Iteration 177, loss = 0.01004416\n",
      "Iteration 178, loss = 0.00992606\n",
      "Iteration 179, loss = 0.00981952\n",
      "Iteration 180, loss = 0.00971145\n",
      "Iteration 181, loss = 0.00960590\n",
      "Iteration 182, loss = 0.00950829\n",
      "Iteration 183, loss = 0.00941741\n",
      "Iteration 184, loss = 0.00930533\n",
      "Iteration 185, loss = 0.00920837\n",
      "Iteration 186, loss = 0.00911834\n",
      "Iteration 187, loss = 0.00903233\n",
      "Iteration 188, loss = 0.00893032\n",
      "Iteration 189, loss = 0.00885468\n",
      "Iteration 190, loss = 0.00876526\n",
      "Iteration 191, loss = 0.00868610\n",
      "Iteration 192, loss = 0.00861169\n",
      "Iteration 193, loss = 0.00854502\n",
      "Iteration 194, loss = 0.00842554\n",
      "Iteration 195, loss = 0.00836532\n",
      "Iteration 196, loss = 0.00827094\n",
      "Iteration 197, loss = 0.00820557\n",
      "Iteration 198, loss = 0.00811491\n",
      "Iteration 199, loss = 0.00805402\n",
      "Iteration 200, loss = 0.00798971\n",
      "Iteration 201, loss = 0.00792259\n",
      "Iteration 202, loss = 0.00785654\n",
      "Iteration 203, loss = 0.00775709\n",
      "Iteration 204, loss = 0.00769832\n",
      "Iteration 205, loss = 0.00762818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44477427\n",
      "Iteration 2, loss = 0.38855976\n",
      "Iteration 3, loss = 0.38726945\n",
      "Iteration 4, loss = 0.38672843\n",
      "Iteration 5, loss = 0.38568034\n",
      "Iteration 6, loss = 0.38476569\n",
      "Iteration 7, loss = 0.38445690\n",
      "Iteration 8, loss = 0.38344759\n",
      "Iteration 9, loss = 0.38279975\n",
      "Iteration 10, loss = 0.38251782\n",
      "Iteration 11, loss = 0.38128025\n",
      "Iteration 12, loss = 0.37966513\n",
      "Iteration 13, loss = 0.37924410\n",
      "Iteration 14, loss = 0.37673878\n",
      "Iteration 15, loss = 0.37538221\n",
      "Iteration 16, loss = 0.37299607\n",
      "Iteration 17, loss = 0.37065081\n",
      "Iteration 18, loss = 0.36797105\n",
      "Iteration 19, loss = 0.36520469\n",
      "Iteration 20, loss = 0.36170553\n",
      "Iteration 21, loss = 0.35737796\n",
      "Iteration 22, loss = 0.35311164\n",
      "Iteration 23, loss = 0.34737492\n",
      "Iteration 24, loss = 0.34163468\n",
      "Iteration 25, loss = 0.33474423\n",
      "Iteration 26, loss = 0.32807386\n",
      "Iteration 27, loss = 0.32014736\n",
      "Iteration 28, loss = 0.31005003\n",
      "Iteration 29, loss = 0.29984154\n",
      "Iteration 30, loss = 0.28949615\n",
      "Iteration 31, loss = 0.27796293\n",
      "Iteration 32, loss = 0.26588144\n",
      "Iteration 33, loss = 0.25342353\n",
      "Iteration 34, loss = 0.24159016\n",
      "Iteration 35, loss = 0.22939497\n",
      "Iteration 36, loss = 0.21788640\n",
      "Iteration 37, loss = 0.20581467\n",
      "Iteration 38, loss = 0.19538966\n",
      "Iteration 39, loss = 0.18480148\n",
      "Iteration 40, loss = 0.17515398\n",
      "Iteration 41, loss = 0.16663886\n",
      "Iteration 42, loss = 0.15819071\n",
      "Iteration 43, loss = 0.15035987\n",
      "Iteration 44, loss = 0.14355460\n",
      "Iteration 45, loss = 0.13765264\n",
      "Iteration 46, loss = 0.13094064\n",
      "Iteration 47, loss = 0.12561396\n",
      "Iteration 48, loss = 0.12037627\n",
      "Iteration 49, loss = 0.11583032\n",
      "Iteration 50, loss = 0.11116582\n",
      "Iteration 51, loss = 0.10689194\n",
      "Iteration 52, loss = 0.10308881\n",
      "Iteration 53, loss = 0.09940068\n",
      "Iteration 54, loss = 0.09575985\n",
      "Iteration 55, loss = 0.09321441\n",
      "Iteration 56, loss = 0.08982174\n",
      "Iteration 57, loss = 0.08712158\n",
      "Iteration 58, loss = 0.08430654\n",
      "Iteration 59, loss = 0.08166675\n",
      "Iteration 60, loss = 0.07912408\n",
      "Iteration 61, loss = 0.07681757\n",
      "Iteration 62, loss = 0.07461874\n",
      "Iteration 63, loss = 0.07257004\n",
      "Iteration 64, loss = 0.07066909\n",
      "Iteration 65, loss = 0.06859844\n",
      "Iteration 66, loss = 0.06685303\n",
      "Iteration 67, loss = 0.06514767\n",
      "Iteration 68, loss = 0.06341072\n",
      "Iteration 69, loss = 0.06184489\n",
      "Iteration 70, loss = 0.06007449\n",
      "Iteration 71, loss = 0.05875211\n",
      "Iteration 72, loss = 0.05725456\n",
      "Iteration 73, loss = 0.05596303\n",
      "Iteration 74, loss = 0.05442204\n",
      "Iteration 75, loss = 0.05307643\n",
      "Iteration 76, loss = 0.05185549\n",
      "Iteration 77, loss = 0.05077294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78, loss = 0.04952733\n",
      "Iteration 79, loss = 0.04864586\n",
      "Iteration 80, loss = 0.04718657\n",
      "Iteration 81, loss = 0.04618549\n",
      "Iteration 82, loss = 0.04514586\n",
      "Iteration 83, loss = 0.04408727\n",
      "Iteration 84, loss = 0.04330914\n",
      "Iteration 85, loss = 0.04251920\n",
      "Iteration 86, loss = 0.04142051\n",
      "Iteration 87, loss = 0.04041690\n",
      "Iteration 88, loss = 0.03961802\n",
      "Iteration 89, loss = 0.03869162\n",
      "Iteration 90, loss = 0.03779996\n",
      "Iteration 91, loss = 0.03715604\n",
      "Iteration 92, loss = 0.03641326\n",
      "Iteration 93, loss = 0.03580071\n",
      "Iteration 94, loss = 0.03488622\n",
      "Iteration 95, loss = 0.03423545\n",
      "Iteration 96, loss = 0.03353000\n",
      "Iteration 97, loss = 0.03282896\n",
      "Iteration 98, loss = 0.03210806\n",
      "Iteration 99, loss = 0.03149967\n",
      "Iteration 100, loss = 0.03103996\n",
      "Iteration 101, loss = 0.03028958\n",
      "Iteration 102, loss = 0.02961042\n",
      "Iteration 103, loss = 0.02905679\n",
      "Iteration 104, loss = 0.02852544\n",
      "Iteration 105, loss = 0.02801203\n",
      "Iteration 106, loss = 0.02752872\n",
      "Iteration 107, loss = 0.02696443\n",
      "Iteration 108, loss = 0.02645337\n",
      "Iteration 109, loss = 0.02599692\n",
      "Iteration 110, loss = 0.02546514\n",
      "Iteration 111, loss = 0.02501589\n",
      "Iteration 112, loss = 0.02452278\n",
      "Iteration 113, loss = 0.02406371\n",
      "Iteration 114, loss = 0.02363766\n",
      "Iteration 115, loss = 0.02323995\n",
      "Iteration 116, loss = 0.02282373\n",
      "Iteration 117, loss = 0.02250200\n",
      "Iteration 118, loss = 0.02200426\n",
      "Iteration 119, loss = 0.02163242\n",
      "Iteration 120, loss = 0.02128585\n",
      "Iteration 121, loss = 0.02089739\n",
      "Iteration 122, loss = 0.02052166\n",
      "Iteration 123, loss = 0.02035206\n",
      "Iteration 124, loss = 0.01996712\n",
      "Iteration 125, loss = 0.01955302\n",
      "Iteration 126, loss = 0.01933027\n",
      "Iteration 127, loss = 0.01895017\n",
      "Iteration 128, loss = 0.01862147\n",
      "Iteration 129, loss = 0.01835055\n",
      "Iteration 130, loss = 0.01803549\n",
      "Iteration 131, loss = 0.01779013\n",
      "Iteration 132, loss = 0.01761501\n",
      "Iteration 133, loss = 0.01729666\n",
      "Iteration 134, loss = 0.01710844\n",
      "Iteration 135, loss = 0.01680074\n",
      "Iteration 136, loss = 0.01642220\n",
      "Iteration 137, loss = 0.01616677\n",
      "Iteration 138, loss = 0.01614702\n",
      "Iteration 139, loss = 0.01578664\n",
      "Iteration 140, loss = 0.01551930\n",
      "Iteration 141, loss = 0.01525147\n",
      "Iteration 142, loss = 0.01501193\n",
      "Iteration 143, loss = 0.01487061\n",
      "Iteration 144, loss = 0.01459719\n",
      "Iteration 145, loss = 0.01438989\n",
      "Iteration 146, loss = 0.01419403\n",
      "Iteration 147, loss = 0.01400554\n",
      "Iteration 148, loss = 0.01385583\n",
      "Iteration 149, loss = 0.01364457\n",
      "Iteration 150, loss = 0.01347224\n",
      "Iteration 151, loss = 0.01334393\n",
      "Iteration 152, loss = 0.01311089\n",
      "Iteration 153, loss = 0.01291763\n",
      "Iteration 154, loss = 0.01275847\n",
      "Iteration 155, loss = 0.01257770\n",
      "Iteration 156, loss = 0.01241398\n",
      "Iteration 157, loss = 0.01229464\n",
      "Iteration 158, loss = 0.01212177\n",
      "Iteration 159, loss = 0.01195751\n",
      "Iteration 160, loss = 0.01181592\n",
      "Iteration 161, loss = 0.01168020\n",
      "Iteration 162, loss = 0.01152630\n",
      "Iteration 163, loss = 0.01139528\n",
      "Iteration 164, loss = 0.01124467\n",
      "Iteration 165, loss = 0.01112168\n",
      "Iteration 166, loss = 0.01100631\n",
      "Iteration 167, loss = 0.01085703\n",
      "Iteration 168, loss = 0.01072680\n",
      "Iteration 169, loss = 0.01061004\n",
      "Iteration 170, loss = 0.01049283\n",
      "Iteration 171, loss = 0.01036856\n",
      "Iteration 172, loss = 0.01025151\n",
      "Iteration 173, loss = 0.01014381\n",
      "Iteration 174, loss = 0.01002379\n",
      "Iteration 175, loss = 0.00994088\n",
      "Iteration 176, loss = 0.00979967\n",
      "Iteration 177, loss = 0.00969940\n",
      "Iteration 178, loss = 0.00960996\n",
      "Iteration 179, loss = 0.00953299\n",
      "Iteration 180, loss = 0.00946500\n",
      "Iteration 181, loss = 0.00928774\n",
      "Iteration 182, loss = 0.00921621\n",
      "Iteration 183, loss = 0.00912929\n",
      "Iteration 184, loss = 0.00899901\n",
      "Iteration 185, loss = 0.00892181\n",
      "Iteration 186, loss = 0.00885776\n",
      "Iteration 187, loss = 0.00873724\n",
      "Iteration 188, loss = 0.00865979\n",
      "Iteration 189, loss = 0.00856481\n",
      "Iteration 190, loss = 0.00847510\n",
      "Iteration 191, loss = 0.00839312\n",
      "Iteration 192, loss = 0.00831648\n",
      "Iteration 193, loss = 0.00823414\n",
      "Iteration 194, loss = 0.00815037\n",
      "Iteration 195, loss = 0.00807983\n",
      "Iteration 196, loss = 0.00801289\n",
      "Iteration 197, loss = 0.00793346\n",
      "Iteration 198, loss = 0.00786423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45503298\n",
      "Iteration 2, loss = 0.39151884\n",
      "Iteration 3, loss = 0.38829001\n",
      "Iteration 4, loss = 0.38812686\n",
      "Iteration 5, loss = 0.38717828\n",
      "Iteration 6, loss = 0.38767684\n",
      "Iteration 7, loss = 0.38696163\n",
      "Iteration 8, loss = 0.38543973\n",
      "Iteration 9, loss = 0.38436355\n",
      "Iteration 10, loss = 0.38412794\n",
      "Iteration 11, loss = 0.38282457\n",
      "Iteration 12, loss = 0.38162240\n",
      "Iteration 13, loss = 0.38057699\n",
      "Iteration 14, loss = 0.37893544\n",
      "Iteration 15, loss = 0.37736085\n",
      "Iteration 16, loss = 0.37572658\n",
      "Iteration 17, loss = 0.37330508\n",
      "Iteration 18, loss = 0.37136199\n",
      "Iteration 19, loss = 0.36806849\n",
      "Iteration 20, loss = 0.36514639\n",
      "Iteration 21, loss = 0.36077380\n",
      "Iteration 22, loss = 0.35671587\n",
      "Iteration 23, loss = 0.35234966\n",
      "Iteration 24, loss = 0.34615514\n",
      "Iteration 25, loss = 0.33992059\n",
      "Iteration 26, loss = 0.33278933\n",
      "Iteration 27, loss = 0.32530940\n",
      "Iteration 28, loss = 0.31618609\n",
      "Iteration 29, loss = 0.30735794\n",
      "Iteration 30, loss = 0.29609589\n",
      "Iteration 31, loss = 0.28556598\n",
      "Iteration 32, loss = 0.27355117\n",
      "Iteration 33, loss = 0.26127649\n",
      "Iteration 34, loss = 0.24938710\n",
      "Iteration 35, loss = 0.23709777\n",
      "Iteration 36, loss = 0.22502147\n",
      "Iteration 37, loss = 0.21353083\n",
      "Iteration 38, loss = 0.20160756\n",
      "Iteration 39, loss = 0.19169392\n",
      "Iteration 40, loss = 0.18114249\n",
      "Iteration 41, loss = 0.17161904\n",
      "Iteration 42, loss = 0.16300040\n",
      "Iteration 43, loss = 0.15532652\n",
      "Iteration 44, loss = 0.14687253\n",
      "Iteration 45, loss = 0.14075714\n",
      "Iteration 46, loss = 0.13422243\n",
      "Iteration 47, loss = 0.12815187\n",
      "Iteration 48, loss = 0.12233977\n",
      "Iteration 49, loss = 0.11741168\n",
      "Iteration 50, loss = 0.11292548\n",
      "Iteration 51, loss = 0.10872616\n",
      "Iteration 52, loss = 0.10443874\n",
      "Iteration 53, loss = 0.10050516\n",
      "Iteration 54, loss = 0.09697127\n",
      "Iteration 55, loss = 0.09374886\n",
      "Iteration 56, loss = 0.09065554\n",
      "Iteration 57, loss = 0.08807941\n",
      "Iteration 58, loss = 0.08480571\n",
      "Iteration 59, loss = 0.08221271\n",
      "Iteration 60, loss = 0.07973988\n",
      "Iteration 61, loss = 0.07755533\n",
      "Iteration 62, loss = 0.07515793\n",
      "Iteration 63, loss = 0.07300586\n",
      "Iteration 64, loss = 0.07085390\n",
      "Iteration 65, loss = 0.06912843\n",
      "Iteration 66, loss = 0.06709184\n",
      "Iteration 67, loss = 0.06544717\n",
      "Iteration 68, loss = 0.06349435\n",
      "Iteration 69, loss = 0.06203758\n",
      "Iteration 70, loss = 0.06041144\n",
      "Iteration 71, loss = 0.05904201\n",
      "Iteration 72, loss = 0.05741903\n",
      "Iteration 73, loss = 0.05618583\n",
      "Iteration 74, loss = 0.05446326\n",
      "Iteration 75, loss = 0.05345961\n",
      "Iteration 76, loss = 0.05223513\n",
      "Iteration 77, loss = 0.05073333\n",
      "Iteration 78, loss = 0.04965453\n",
      "Iteration 79, loss = 0.04847197\n",
      "Iteration 80, loss = 0.04738160\n",
      "Iteration 81, loss = 0.04625674\n",
      "Iteration 82, loss = 0.04523314\n",
      "Iteration 83, loss = 0.04420287\n",
      "Iteration 84, loss = 0.04322754\n",
      "Iteration 85, loss = 0.04241445\n",
      "Iteration 86, loss = 0.04139396\n",
      "Iteration 87, loss = 0.04056648\n",
      "Iteration 88, loss = 0.03980981\n",
      "Iteration 89, loss = 0.03874596\n",
      "Iteration 90, loss = 0.03803272\n",
      "Iteration 91, loss = 0.03715464\n",
      "Iteration 92, loss = 0.03648280\n",
      "Iteration 93, loss = 0.03574149\n",
      "Iteration 94, loss = 0.03498351\n",
      "Iteration 95, loss = 0.03430079\n",
      "Iteration 96, loss = 0.03362866\n",
      "Iteration 97, loss = 0.03278766\n",
      "Iteration 98, loss = 0.03232030\n",
      "Iteration 99, loss = 0.03149560\n",
      "Iteration 100, loss = 0.03095897\n",
      "Iteration 101, loss = 0.03037065\n",
      "Iteration 102, loss = 0.02977398\n",
      "Iteration 103, loss = 0.02916274\n",
      "Iteration 104, loss = 0.02861103\n",
      "Iteration 105, loss = 0.02811660\n",
      "Iteration 106, loss = 0.02749999\n",
      "Iteration 107, loss = 0.02731877\n",
      "Iteration 108, loss = 0.02644954\n",
      "Iteration 109, loss = 0.02609464\n",
      "Iteration 110, loss = 0.02546866\n",
      "Iteration 111, loss = 0.02504382\n",
      "Iteration 112, loss = 0.02459544\n",
      "Iteration 113, loss = 0.02413285\n",
      "Iteration 114, loss = 0.02364524\n",
      "Iteration 115, loss = 0.02329385\n",
      "Iteration 116, loss = 0.02285555\n",
      "Iteration 117, loss = 0.02243573\n",
      "Iteration 118, loss = 0.02205307\n",
      "Iteration 119, loss = 0.02165685\n",
      "Iteration 120, loss = 0.02127257\n",
      "Iteration 121, loss = 0.02096504\n",
      "Iteration 122, loss = 0.02062529\n",
      "Iteration 123, loss = 0.02027751\n",
      "Iteration 124, loss = 0.01996051\n",
      "Iteration 125, loss = 0.01959056\n",
      "Iteration 126, loss = 0.01932069\n",
      "Iteration 127, loss = 0.01906941\n",
      "Iteration 128, loss = 0.01876037\n",
      "Iteration 129, loss = 0.01838261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 130, loss = 0.01811509\n",
      "Iteration 131, loss = 0.01781734\n",
      "Iteration 132, loss = 0.01749843\n",
      "Iteration 133, loss = 0.01722696\n",
      "Iteration 134, loss = 0.01700578\n",
      "Iteration 135, loss = 0.01672578\n",
      "Iteration 136, loss = 0.01645823\n",
      "Iteration 137, loss = 0.01623979\n",
      "Iteration 138, loss = 0.01601761\n",
      "Iteration 139, loss = 0.01573399\n",
      "Iteration 140, loss = 0.01551510\n",
      "Iteration 141, loss = 0.01533144\n",
      "Iteration 142, loss = 0.01505793\n",
      "Iteration 143, loss = 0.01483097\n",
      "Iteration 144, loss = 0.01463550\n",
      "Iteration 145, loss = 0.01453764\n",
      "Iteration 146, loss = 0.01426825\n",
      "Iteration 147, loss = 0.01403061\n",
      "Iteration 148, loss = 0.01385481\n",
      "Iteration 149, loss = 0.01365299\n",
      "Iteration 150, loss = 0.01347941\n",
      "Iteration 151, loss = 0.01329594\n",
      "Iteration 152, loss = 0.01312206\n",
      "Iteration 153, loss = 0.01294695\n",
      "Iteration 154, loss = 0.01277130\n",
      "Iteration 155, loss = 0.01262586\n",
      "Iteration 156, loss = 0.01244432\n",
      "Iteration 157, loss = 0.01232437\n",
      "Iteration 158, loss = 0.01213893\n",
      "Iteration 159, loss = 0.01201878\n",
      "Iteration 160, loss = 0.01185420\n",
      "Iteration 161, loss = 0.01175738\n",
      "Iteration 162, loss = 0.01155300\n",
      "Iteration 163, loss = 0.01141540\n",
      "Iteration 164, loss = 0.01129428\n",
      "Iteration 165, loss = 0.01116814\n",
      "Iteration 166, loss = 0.01102591\n",
      "Iteration 167, loss = 0.01089939\n",
      "Iteration 168, loss = 0.01076994\n",
      "Iteration 169, loss = 0.01063170\n",
      "Iteration 170, loss = 0.01051307\n",
      "Iteration 171, loss = 0.01037935\n",
      "Iteration 172, loss = 0.01027368\n",
      "Iteration 173, loss = 0.01017244\n",
      "Iteration 174, loss = 0.01004554\n",
      "Iteration 175, loss = 0.00994028\n",
      "Iteration 176, loss = 0.00982255\n",
      "Iteration 177, loss = 0.00974002\n",
      "Iteration 178, loss = 0.00964555\n",
      "Iteration 179, loss = 0.00954317\n",
      "Iteration 180, loss = 0.00943076\n",
      "Iteration 181, loss = 0.00930647\n",
      "Iteration 182, loss = 0.00921157\n",
      "Iteration 183, loss = 0.00912222\n",
      "Iteration 184, loss = 0.00901836\n",
      "Iteration 185, loss = 0.00892209\n",
      "Iteration 186, loss = 0.00883806\n",
      "Iteration 187, loss = 0.00877915\n",
      "Iteration 188, loss = 0.00867219\n",
      "Iteration 189, loss = 0.00857132\n",
      "Iteration 190, loss = 0.00848674\n",
      "Iteration 191, loss = 0.00842066\n",
      "Iteration 192, loss = 0.00831863\n",
      "Iteration 193, loss = 0.00824392\n",
      "Iteration 194, loss = 0.00817209\n",
      "Iteration 195, loss = 0.00808582\n",
      "Iteration 196, loss = 0.00800745\n",
      "Iteration 197, loss = 0.00793687\n",
      "Iteration 198, loss = 0.00787142\n",
      "Iteration 199, loss = 0.00778936\n",
      "Iteration 200, loss = 0.00773182\n",
      "Iteration 201, loss = 0.00767162\n",
      "Iteration 202, loss = 0.00758686\n",
      "Iteration 203, loss = 0.00754153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "\n",
      "Temps d'execution = 168.817543 seconde\n",
      "\n",
      "Cross-validation F1 spam scores: [0.92957746 0.9109589  0.94303797 0.94852941 0.93772894 0.91946309\n",
      " 0.91791045 0.92567568 0.9025974  0.910299  ]\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=.20, random_state=42)\n",
    "cv.get_n_splits(x)\n",
    "\n",
    "tmps1=time.time()\n",
    "\n",
    "scores = cross_val_score(mlp, x, y, cv=cv, scoring='f1')\n",
    "\n",
    "tmps2=time.time() - tmps1\n",
    "print(\"Temps d'execution = %f seconde\\n\" %tmps2)\n",
    "\n",
    "print(\"Cross-validation F1 ham scores: {}\".format(scores))\n",
    "\n",
    "tmps1=time.time()\n",
    "\n",
    "scores2 = cross_val_score(mlp, x, y2, cv=cv, scoring='f1')\n",
    "\n",
    "tmps2=time.time() - tmps1\n",
    "print(\"\\n\\nTemps d'execution = %f seconde\\n\" %tmps2)\n",
    "\n",
    "print(\"Cross-validation F1 spam scores: {}\".format(scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'execution = 18.630127 seconde\n",
      "\n",
      "Cross-validation F1 ham scores: [0.98773006 0.9825998  0.98751301 0.99185336 0.98878695 0.98614674\n",
      " 0.98830707 0.98360656 0.97986577 0.98399587]\n",
      "\n",
      "\n",
      "Temps d'execution = 18.258321 seconde\n",
      "\n",
      "Cross-validation F1 spam scores: [0.91240876 0.87681159 0.92207792 0.93984962 0.91791045 0.90391459\n",
      " 0.91254753 0.88489209 0.8668942  0.89419795]\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=.20, random_state=42)\n",
    "cv.get_n_splits(x)\n",
    "\n",
    "tmps1=time.time()\n",
    "\n",
    "scores = cross_val_score(svclass, x, y, cv=cv, scoring='f1')\n",
    "\n",
    "tmps2=time.time() - tmps1\n",
    "print(\"Temps d'execution = %f seconde\\n\" %tmps2)\n",
    "\n",
    "print(\"Cross-validation F1 ham scores: {}\".format(scores))\n",
    "\n",
    "tmps1=time.time()\n",
    "\n",
    "scores2 = cross_val_score(svclass, x, y2, cv=cv, scoring='f1')\n",
    "\n",
    "tmps2=time.time() - tmps1\n",
    "print(\"\\n\\nTemps d'execution = %f seconde\\n\" %tmps2)\n",
    "\n",
    "print(\"Cross-validation F1 spam scores: {}\".format(scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Compare at least three classification algorithms in terms of ** f1 score **. Which is the most powerful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "- F1_score Ham  : 0.97                    \n",
    "- F1_score Spam : 0.82\n",
    "- Temps d'éxécution : 0.173481 seconde   \n",
    "\n",
    "\n",
    "\n",
    "- Score cross validation ham: \n",
    "[0.97672065 0.97163121 0.97380586 0.98330804 0.97531486 0.97052846 0.97782258 0.9721519  0.96390442 0.97192445]\n",
    "- Mean Cross validation score ham : __0.973711243__\n",
    "\n",
    "- Score cross validation spam : \n",
    "[0.81889764 0.78125    0.81978799 0.86956522 0.8        0.77862595 0.82113821 0.78431373 0.73003802 0.79704797]\n",
    "- Mean Cross validation score spam : __0.800066473__\n",
    "\n",
    "\n",
    "- Temps d'éxécution cross validation : 1.076684 seconde\n",
    "\n",
    "\n",
    "\n",
    "#### MPL Classifier\n",
    "\n",
    "- F1_score Ham : 0.99\n",
    "- F1_score Spam : 0.94\n",
    "\n",
    "- Temps d'éxécution: 22.520708 seconde\n",
    "\n",
    "- Score cross validation ham: [0.98206278 0.97668161 0.9838565  0.98744395 0.98475336 0.97847534\n",
    " 0.98026906 0.98026906 0.97309417 0.97578475] \n",
    "- Mean Cross validation score ham : __0.988594359__\n",
    "\n",
    "- Score cross validation spam : [0.92957746 0.9109589  0.94303797 0.94852941 0.93772894 0.91946309\n",
    " 0.91791045 0.92567568 0.9025974  0.910299  ]\n",
    "- Mean cross validation score spam : __0.924577829__\n",
    "\n",
    "- Temps d'éxécution cross validation: 183.508235 seconde\n",
    "\n",
    "#### SVC\n",
    "\n",
    "- F1_score : 0.99 \n",
    "- F1_score : 0.92\n",
    "\n",
    "- Temps d'éxécution: 1.631358 seconde\n",
    "\n",
    "- Score cross validation ham: [0.97847534 0.96950673 0.97847534 0.98565022 0.98026906 0.97578475\n",
    " 0.9793722  0.97130045 0.96502242 0.97219731] \n",
    "- Mean Cross validation score : __0.975__\n",
    "\n",
    "- Score cross validation spam: [0.91240876, 0.87681159, 0.92207792, 0.93984962, 0.91791045, 0.90391459,\n",
    " 0.91254753, 0.88489209, 0.8668942,  0.89419795] \n",
    "- Mean Cross validation score : __0.903150470__\n",
    "\n",
    "- Temps d'éxécution cross validation: 20.917359 seconde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973711243\n",
      "0.800066473\n",
      "0.988594359\n",
      "0.9245778299999999\n",
      "0.9031504700000001\n"
     ]
    }
   ],
   "source": [
    "# mean cross validation score calcule\n",
    "\n",
    "a1 = [0.97672065, 0.97163121, 0.97380586, 0.98330804, 0.97531486, 0.97052846,\n",
    " 0.97782258 ,0.9721519  ,0.96390442 ,0.97192445]\n",
    "print(sum(a1)/len(a1))\n",
    "\n",
    "a2 = [0.81889764, 0.78125,    0.81978799, 0.86956522, 0.8        ,0.77862595\n",
    " ,0.82113821 ,0.78431373 ,0.73003802 ,0.79704797]\n",
    "print(sum(a2)/len(a2))\n",
    "\n",
    "b1 = [0.98972251, 0.98658411, 0.99059561, 0.99234303, 0.99131323, 0.98757764\n",
    " ,0.98878695, 0.9881137,  0.9849037 , 0.98600311]\n",
    "print(sum(b1)/len(b1))\n",
    "\n",
    "b2 = [0.92957746, 0.9109589,  0.94303797, 0.94852941, 0.93772894, 0.91946309,\n",
    " 0.91791045, 0.92567568, 0.9025974,  0.910299  ]\n",
    "print(sum(b2)/len(b2))\n",
    "\n",
    "c2 = [0.91240876, 0.87681159, 0.92207792, 0.93984962, 0.91791045, 0.90391459,\n",
    " 0.91254753, 0.88489209, 0.8668942,  0.89419795]\n",
    "print(sum(c2)/len(c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
